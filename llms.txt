This file is a merged representation of the entire codebase, combined into a single document by Repomix.

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files (if enabled)
5. Multiple file entries, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's configuration
- Binary files are not included in this packed representation. Please refer to the Repository Structure section for a complete list of file paths, including binary files
- Files matching patterns in .gitignore are excluded
- Files matching default ignore patterns are excluded
- Files are sorted by Git change count (files with more changes are at the bottom)
</notes>

</file_summary>

<directory_structure>
.cursor/
  rules/
    0project.mdc
    cleanup.mdc
    filetree.mdc
    quality.mdc
.github/
  workflows/
    push.yml
    release.yml
.idx/
  dev.nix
  integrations.json
src/
  twat_genai/
    core/
      __init__.py
      config.py
      image_utils.py
      image.py
      lora.py
      models.py
      prompt.py
    engines/
      fal/
        __init__.py
        client.py
        config.py
        lora.py
        models.py
        outpaint.py
        upscale.py
      __init__.py
      base.py
    __init__.py
    __main___loras.json
    __main__.py
    cli.py
tests/
  conftest.py
  test_cli.py
  test_fal_client.py
  test_image_utils.py
  test_twat_genai.py
.cursorindexingignore
.gitignore
.pre-commit-config.yaml
CHANGELOG.md
CHANGES.md
cleanup.py
LICENSE
LOG.md
PLAN.md
pyproject.toml
README.md
test_outpaint.py
TODO.md
VERSION.txt
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path=".cursor/rules/0project.mdc">
---
description: About this project
globs:
---
# About this project

`twat-fs` is a file system utility library focused on robust and extensible file upload capabilities with multiple provider support. It provides:

- Multi-provider upload system with smart fallback (catbox.moe default, plus Dropbox, S3, etc.)
- Automatic retry for temporary failures, fallback for permanent ones
- URL validation and clean developer experience with type hints
- Simple CLI: `python -m twat_fs upload_file path/to/file.txt`
- Easy installation: `uv pip install twat-fs` (basic) or `uv pip install 'twat-fs[all,dev]'` (all features)

## Development Notes
- Uses `uv` for Python package management
- Quality tools: ruff, mypy, pytest
- Clear provider protocol for adding new storage backends
- Strong typing and runtime checks throughout
</file>

<file path=".cursor/rules/cleanup.mdc">
---
description: Run `cleanup.py` script before and after changes
globs:
---
Before you do any changes or if I say "cleanup", run the `cleanup.py update` script in the main folder. Analyze the results, describe recent changes in @LOG.md and edit @TODO.md to update priorities and plan next changes. PERFORM THE CHANGES, then run the `cleanup.py status` script and react to the results.

When you edit @TODO.md, lead in lines with empty GFM checkboxes if things aren't done (`- [ ] `) vs. filled (`- [x] `) if done.
</file>

<file path=".cursor/rules/filetree.mdc">
---
description: File tree of the project
globs:
---
[ 768]  .
├── [  64]  .benchmarks
├── [  96]  .cursor
│   └── [ 224]  rules
│       ├── [ 821]  0project.mdc
│       ├── [ 516]  cleanup.mdc
│       ├── [1.6K]  filetree.mdc
│       └── [2.0K]  quality.mdc
├── [  96]  .github
│   └── [ 128]  workflows
│       ├── [2.7K]  push.yml
│       └── [1.4K]  release.yml
├── [3.5K]  .gitignore
├── [ 500]  .pre-commit-config.yaml
├── [ 987]  CLEANUP.txt
├── [1.0K]  LICENSE
├── [2.5K]  LOG.md
├── [2.9K]  README.md
├── [ 57K]  REPO_CONTENT.txt
├── [   7]  VERSION.txt
├── [ 13K]  cleanup.py
├── [ 160]  dist
├── [8.2K]  pyproject.toml
├── [ 128]  src
│   └── [ 352]  twat_genai
│       ├── [ 864]  __init__.py
│       ├── [ 22K]  __main__.py
│       ├── [1.1K]  __main___loras.json
│       ├── [6.4K]  cli.py
│       ├── [ 224]  core
│       │   ├── [1.7K]  config.py
│       │   ├── [1.3K]  image.py
│       │   ├── [1.3K]  models.py
│       │   └── [8.0K]  prompt.py
│       └── [ 192]  engines
│           ├── [1.7K]  base.py
│           └── [ 256]  fal
│               ├── [3.3K]  __init__.py
│               ├── [4.5K]  client.py
│               ├── [3.2K]  config.py
│               ├── [5.9K]  lora.py
│               └── [1.2K]  models.py
└── [ 128]  tests
    └── [ 154]  test_twat_genai.py

13 directories, 31 files
</file>

<file path=".cursor/rules/quality.mdc">
---
description: Quality
globs:
---
- **Verify Information**: Always verify information before presenting it. Do not make assumptions or speculate without clear evidence.
- **No Apologies**: Never use apologies.
- **No Whitespace Suggestions**: Don't suggest whitespace changes.
- **No Inventions**: Don't invent major changes other than what's explicitly requested.
- **No Unnecessary Confirmations**: Don't ask for confirmation of information already provided in the context.
- **Preserve Existing Code**: Don't remove unrelated code or functionalities. Pay attention to preserving existing structures.
- **No Implementation Checks**: Don't ask the user to verify implementations that are visible in the provided context.
- **No Unnecessary Updates**: Don't suggest updates or changes to files when there are no actual modifications needed.
- **No Current Implementation**: Don't show or discuss the current implementation unless specifically requested.
- **Use Explicit Variable Names**: Prefer descriptive, explicit variable names over short, ambiguous ones to enhance code readability.
- **Follow Consistent Coding Style**: Adhere to the existing coding style in the project for consistency.
- **Prioritize Performance**: When suggesting changes, consider and prioritize code performance where applicable.
- **Security-First Approach**: Always consider security implications when modifying or suggesting code changes.
- **Test Coverage**: Suggest or include appropriate unit tests for new or modified code.
- **Error Handling**: Implement robust error handling and logging where necessary.
- **Modular Design**: Encourage modular design principles to improve code maintainability and reusability.
- **Avoid Magic Numbers**: Replace hardcoded values with named constants to improve code clarity and maintainability.
- **Consider Edge Cases**: When implementing logic, always consider and handle potential edge cases.
- **Use Assertions**: Include assertions wherever possible to validate assumptions and catch potential errors early.
</file>

<file path=".github/workflows/push.yml">
name: Build & Test

on:
  push:
    branches: [main]
    tags-ignore: ["v*"]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write
  id-token: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  quality:
    name: Code Quality
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Run Ruff lint
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "check --output-format=github"

      - name: Run Ruff Format
        uses: astral-sh/ruff-action@v3
        with:
          version: "latest"
          args: "format --check --respect-gitignore"

  test:
    name: Run Tests
    needs: quality
    strategy:
      matrix:
        python-version: ["3.10"]
        os: [ubuntu-latest]
      fail-fast: true
    runs-on: ${{ matrix.os }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ matrix.python-version }}

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: ${{ matrix.python-version }}
          enable-cache: true
          cache-suffix: ${{ matrix.os }}-${{ matrix.python-version }}

      - name: Install test dependencies
        run: |
          uv pip install --system --upgrade pip
          uv pip install --system ".[test]"

      - name: Run tests with Pytest
        run: uv run pytest -n auto --maxfail=1 --disable-warnings --cov-report=xml --cov-config=pyproject.toml --cov=src/twat_genai --cov=tests tests/

      - name: Upload coverage report
        uses: actions/upload-artifact@v4
        with:
          name: coverage-${{ matrix.python-version }}-${{ matrix.os }}
          path: coverage.xml

  build:
    name: Build Distribution
    needs: test
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Upload distribution artifacts
        uses: actions/upload-artifact@v4
        with:
          name: dist-files
          path: dist/
          retention-days: 5
</file>

<file path=".github/workflows/release.yml">
name: Release

on:
  push:
    tags: ["v*"]

permissions:
  contents: write
  id-token: write

jobs:
  release:
    name: Release to PyPI
    runs-on: ubuntu-latest
    environment:
      name: pypi
      url: https://pypi.org/p/twat-genai
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.12"

      - name: Install UV
        uses: astral-sh/setup-uv@v5
        with:
          version: "latest"
          python-version: "3.12"
          enable-cache: true

      - name: Install build tools
        run: uv pip install build hatchling hatch-vcs

      - name: Build distributions
        run: uv run python -m build --outdir dist

      - name: Verify distribution files
        run: |
          ls -la dist/
          test -n "$(find dist -name '*.whl')" || (echo "Wheel file missing" && exit 1)
          test -n "$(find dist -name '*.tar.gz')" || (echo "Source distribution missing" && exit 1)

      - name: Publish to PyPI
        uses: pypa/gh-action-pypi-publish@release/v1
        with:
          password: ${{ secrets.PYPI_TOKEN }}

      - name: Create GitHub Release
        uses: softprops/action-gh-release@v1
        with:
          files: dist/*
          generate_release_notes: true
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
</file>

<file path=".idx/dev.nix">
# To learn more about how to use Nix to configure your environment
# see: https://firebase.google.com/docs/studio/customize-workspace
{ pkgs, ... }: {
  # Which nixpkgs channel to use.
  channel = "stable-24.11"; # or "unstable"

  # Use https://search.nixos.org/packages to find packages
  packages = [
    pkgs.python312Full
    pkgs.python312Packages.uv
  ];

  # Sets environment variables in the workspace
  env = {};
  idx = {
    # Search for the extensions you want on https://open-vsx.org/ and use "publisher.id"
    extensions = [
      "ms-python.python"
      "beardedbear.beardedtheme"
      "bierner.markdown-image-size"
      "bmewburn.vscode-intelephense-client"
      "bradgashler.htmltagwrap"
      "charliermarsh.ruff"
      "christian-kohler.npm-intellisense"
      "codezombiech.gitignore"
      "continue.continue"
      "darkriszty.markdown-table-prettify"
      "davidanson.vscode-markdownlint"
      "dotjoshjohnson.xml"
      "ecmel.vscode-html-css"
      "esbenp.prettier-vscode"
      "fcrespo82.markdown-table-formatter"
      "fill-labs.dependi"
      "formulahendry.auto-close-tag"
      "foxundermoon.shell-format"
      "giga-ai.giga-ai"
      "github.vscode-github-actions"
      "github.vscode-pull-request-github"
      "hilleer.yaml-plus-json"
      "ionutvmi.path-autocomplete"
      "jebbs.markdown-extended"
      "jock.svg"
      "josee9988.minifyall"
      "kilocode.kilo-code"
      "lkrms.inifmt"
      "mblode.pretty-formatter"
      "mechatroner.rainbow-csv"
      "mikestead.dotenv"
      "mikoz.autoflake-extension"
      "mikoz.isort-formatter"
      "mrmlnc.vscode-scss"
      "ms-python.black-formatter"
      "ms-python.debugpy"
      "ms-python.flake8"
      "ms-python.isort"
      "ms-python.mypy-type-checker"
      "ms-python.pylint"
      "ms-toolsai.jupyter"
      "ms-vscode.cmake-tools"
      "ms-vscode.cpptools-themes"
      "ms-vscode.live-server"
      "ms-vscode.makefile-tools"
      "ms-vscode.powershell"
      "ms-windows-ai-studio.windows-ai-studio"
      "mtxr.sqltools"
      "oderwat.indent-rainbow"
      "pascalreitermann93.vscode-yaml-sort"
      "prateekmahendrakar.prettyxml"
      "qcz.text-power-tools"
      "redhat.vscode-yaml"
      "robole.marky-dynamic"
      "robole.marky-edit"
      "robole.marky-markdown"
      "robole.marky-stats"
      "rooveterinaryinc.roo-cline"
      "saketsarin.composer-web"
      "streetsidesoftware.code-spell-checker"
      "streetsidesoftware.code-spell-checker-polish"
      "stylelint.vscode-stylelint"
      "sylar.vscode-plugin-installer"
      "takumii.markdowntable"
      "timonwong.shellcheck"
      "twxs.cmake"
      "tyriar.sort-lines"
      "unifiedjs.vscode-remark"
      "visbydev.folder-path-color"
    ];

    # Enable previews
    previews = {
      enable = true;
      previews = {
        # web = {
        #   # Example: run "npm run dev" with PORT set to IDX's defined port for previews,
        #   # and show it in IDX's web preview panel
        #   command = ["npm" "run" "dev"];
        #   manager = "web";
        #   env = {
        #     # Environment variables to set for your server
        #     PORT = "$PORT";
        #   };
        # };
      };
    };

    # Workspace lifecycle hooks
    workspace = {
      # Runs when a workspace is first created
      onCreate = {
        # Example: install JS dependencies from NPM
        # npm-install = "npm install";
      };
      # Runs when the workspace is (re)started
      onStart = {
        # Example: start a background task to watch and re-build backend code
        # watch-backend = "npm run watch-backend";
      };
    };
  };
}
</file>

<file path=".idx/integrations.json">
{}
</file>

<file path="src/twat_genai/core/__init__.py">
"""Core components for twat-genai."""
</file>

<file path="src/twat_genai/core/config.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic", "Pillow"]
# ///
"""Core configuration models and type definitions for twat-genai."""

from __future__ import annotations

from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional, Union

from pydantic import BaseModel
from PIL import Image  # Import PIL Image directly, not conditionally

# Import core types used elsewhere, define EngineConfig in base.py

if TYPE_CHECKING:
    from PIL import Image

# Type aliases
Prompts = list[str]
OutputDir = Optional[Path]
GuidanceScale = float
NumInferenceSteps = int
URLStr = str
RequestID = str
JsonDict = dict[str, Any]
ImageSizeNames = Optional[list[str]]


class ImageSizeWH(BaseModel):
    """Width and height for a custom image size."""

    width: int
    height: int


# Type alias for combined size representation
ImageSize = Union[ImageSizeNames, ImageSizeWH]


class ImageInput(BaseModel):
    """Represents an image input that can be a URL, file path, or PIL Image."""

    url: str | None = None
    path: Path | None = None
    pil_image: Image.Image | None = None

    model_config = {"arbitrary_types_allowed": True}

    @property
    def is_valid(self) -> bool:
        """Check if exactly one input type is provided."""
        return (
            sum(1 for x in (self.url, self.path, self.pil_image) if x is not None) == 1
        )

    async def to_url(self, client=None) -> str:
        """Convert the input to a URL format.

        If the input is already a URL, return it directly.
        For path and PIL image handling, the client object should be provided.

        Args:
            client: Optional API client for uploading images if needed.

        Returns:
            str: URL to the image.

        Raises:
            ValueError: If no valid input exists.
        """
        if self.url:
            return self.url

        # These cases require a client to handle uploads
        if client is None:
            msg = "Client required to convert path or PIL image to URL."
            raise ValueError(msg)

        # Actual implementation handled in specific engine adapter
        msg = "Implementation should be provided by engine adapter"
        raise ValueError(msg)


class ImageResult(BaseModel):
    """Result of a single image generation."""

    request_id: str
    timestamp: str
    result: JsonDict
    image_info: dict[str, Any]
    image: Image.Image | None = None
    original_prompt: str | None = None
    job_params: dict[str, Any] | None = None

    model_config = {"arbitrary_types_allowed": True}


# EngineConfig is defined in engines/base.py to avoid circular imports
# class EngineConfig(BaseModel):
#     guidance_scale: float = 3.5
#     num_inference_steps: int = 28
#     image_size: ImageSize = ImageSizes.SQ
#     enable_safety_checker: bool = False
</file>

<file path="src/twat_genai/core/image_utils.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["pillow", "requests", "httpx", "loguru"]
# ///
# this_file: src/twat_genai/core/image_utils.py

"""
Image processing utilities including resizing and downloading.
Primarily adapted from the superres tool.
"""

import tempfile
from pathlib import Path

import httpx  # Changed from aiohttp
from loguru import logger  # Changed from logging
from PIL import Image, ImageDraw

# Import definitions from within the package
from twat_genai.engines.fal.upscale import UPSCALE_TOOL_MAX_INPUT_SIZES
from twat_genai.engines.fal.config import ModelTypes


def resize_image_if_needed(image_path: str, model_type: ModelTypes) -> str | None:
    """
    Resize the image if it exceeds the maximum dimensions for the specific upscale model.

    Args:
        image_path: Path to the image file
        model_type: The upscale ModelType to check dimensions against

    Returns:
        Optional[str]: Path to the resized image (temp file) if resizing occurred, None otherwise.
                       Returns None if the model_type is not an upscaler or has no size limit.
    """
    # Only resize for known upscale models with size limits
    if model_type not in UPSCALE_TOOL_MAX_INPUT_SIZES:
        logger.debug(
            f"No max size defined or not an upscaler: {model_type.name}, skipping resize check."
        )
        return None

    try:
        with Image.open(image_path) as img:
            width, height = img.size
            max_size = UPSCALE_TOOL_MAX_INPUT_SIZES[model_type]

            # Check if resizing is needed
            if width <= max_size and height <= max_size:
                logger.debug(
                    f"Image size {width}x{height} is within limits for {model_type.name}, no resize needed."
                )
                return None  # No resize needed

            # Calculate new dimensions while maintaining aspect ratio
            if width > height:
                new_width = max_size
                new_height = int(height * (max_size / width))
            else:
                new_height = max_size
                new_width = int(width * (max_size / height))

            # Create resized image
            logger.debug(
                f"Resizing image from {width}x{height} to {new_width}x{new_height} for {model_type.name}"
            )
            resized = img.resize((new_width, new_height), Image.Resampling.LANCZOS)

            # Save to a new temporary file
            # Keep original format if possible, default to JPEG
            img_format = img.format or "JPEG"
            suffix = f".{img_format.lower()}"
            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
                resized.save(tmp.name, format=img_format, quality=95)
                logger.info(
                    f"Image resized ({new_width}x{new_height}) and saved to temporary file: {tmp.name}"
                )
                return tmp.name

    except FileNotFoundError:
        logger.error(f"Image file not found for resizing: {image_path}")
        raise
    except Exception as e:
        logger.error(
            f"Error resizing image {image_path} for {model_type.name}: {e}",
            exc_info=True,
        )
        msg = f"Error resizing image: {e!s}"
        raise RuntimeError(msg) from e


async def download_image_to_temp(url: str) -> str:
    """
    Download an image from a URL to a temporary file.

    Args:
        url: URL of the image to download

    Returns:
        str: Path to the downloaded image file
    """
    try:
        async with httpx.AsyncClient() as client:
            logger.debug(f"Downloading image from URL: {url}")
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()

            # Try to determine suffix from content-type or URL
            content_type = response.headers.get("content-type", "").lower()
            if "jpeg" in content_type or "jpg" in content_type:
                suffix = ".jpg"
            elif "png" in content_type:
                suffix = ".png"
            else:
                # Fallback based on URL extension or default to jpg
                suffix = Path(url).suffix.lower() or ".jpg"
                if suffix not in [".jpg", ".jpeg", ".png"]:
                    suffix = ".jpg"

            # Create a temporary file
            with tempfile.NamedTemporaryFile(suffix=suffix, delete=False) as tmp:
                tmp.write(response.content)
                logger.info(
                    f"Image downloaded from {url} to temporary file: {tmp.name}"
                )
                return tmp.name
    except httpx.RequestError as e:
        logger.error(f"HTTP error downloading image from {url}: {e}", exc_info=True)
        msg = f"Failed to download image from {url}: {e}"
        raise RuntimeError(msg) from e
    except Exception as e:
        logger.error(f"Error downloading image {url} to temp file: {e}", exc_info=True)
        msg = f"Failed to download image from {url}: {e!s}"
        raise RuntimeError(msg) from e


async def download_image(url: str, output_path: str | Path) -> None:
    """
    Download an image from a URL and save it to the specified path.

    Args:
        url: URL of the image to download
        output_path: Path where the image should be saved
    """
    try:
        async with httpx.AsyncClient() as client:
            logger.debug(f"Downloading image from {url} to {output_path}")
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
            # Ensure output_path is a Path object
            output_path_obj = Path(output_path)
            output_path_obj.parent.mkdir(
                parents=True, exist_ok=True
            )  # Ensure dir exists
            output_path_obj.write_bytes(response.content)
            logger.info(f"Image downloaded from {url} and saved to: {output_path_obj}")
    except httpx.RequestError as e:
        logger.error(
            f"HTTP error downloading image from {url} to {output_path}: {e}",
            exc_info=True,
        )
        msg = f"Failed to download image from {url} to {output_path}: {e}"
        raise RuntimeError(msg) from e
    except Exception as e:
        logger.error(f"Error downloading {url} to {output_path}: {e}", exc_info=True)
        msg = f"Failed to download image from {url} to {output_path}: {e!s}"
        raise RuntimeError(msg) from e


def create_outpaint_mask(
    image_width: int, image_height: int, target_width: int, target_height: int
) -> tuple[str, list[int]]:
    """
    Creates a mask image for flux-based outpainting.
    The mask is a black image of the target size with a white rectangle in the center
    representing the original image area.

    Args:
        image_width: Width of the original image
        image_height: Height of the original image
        target_width: Width of the target (outpainted) image
        target_height: Height of the target (outpainted) image

    Returns:
        tuple[str, list[int]]: Path to the mask image and position [x, y] of the original image
    """
    logger.debug(
        f"Creating outpaint mask for {image_width}x{image_height} -> {target_width}x{target_height}"
    )

    # Calculate the position of the original image in the target image (centered)
    offset_x = (target_width - image_width) // 2
    offset_y = (target_height - image_height) // 2
    position = [offset_x, offset_y]

    # Create a black canvas of the target size
    mask_img = Image.new("L", (target_width, target_height), color=0)

    # Draw a white rectangle in the center where the original image will be
    draw = ImageDraw.Draw(mask_img)
    draw.rectangle(
        ((offset_x, offset_y), (offset_x + image_width, offset_y + image_height)),
        fill=255,
    )

    # Save to a temporary file
    with tempfile.NamedTemporaryFile(suffix=".png", delete=False) as tmp:
        mask_img.save(tmp.name, format="PNG")
        logger.info(f"Created outpaint mask and saved to temporary file: {tmp.name}")
        return tmp.name, position


async def create_flux_inpainting_assets(
    input_image_url: str, target_width: int, target_height: int
) -> tuple[str, str, list[int]]:
    """
    Creates the 'big image' and 'big mask' needed for flux inpainting/outpainting.

    1. Downloads the input image.
    2. Creates a 'small mask' from the input's alpha channel (or black if no alpha).
       - Transparent pixels in input -> White in small mask.
       - Opaque pixels in input -> Black in small mask.
    3. Creates a 'big image' (target size, transparent) with the input pasted in the center.
    4. Creates a 'big mask' (target size, white) with the 'small mask' pasted in the center.

    Args:
        input_image_url: URL of the original input image.
        target_width: Width of the target (output) image.
        target_height: Height of the target (output) image.

    Returns:
        tuple[str, str, list[int]]: Paths to the temporary 'big image' file,
                                   'big mask' file, and the calculated [x, y] position
                                   of the original image within the target canvas.
    """
    temp_files_to_clean = []
    try:
        # 1. Download input image
        logger.debug(f"Downloading input image for flux assets: {input_image_url}")
        input_image_path = await download_image_to_temp(input_image_url)
        temp_files_to_clean.append(input_image_path)
        logger.debug(f"Input image downloaded to: {input_image_path}")

        with Image.open(input_image_path) as input_img:
            input_width, input_height = input_img.size
            logger.debug(f"Input image size: {input_width}x{input_height}")

            # Calculate center position
            offset_x = (target_width - input_width) // 2
            offset_y = (target_height - input_height) // 2
            position = [offset_x, offset_y]
            logger.debug(f"Calculated center position: {position}")

            # 2. Create 'small mask'
            small_mask_img = None
            if input_img.mode in ("RGBA", "LA") or (input_img.info.get("transparency")):
                try:
                    logger.debug(
                        "Input image has alpha channel. Extracting and inverting."
                    )
                    alpha = input_img.getchannel("A")
                    # Invert: Transparent (0) -> White (255), Opaque (255) -> Black (0)
                    small_mask_img = alpha.point(lambda p: 255 - p)
                    small_mask_img = small_mask_img.convert("L")  # Ensure grayscale
                except ValueError:
                    logger.warning(
                        "Could not get alpha channel, creating black small mask."
                    )
                    small_mask_img = Image.new(
                        "L", (input_width, input_height), color=0
                    )
            else:
                logger.debug(
                    "Input image has no alpha channel. Creating black small mask."
                )
                small_mask_img = Image.new("L", (input_width, input_height), color=0)

            # --- Crop the small mask --- #
            if (
                input_width > 4 and input_height > 4
            ):  # Ensure image is large enough to crop
                crop_box = (2, 2, input_width - 2, input_height - 2)
                logger.debug(f"Cropping small mask with box: {crop_box}")
                cropped_small_mask_img = small_mask_img.crop(crop_box)
                # The position for pasting needs adjustment due to the crop
                paste_position_big_mask = (position[0] + 2, position[1] + 2)
            else:
                logger.warning("Small mask too small to crop by 2px, using original.")
                cropped_small_mask_img = small_mask_img
                paste_position_big_mask = tuple(position)  # Use original position
            # --- End Crop ---

            # Save small mask temporarily (use the cropped version)
            with tempfile.NamedTemporaryFile(
                suffix="_small_mask_cropped.png", delete=False
            ) as tmp_small_mask:
                cropped_small_mask_img.save(tmp_small_mask.name, "PNG")
                small_mask_path = tmp_small_mask.name
                temp_files_to_clean.append(small_mask_path)
                logger.debug(f"Small mask saved to: {small_mask_path}")

            # 3. Create 'big image' (transparent canvas with input centered)
            logger.debug("Creating big image (transparent canvas).")
            big_image_img = Image.new(
                "RGBA", (target_width, target_height), (0, 0, 0, 0)
            )
            # Paste input image using its own alpha if available
            paste_mask = (
                input_img.split()[-1] if input_img.mode in ("RGBA", "LA") else None
            )
            big_image_img.paste(
                input_img.convert("RGBA"), tuple(position), mask=paste_mask
            )

            # Save big image
            with tempfile.NamedTemporaryFile(
                suffix="_big_image.png", delete=False
            ) as tmp_big_image:
                big_image_img.save(tmp_big_image.name, "PNG")
                big_image_path = tmp_big_image.name
                temp_files_to_clean.append(big_image_path)
                logger.info(f"Big image saved to temporary file: {big_image_path}")

            # 4. Create 'big mask' (white canvas with small mask centered)
            logger.debug("Creating big mask (white canvas).")
            big_mask_img = Image.new("L", (target_width, target_height), color=255)
            # Paste the *cropped* small mask at the *adjusted* position
            big_mask_img.paste(cropped_small_mask_img, paste_position_big_mask)

            # Save big mask
            with tempfile.NamedTemporaryFile(
                suffix="_big_mask.png", delete=False
            ) as tmp_big_mask:
                big_mask_img.save(tmp_big_mask.name, "PNG")
                big_mask_path = tmp_big_mask.name
                temp_files_to_clean.append(big_mask_path)
                logger.info(f"Big mask saved to temporary file: {big_mask_path}")

        # Return paths to the final assets and the position
        return big_image_path, big_mask_path, position

    except Exception as e:
        logger.error(f"Error creating flux inpainting assets: {e}", exc_info=True)
        # Clean up any files created before the error
        for f_path in temp_files_to_clean:
            try:
                Path(f_path).unlink(missing_ok=True)
            except Exception as cleanup_e:
                logger.warning(f"Failed to clean up temp file {f_path}: {cleanup_e}")
        msg = f"Error creating flux inpainting assets: {e!s}"
        raise RuntimeError(msg) from e


# Note: The original create_outpaint_mask is kept for now,
# but the FAL engine logic will be updated to call create_flux_inpainting_assets instead.

# Example usage (for testing, needs an async context):
# async def main():
#     img_url = "URL_TO_YOUR_TRANSPARENT_PNG_OR_OTHER_IMAGE"
#     tgt_w, tgt_h = 1920, 1080
#     try:
#         big_img_p, big_mask_p, pos = await create_flux_inpainting_assets(img_url, tgt_w, tgt_h)
#         print(f"Big Image Path: {big_img_p}")
#         print(f"Big Mask Path: {big_mask_p}")
#         print(f"Position: {pos}")
#         # Remember to clean up the files: Path(big_img_p).unlink(); Path(big_mask_p).unlink()
#     except Exception as e:
#         print(f"Error: {e}")
#
# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main())


def create_genfill_border_mask(
    original_width: int,
    original_height: int,
    target_width: int,
    target_height: int,
    border_thickness: int,
) -> str:
    """
    Creates a mask for the Bria GenFill post-processing step.
    The mask is black except for a white border around the original image area.
    The border extends partially into the original area and partially into the outpainted area.

    Args:
        original_width: Width of the original image area within the target canvas.
        original_height: Height of the original image area within the target canvas.
        target_width: Width of the target canvas (outpainted image size).
        target_height: Height of the target canvas (outpainted image size).
        border_thickness: The thickness of the border mask in pixels.

    Returns:
        str: Path to the temporary mask image file (PNG).
    """
    if border_thickness <= 0:
        msg = "Border thickness must be positive to create a genfill mask."
        raise ValueError(msg)

    logger.debug(
        f"Creating GenFill border mask: original={original_width}x{original_height}, "
        f"target={target_width}x{target_height}, border={border_thickness}px"
    )

    # Create a black canvas of the target size
    mask_img = Image.new("L", (target_width, target_height), color=0)
    draw = ImageDraw.Draw(mask_img)

    # Calculate the coordinates of the original image area (centered)
    offset_x = (target_width - original_width) // 2
    offset_y = (target_height - original_height) // 2
    orig_left = offset_x
    orig_top = offset_y
    orig_right = offset_x + original_width
    orig_bottom = offset_y + original_height

    # Calculate border dimensions (10% inside, 90% outside)
    inside_border = round(border_thickness * 0.10)
    outside_border = (
        border_thickness - inside_border
    )  # Ensure total thickness is correct

    # Calculate the coordinates for the white border rectangle
    # Outer bounds
    border_left = max(0, orig_left - outside_border)
    border_top = max(0, orig_top - outside_border)
    border_right = min(target_width, orig_right + outside_border)
    border_bottom = min(target_height, orig_bottom + outside_border)

    # Inner bounds (where the black hole will be punched)
    hole_left = orig_left + inside_border
    hole_top = orig_top + inside_border
    hole_right = orig_right - inside_border
    hole_bottom = orig_bottom - inside_border

    # Draw the outer white rectangle
    logger.debug(
        f"Drawing outer white border: ({border_left}, {border_top}) to ({border_right}, {border_bottom})"
    )
    draw.rectangle(((border_left, border_top), (border_right, border_bottom)), fill=255)

    # Draw the inner black rectangle (punching the hole)
    # Ensure the hole coordinates are valid (right > left, bottom > top)
    if hole_right > hole_left and hole_bottom > hole_top:
        logger.debug(
            f"Drawing inner black hole: ({hole_left}, {hole_top}) to ({hole_right}, {hole_bottom})"
        )
        draw.rectangle(((hole_left, hole_top), (hole_right, hole_bottom)), fill=0)
    else:
        logger.warning(
            "Border thickness too large relative to original image; inner hole not drawn."
        )

    # Save to a temporary file
    try:
        with tempfile.NamedTemporaryFile(
            suffix="_genfill_mask.png", delete=False
        ) as tmp:
            mask_img.save(tmp.name, format="PNG")
            logger.info(
                f"Created GenFill border mask and saved to temporary file: {tmp.name}"
            )
            return tmp.name
    except Exception as e:
        logger.error(f"Failed to save GenFill mask: {e}", exc_info=True)
        msg = f"Failed to save GenFill mask: {e!s}"
        raise RuntimeError(msg) from e
</file>

<file path="src/twat_genai/core/image.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["Pillow"]
# ///
"""Image handling and size definitions for twat-genai."""

from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from pathlib import Path

    from PIL import Image
from loguru import logger


class ImageSizes(str, Enum):
    """Standard image size presets."""

    SQ = "square_hd"
    SQL = "square"
    SDV = "portrait_4_3"
    HDV = "portrait_16_9"
    SD = "landscape_4_3"
    HD = "landscape_16_9"
    PORT = "portrait"
    LAND = "landscape"
    WIDE = "wide"
    ULTRA = "ultra"


class ImageFormats(str, Enum):
    """Supported image formats."""

    JPG = "jpeg"
    PNG = "png"
    PIL = "pil"


# Add ImageSizeWH class for compatibility with code that imports it from here
from pydantic import BaseModel


class ImageSizeWH(BaseModel):
    """Width and height for a custom image size."""

    width: int
    height: int


async def save_image(
    image: Image.Image,
    path: str | Path,
    img_format: ImageFormats | None = None,
    quality: int = 95,
) -> None:
    """Save an image to disk."""
    save_kwargs = {"quality": quality}
    format_str: str | None = None
    if img_format:
        format_str = img_format.value
        save_kwargs["format"] = format_str
    else:
        pass

    try:
        logger.debug(
            f"Saving image to {path} with format '{format_str}' and args: {save_kwargs}"
        )
        image.save(path, **save_kwargs)
        logger.info(f"Saved image to {path}")
    except KeyError:
        logger.warning(
            f"Unknown format enum {img_format} ('{format_str}'), saving without explicit format."
        )
        save_kwargs.pop("format", None)
        try:
            image.save(path, format=None, quality=quality)
        except Exception as e_inner:
            logger.error(
                f"Failed to save image {path} even without format: {e_inner}",
                exc_info=True,
            )
            msg = f"Failed to save image {path}"
            raise RuntimeError(msg) from e_inner

    except Exception as e:
        logger.error(f"Failed to save image to {path}: {e}", exc_info=True)
        msg = f"Failed to save image to {path}"
        raise RuntimeError(msg) from e


def validate_image_size(size_str: str) -> tuple[int, int] | None:
    """
    Validate and parse a custom image size string.
    Returns tuple of (width, height) if valid, None if invalid.
    Format: 'width,height' with integers.
    """
    if "," not in size_str:
        return None
    try:
        w, h = (int(x.strip()) for x in size_str.split(",", 1))
        return w, h
    except (ValueError, TypeError):
        return None


# Removed the placeholder load_image added in previous step
# def load_image(source: str | Path | Image.Image) -> Image.Image:
#     pass
</file>

<file path="src/twat_genai/core/lora.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic"]
# ///
# this_file: src/twat_genai/core/lora.py

"""Core LoRA definitions and models."""

from __future__ import annotations

from pydantic import BaseModel, RootModel

# --- LoRA Models ---


class LoraRecord(BaseModel):
    """Single LoRA record with URL and scale."""

    url: str
    scale: float = 1.0


class LoraRecordList(RootModel[list[LoraRecord]]):
    """List of LoRA records."""


class LoraLib(RootModel[dict[str, LoraRecordList]]):
    """Library of LoRA configurations, mapping prompt keywords to lists of records."""


class LoraSpecEntry(BaseModel):
    """Single LoRA specification entry for inference.

    Represents one LoRA applied with a specific scale and optional prompt trigger.
    Path can be a URL or a local identifier.
    """

    path: str
    scale: float = 1.0
    prompt: str = ""


class CombinedLoraSpecEntry(BaseModel):
    """Combined specification composed of multiple LoraSpecEntry items.

    Used when a single keyword maps to multiple underlying LoRA files.
    """

    entries: list[LoraSpecEntry | CombinedLoraSpecEntry]
    factory_key: str | None = None  # Store the original keyword if created from LoraLib
</file>

<file path="src/twat_genai/core/models.py">
"""Core data models for image generation."""

from __future__ import annotations

from enum import Enum
from pathlib import Path
from typing import TYPE_CHECKING, Any, Optional, Union

from pydantic import BaseModel

if TYPE_CHECKING:
    from PIL import Image


class ModelTypes(str, Enum):
    """Available model types."""

    TEXT = "fal-ai/flux-lora"
    IMAGE = "fal-ai/flux-lora/image-to-image"
    CANNY = "fal-ai/flux-lora-canny"
    DEPTH = "fal-ai/flux-lora-depth"


class ImageSizes(str, Enum):
    """Standard image size presets."""

    SQ = "square_hd"
    SQL = "square"
    SDV = "portrait_4_3"
    HDV = "portrait_16_9"
    SD = "landscape_4_3"
    HD = "landscape_16_9"


class ImageFormats(str, Enum):
    """Supported image formats."""

    JPG = "jpeg"
    PNG = "png"
    PIL = "pil"


class ImageSizeWH(BaseModel):
    """Custom image dimensions."""

    width: int
    height: int


class ImageResult(BaseModel):
    """Result of an image generation operation."""

    request_id: str
    timestamp: str
    result: dict[str, Any]
    image_info: dict[str, Any]
    image: Image.Image | None = None
    original_prompt: str | None = None
    job_params: dict[str, Any] | None = None

    model_config = {"arbitrary_types_allowed": True}


# Type aliases
ImageSize = Union[ImageSizes, ImageSizeWH]
OutputDir = Optional[Path]


class TextToImageResponse(BaseModel):
    id: str
    status: str
    output: list[str | ImageOutput] | None = None
    error: str | None = None
    logs: str | None = None
</file>

<file path="src/twat_genai/core/prompt.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic", "loguru"]
# ///
"""
Midjourney prompt parsing and handling utilities.

This module provides functionality to parse and process Midjourney-style prompts including:
- Basic text prompts
- Image prompts (URLs)
- Multi-prompts with weights (using :: separator)
- Permutation prompts (using {} for alternatives)
- Parameter handling (--param value)
"""

from __future__ import annotations

from loguru import logger
from pydantic import AnyHttpUrl, BaseModel, Field, parse_obj_as

# Constants for prompt parsing
ALLOWED_IMAGE_EXTENSIONS = (".png", ".jpg", ".jpeg", ".gif", ".webp")
MULTI_PROMPT_SEPARATOR = "::"
PERMUTATION_START = "{"
PERMUTATION_END = "}"
PERMUTATION_SEPARATOR = ","
PARAM_PREFIX = "--"


class ImagePrompt(BaseModel):
    """An image URL used as part of a prompt."""

    url: AnyHttpUrl = Field(
        ..., description="Direct image URL ending with allowed extension"
    )
    weight: float = Field(
        default=1.0, description="Image weight/influence (--iw parameter)"
    )

    @property
    def is_valid(self) -> bool:
        """Check if URL ends with allowed image extension."""
        return str(self.url).lower().endswith(ALLOWED_IMAGE_EXTENSIONS)


class PromptParameter(BaseModel):
    """A parameter that modifies prompt behavior."""

    name: str = Field(..., description="Parameter name without -- prefix")
    value: str | None = Field(None, description="Parameter value if any")


class PromptPart(BaseModel):
    """A weighted part of a multi-prompt."""

    text: str = Field(..., description="Text content of this prompt part")
    weight: float = Field(default=1.0, description="Relative weight of this part")


class MidjourneyPrompt(BaseModel):
    """Complete parsed Midjourney prompt structure."""

    image_prompts: list[ImagePrompt] = Field(
        default_factory=list, description="Image URLs used in prompt"
    )
    text_parts: list[PromptPart] = Field(..., description="Text portions of the prompt")
    parameters: list[PromptParameter] = Field(
        default_factory=list, description="Prompt parameters"
    )
    raw_prompt: str = Field(..., description="Original unparsed prompt")

    def to_string(self) -> str:
        """Convert parsed prompt back to string format."""
        parts = []

        # Add image prompts
        for img in self.image_prompts:
            parts.append(str(img.url))
            if img.weight != 1.0:
                parts.append(f"--iw {img.weight}")

        # Add text parts with weights
        text_parts = []
        for part in self.text_parts:
            if part.weight == 1.0:
                text_parts.append(part.text)
            else:
                text_parts.append(f"{part.text}::{part.weight}")
        parts.append(" ".join(text_parts))

        # Add parameters
        for param in self.parameters:
            if param.value:
                parts.append(f"--{param.name} {param.value}")
            else:
                parts.append(f"--{param.name}")

        return " ".join(parts)


def split_top_level(s: str, delimiter: str = ",") -> list[str]:
    """Split string by delimiter only at top level (not inside braces)."""
    parts = []
    current = []
    depth = 0

    for char in s:
        if char == PERMUTATION_START:
            depth += 1
        elif char == PERMUTATION_END and depth > 0:
            depth -= 1

        if char == delimiter and depth == 0:
            parts.append("".join(current).strip())
            current = []
        else:
            current.append(char)

    if current:
        parts.append("".join(current).strip())

    return [p for p in parts if p]


def expand_permutations(prompt: str) -> list[str]:
    """Expand permutation groups in prompt into all combinations."""
    if PERMUTATION_START not in prompt:
        return [prompt]

    start_idx = prompt.find(PERMUTATION_START)
    if start_idx == -1:
        return [prompt]

    # Find matching end brace
    depth = 0
    end_idx = -1
    for i, char in enumerate(prompt[start_idx:], start=start_idx):
        if char == PERMUTATION_START:
            depth += 1
        elif char == PERMUTATION_END:
            depth -= 1
            if depth == 0:
                end_idx = i
                break

    if end_idx == -1:
        msg = f"Unmatched brace in prompt: {prompt}"
        raise ValueError(msg)

    prefix = prompt[:start_idx]
    options = prompt[start_idx + 1 : end_idx]
    suffix = prompt[end_idx + 1 :]

    # Split options and handle escaped commas
    parts = []
    current = []
    escaped = False
    for char in options:
        if char == "\\":
            escaped = True
            continue
        if char == PERMUTATION_SEPARATOR and not escaped:
            parts.append("".join(current).strip())
            current = []
        else:
            current.append(char)
        escaped = False
    if current:
        parts.append("".join(current).strip())

    # Recursively expand remaining permutations
    results = []
    for option in parts:
        for suffix_expanded in expand_permutations(suffix):
            results.append(f"{prefix}{option.strip()}{suffix_expanded}")

    return results


def parse_parameters(text: str) -> tuple[str, list[PromptParameter]]:
    """Extract parameters from prompt text."""
    parts = text.split()
    params = []
    prompt_parts = []

    i = 0
    while i < len(parts):
        part = parts[i]
        if part.startswith(PARAM_PREFIX):
            name = part[2:]  # Remove --
            value = None
            if i + 1 < len(parts) and not parts[i + 1].startswith(PARAM_PREFIX):
                value = parts[i + 1]
                i += 1
            params.append(PromptParameter(name=name, value=value))
        else:
            prompt_parts.append(part)
        i += 1

    return " ".join(prompt_parts), params


def parse_multi_prompt(text: str) -> list[PromptPart]:
    """Parse text into weighted prompt parts."""
    if MULTI_PROMPT_SEPARATOR not in text:
        return [PromptPart(text=text)]

    parts = []
    for part in text.split(MULTI_PROMPT_SEPARATOR):
        part = part.strip()
        if not part:
            continue

        # Check for weight
        weight = 1.0
        if " " in part:
            try:
                text, weight_str = part.rsplit(" ", 1)
                weight = float(weight_str)
            except ValueError:
                text = part
        else:
            text = part

        parts.append(PromptPart(text=text, weight=weight))

    return parts


def parse_prompt(prompt: str) -> MidjourneyPrompt:
    """Parse a complete Midjourney prompt into structured format."""
    # Split into parts and extract parameters
    text, parameters = parse_parameters(prompt)

    # Extract image prompts
    parts = text.split()
    image_prompts = []
    text_parts = []

    for part in parts:
        if any(part.lower().endswith(ext) for ext in ALLOWED_IMAGE_EXTENSIONS):
            try:
                # Ensure URL starts with http/https and parse using Pydantic
                url = (
                    part
                    if part.startswith(("http://", "https://"))
                    else f"https://{part}"
                )
                parsed_url = parse_obj_as(AnyHttpUrl, url)
                image_prompts.append(ImagePrompt(url=parsed_url))
            except Exception as e:
                logger.warning(f"Invalid image URL {part}: {e}")
        else:
            text_parts.append(part)

    # Parse remaining text as multi-prompt
    text = " ".join(text_parts)
    prompt_parts = parse_multi_prompt(text)

    return MidjourneyPrompt(
        image_prompts=image_prompts,
        text_parts=prompt_parts,
        parameters=parameters,
        raw_prompt=prompt,
    )


def normalize_prompts(prompts: str | list[str]) -> list[str]:
    """Normalize and expand a prompt or list of prompts."""
    raw_prompts = (
        split_top_level(prompts, delimiter=";") if isinstance(prompts, str) else prompts
    )

    final_prompts = []
    for raw in raw_prompts:
        final_prompts.extend(expand_permutations(raw.strip()))

    return final_prompts
</file>

<file path="src/twat_genai/engines/fal/__init__.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["fal-client", "python-dotenv"]
# ///
"""FAL image generation engine implementation."""

from __future__ import annotations

import os

import asyncio  # Unused
import tempfile
import time
import shutil
from pathlib import Path  # Make sure Path is imported at the top level

# from enum import Enum # Unused
from typing import TYPE_CHECKING, Any  # Removed Dict, Optional

from dotenv import load_dotenv
from loguru import logger
from PIL import Image  # Import Pillow
from slugify import slugify

# Internal imports
from twat_genai.engines.fal.client import FalApiClient  # Import only the client
from twat_genai.engines.fal.config import (
    # FALJobConfig, # Removed unused import
    ImageToImageConfig,
    ModelTypes,
    UpscaleConfig,
    OutpaintConfig,
)
from twat_genai.engines.base import EngineConfig, ImageGenerationEngine
from twat_genai.core.config import ImageResult, ImageInput
from twat_genai.core import image_utils  # Import the module

if TYPE_CHECKING:
    pass  # No Path import needed here as it's imported at module level

load_dotenv()


class FALEngine(ImageGenerationEngine):
    """FAL image generation engine implementation.

    Handles Text-to-Image, Image-to-Image, Upscaling, and Outpainting via FAL.
    """

    def __init__(self, output_dir: Path | None = None) -> None:
        """
        Initialize the FAL engine.

        Args:
            output_dir: Directory to save generated images and metadata.
        """
        self.output_dir = output_dir
        self.api_key: str | None = None
        self.client: FalApiClient | None = None

    async def initialize(self) -> None:
        """Initialize the engine, verify API key, and create client."""
        self.api_key = os.getenv("FAL_KEY")
        if not self.api_key:
            msg = "FAL_KEY environment variable not set. Please set it with your FAL API key."
            logger.error(msg)
            raise ValueError(msg)
        logger.debug("FAL_KEY found. Initializing FalApiClient.")
        self.client = FalApiClient()  # Instantiate our client

    async def _prepare_image_input(
        self,
        config_input: ImageInput,
        model_type: ModelTypes,
    ) -> tuple[str, int, int, Path | None]:
        """Handles image downloading, resizing, uploading, and returns URL and dimensions.

        Returns:
            tuple[str, int, int, Path | None]: Uploaded image URL, original width, original height, and optional local path
        """
        if not self.client:
            msg = "FALEngine not initialized. Call initialize() first."
            raise RuntimeError(msg)

        # The input image path to return
        original_input_path: Path | None = config_input.path

        # Try to convert ImageInput to FALImageInput for better handling
        try:
            from twat_genai.engines.fal.models import FALImageInput

            # If it's already a FALImageInput, use it
            if isinstance(config_input, FALImageInput):
                fal_input = config_input
            else:
                # Otherwise convert base ImageInput to FALImageInput
                fal_input = FALImageInput.from_base(config_input)

            # Get the URL using the FALImageInput implementation
            image_url = await fal_input.to_url(client=self.client)

            # Download the image to get dimensions
            temp_download_path = await image_utils.download_image_to_temp(image_url)
            temp_resized_path = None

            try:
                # Get original dimensions
                with Image.open(temp_download_path) as img:
                    original_width, original_height = img.size
                    logger.debug(
                        f"Original image dimensions: {original_width}x{original_height}"
                    )

                # Resize image if it's an upscaler model and needs resizing
                image_path_final = temp_download_path
                if model_type.name.startswith("UPSCALER_"):
                    logger.debug(
                        f"Model {model_type.name} is upscaler, checking dimensions..."
                    )
                    temp_resized_path = image_utils.resize_image_if_needed(
                        image_path=temp_download_path, model_type=model_type
                    )
                    if temp_resized_path:
                        logger.info(
                            f"Image resized for upscaler to: {temp_resized_path}"
                        )
                        image_path_final = temp_resized_path

                        # Re-upload if resized
                        if temp_resized_path != temp_download_path:
                            logger.info("Re-uploading resized image to FAL storage...")
                            image_url = await self.client.upload_image(image_path_final)
                            logger.info(
                                f"Resized image uploaded successfully: {image_url}"
                            )
                    else:
                        logger.debug("Upscaler input within limits, no resize needed.")

                # Clean up temporary files
                try:
                    if temp_download_path:
                        Path(temp_download_path).unlink()
                    if temp_resized_path and temp_resized_path != temp_download_path:
                        Path(temp_resized_path).unlink()
                except Exception as e:
                    logger.warning(f"Failed to clean up temp files: {e}")

                if not original_width or not original_height:
                    msg = "Internal error: Original image dimensions not obtained."
                    logger.error(msg)
                    raise RuntimeError(msg)

                return image_url, original_width, original_height, original_input_path

            except Exception as e:
                logger.error(f"Error processing image: {e}", exc_info=True)
                msg = f"Failed to process image: {e!s}"
                raise RuntimeError(msg) from e

        except ImportError as e:
            logger.error(f"Failed to import FALImageInput: {e}", exc_info=True)
            msg = f"Error importing FALImageInput: {e!s}"
            raise RuntimeError(msg) from e

        except Exception as e:
            logger.error(f"Failed to prepare image: {e}", exc_info=True)
            msg = f"Error preparing image input: {e!s}"
            raise RuntimeError(msg) from e

    async def generate(
        self,
        prompt: str,
        config: EngineConfig,
        **kwargs: Any,
    ) -> ImageResult:
        """
        Generate an image using FAL, handling different model types.

        Args:
            prompt: Text prompt for generation (used by TTI, Upscale, Outpaint)
            config: Base engine configuration (guidance_scale, num_inference_steps, image_size)
            **kwargs: Additional parameters including:
                model (ModelTypes): The specific FAL model/operation to use.
                image_config (ImageToImageConfig): Config for I2I, Canny, Depth.
                upscale_config (UpscaleConfig): Config for upscaling operations.
                outpaint_config (OutpaintConfig): Config for outpainting operations.
                lora_spec: LoRA configuration (used by TTI, I2I, Canny, Depth).
                filename_suffix: Optional suffix for generated filenames.
                filename_prefix: Optional prefix for generated filenames.

        Returns:
            ImageResult containing information about the generated image.
        """
        if not self.client:
            await self.initialize()  # Ensure client is initialized
            if not self.client:
                msg = "Failed to initialize FALEngine client."
                raise RuntimeError(msg)

        model: ModelTypes = kwargs.get("model", ModelTypes.TEXT)
        logger.info(f"Starting FAL generation job for model type: {model.name}")

        # --- Handle Upscaling ---
        if model.name.startswith("UPSCALER_"):
            upscale_config: UpscaleConfig | None = kwargs.get("upscale_config")
            if not upscale_config:
                msg = "upscale_config is required for upscaling models."
                raise ValueError(msg)

            # Prepare image input URL (download/resize/upload)
            (
                image_url,
                original_width,
                original_height,
                input_path,
            ) = await self._prepare_image_input(
                upscale_config.input_image, model_type=model
            )

            # Prepare upscale-specific arguments
            upscale_args = upscale_config.model_dump(
                exclude_none=True, exclude={"input_image"}
            )
            upscale_args["prompt"] = prompt  # Add main prompt if provided

            # Default output_dir to input image subfolder if no explicit output_dir provided
            output_dir = self.output_dir
            if not output_dir and input_path:
                output_dir = input_path.parent / input_path.stem
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.debug(
                    f"Using input image basename for output directory: {output_dir}"
                )

            logger.debug(
                f"Calling client.process_upscale for {model.name} with args: {upscale_args}"
            )
            return await self.client.process_upscale(
                image_url=image_url,
                output_dir=output_dir,
                filename_suffix=kwargs.get("filename_suffix"),
                filename_prefix=kwargs.get("filename_prefix")
                or (input_path.stem if input_path else None),
                upscaler=model.name.replace("UPSCALER_", "").lower(),
                **upscale_args,
            )

        # --- Handle Outpainting ---
        elif model in [ModelTypes.OUTPAINT_BRIA, ModelTypes.OUTPAINT_FLUX]:
            outpaint_config: OutpaintConfig | None = kwargs.get("outpaint_config")
            if not outpaint_config:
                msg = "outpaint_config is required for outpainting model."
                raise ValueError(msg)

            # Prepare image input URL and get actual dimensions
            (
                image_url,
                original_width,
                original_height,
                input_path,
            ) = await self._prepare_image_input(
                outpaint_config.input_image, model_type=model
            )

            # Default output_dir to input image subfolder if no explicit output_dir provided
            output_dir = self.output_dir
            if not output_dir and input_path:
                output_dir = input_path.parent / input_path.stem
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.debug(
                    f"Using input image basename for output directory: {output_dir}"
                )

            # Validate target dimensions against original dimensions
            if (
                original_width > outpaint_config.target_width
                or original_height > outpaint_config.target_height
            ):
                msg = (
                    f"Original image ({original_width}x{original_height}) cannot be larger "
                    f"than target size ({outpaint_config.target_width}x{outpaint_config.target_height}) for outpainting."
                )
                logger.error(msg)
                raise ValueError(msg)

            # Different handling depending on outpaint_tool
            if model == ModelTypes.OUTPAINT_BRIA:
                # Calculate placement (center the original image)
                offset_x = (outpaint_config.target_width - original_width) // 2
                offset_y = (outpaint_config.target_height - original_height) // 2
                original_image_location = [offset_x, offset_y]
                original_image_size = [original_width, original_height]
                logger.debug(
                    f"Calculated original_image_location: {original_image_location}, "
                    f"original_image_size: {original_image_size}"
                )

                # Prepare outpaint-specific arguments
                outpaint_args = outpaint_config.model_dump(
                    exclude_none=True, exclude={"input_image"}
                )
                # Overwrite/add calculated values for the API call
                outpaint_args["image_url"] = image_url
                outpaint_args["original_image_location"] = original_image_location
                outpaint_args["original_image_size"] = original_image_size
                # Ensure canvas size matches target size from config
                outpaint_args["canvas_width"] = outpaint_config.target_width
                outpaint_args["canvas_height"] = outpaint_config.target_height
                # Add canvas_size parameter (required by the API)
                outpaint_args["canvas_size"] = [
                    outpaint_config.target_width,
                    outpaint_config.target_height,
                ]
                # Ensure num_outputs matches config
                outpaint_args["num_outputs"] = outpaint_config.num_images

                logger.debug(
                    f"Calling client.process_outpaint for BRIA with calculated args: {outpaint_args}"
                )
                bria_outpaint_result = await self.client.process_outpaint(
                    prompt=prompt,
                    image_url=image_url,
                    lora_spec=None,  # No LoRA for BRIA
                    output_dir=output_dir,
                    filename_suffix=kwargs.get("filename_suffix"),
                    filename_prefix=kwargs.get("filename_prefix"),
                    outpaint_tool="bria",
                    target_width=outpaint_config.target_width,
                    target_height=outpaint_config.target_height,
                    num_images=outpaint_config.num_images,
                    original_image_location=original_image_location,
                    original_image_size=original_image_size,
                    # Pass the calculated canvas_size
                    canvas_size=outpaint_args.get("canvas_size"),
                )

                # --- BRIA GenFill Post-processing --- #
                if outpaint_config.border_thickness_factor > 0:
                    logger.info("Applying GenFill post-processing for BRIA outpaint.")
                    genfill_mask_path: str | None = None
                    # We need the result from the initial outpaint call
                    outpaint_result = (
                        bria_outpaint_result  # Store the result temporarily
                    )

                    try:
                        # Get URL and dimensions of the outpainted image
                        outpainted_image_url = outpaint_result.image_info.get("url")
                        outpainted_width = outpaint_result.image_info.get(
                            "width", outpaint_config.target_width
                        )
                        outpainted_height = outpaint_result.image_info.get(
                            "height", outpaint_config.target_height
                        )

                        if not outpainted_image_url:
                            logger.warning(
                                "No URL found in outpaint result, skipping GenFill."
                            )
                            return outpaint_result  # Return original if URL missing

                        # Calculate border thickness in pixels
                        min_dim = min(outpainted_width, outpainted_height)
                        border_thickness = round(
                            min_dim * outpaint_config.border_thickness_factor
                        )
                        logger.debug(
                            f"Calculated GenFill border thickness: {border_thickness}px"
                        )

                        if border_thickness <= 0:
                            logger.info(
                                "Calculated border thickness is zero or less, skipping GenFill."
                            )
                            return outpaint_result

                        # Create the border mask
                        from twat_genai.core.image_utils import (
                            create_genfill_border_mask,
                        )

                        genfill_mask_path = create_genfill_border_mask(
                            original_width=original_width,  # Width of the *original* paste area
                            original_height=original_height,  # Height of the *original* paste area
                            target_width=outpainted_width,  # Width of the *outpainted* image
                            target_height=outpainted_height,  # Height of the *outpainted* image
                            border_thickness=border_thickness,
                        )

                        # --- Save Debug GenFill Mask if Verbose ---
                        is_verbose = kwargs.get("verbose", False)
                        if is_verbose and self.output_dir and genfill_mask_path:
                            try:
                                # Construct debug mask filename
                                # Reuse prefix/timestamp logic from flux verbose save if applicable
                                # Or create a specific naming convention here
                                filename_prefix = (
                                    kwargs.get("filename_prefix") or "genfill"
                                )
                                timestamp_short = time.strftime("%Y%m%d%H%M%S")
                                main_suffix = (
                                    kwargs.get("filename_suffix") or ""
                                ) + "_bria_outpaint"
                                debug_mask_stem = f"{filename_prefix}{timestamp_short}{main_suffix}_genfill_debug_mask"
                                debug_mask_filename = slugify(debug_mask_stem) + ".png"
                                debug_mask_save_path = (
                                    self.output_dir / debug_mask_filename
                                )

                                # Copy the temporary mask file
                                shutil.copy2(genfill_mask_path, debug_mask_save_path)
                                logger.info(
                                    f"Saved debug GenFill mask to: {debug_mask_save_path}"
                                )
                            except Exception as e:
                                logger.warning(
                                    f"Failed to save debug GenFill mask: {e}"
                                )
                        # --- End Debug GenFill Mask Save ---

                        # Upload the mask
                        logger.debug("Uploading GenFill border mask...")
                        genfill_mask_url = await self.client.upload_image(
                            genfill_mask_path
                        )
                        logger.info(f"Uploaded GenFill mask: {genfill_mask_url}")

                        # Call the genfill process
                        genfill_suffix = (
                            kwargs.get("filename_suffix") or ""
                        ) + "_genfill"
                        genfill_result = await self.client.process_genfill(
                            prompt=prompt,  # Use original prompt
                            image_url=outpainted_image_url,
                            mask_url=genfill_mask_url,
                            output_dir=self.output_dir,
                            filename_suffix=genfill_suffix,
                            filename_prefix=kwargs.get("filename_prefix"),
                            num_images=outpaint_config.num_images,
                            # Pass other relevant params like negative_prompt from outpaint_config
                            negative_prompt=outpaint_config.negative_prompt,
                        )
                        logger.info("GenFill post-processing complete.")
                        return genfill_result

                    except Exception as e:
                        logger.error(
                            f"Error during GenFill post-processing: {e}", exc_info=True
                        )
                        # Fallback to returning the original outpaint result
                        logger.warning(
                            "GenFill failed, returning original outpaint result."
                        )
                        return outpaint_result
                    finally:
                        # Clean up the temporary genfill mask file
                        if genfill_mask_path:
                            try:
                                Path(genfill_mask_path).unlink(missing_ok=True)
                                logger.debug(
                                    f"Deleted temp GenFill mask: {genfill_mask_path}"
                                )
                            except Exception as e:
                                logger.warning(
                                    f"Failed to clean up GenFill mask file: {e}"
                                )
                else:
                    # No border thickness specified, return original BRIA result
                    logger.debug(
                        "Border thickness factor is zero or less, skipping GenFill."
                    )
                    return bria_outpaint_result

            elif model == ModelTypes.OUTPAINT_FLUX:
                # For flux-based outpainting, use the inpainting endpoint with custom assets
                from twat_genai.core.image_utils import (
                    create_flux_inpainting_assets,
                    download_image_to_temp,  # Keep for cleanup reference if needed
                )

                big_image_path: str | None = None
                big_mask_path: str | None = None

                try:
                    # 1. Create the 'big image' and 'big mask' assets
                    logger.debug("Creating flux inpainting/outpainting assets...")
                    # We need the original input image URL, which _prepare_image_input gives us.
                    # The image_url variable holds this.
                    (
                        big_image_path,
                        big_mask_path,
                        _,
                    ) = await create_flux_inpainting_assets(
                        input_image_url=image_url,  # Use the URL from _prepare_image_input
                        target_width=outpaint_config.target_width,
                        target_height=outpaint_config.target_height,
                    )
                    logger.info(
                        f"Flux assets created: Big Image={big_image_path}, Big Mask={big_mask_path}"
                    )

                    # 2. Upload the generated assets
                    logger.debug("Uploading flux assets...")
                    big_image_url = await self.client.upload_image(big_image_path)
                    big_mask_url = await self.client.upload_image(big_mask_path)
                    logger.info(
                        f"Flux assets uploaded: Big Image URL={big_image_url}, Big Mask URL={big_mask_url}"
                    )

                    # --- Save Debug Mask if Verbose ---
                    is_verbose = kwargs.get("verbose", False)
                    if is_verbose and self.output_dir and big_mask_path:
                        try:
                            # Determine a name for the debug mask based on potential output
                            # Use filename_prefix if provided, otherwise generate from prompt
                            filename_prefix = kwargs.get("filename_prefix")
                            if not filename_prefix and prompt:
                                words = prompt.split()
                                filename_prefix = (
                                    "_".join(words[:2]).lower() + "_"
                                    if len(words) >= 2
                                    else (words[0].lower() + "_")
                                    if words
                                    else "image_"
                                )
                            elif not filename_prefix:
                                filename_prefix = "image_"

                            # Basic timestamp/ID part
                            timestamp_short = time.strftime("%Y%m%d%H%M%S")

                            # Suffix for the main output image
                            main_suffix = (
                                kwargs.get("filename_suffix") or "_flux_outpaint"
                            )

                            # Construct debug mask filename
                            debug_mask_stem = f"{filename_prefix}{timestamp_short}{main_suffix}_debug_mask"

                            # Slugify and add extension (mask is PNG)
                            debug_mask_filename = slugify(debug_mask_stem) + ".png"
                            debug_mask_save_path = self.output_dir / debug_mask_filename

                            # Copy the temporary mask file
                            shutil.copy2(big_mask_path, debug_mask_save_path)
                            logger.info(
                                f"Saved debug flux mask to: {debug_mask_save_path}"
                            )
                        except Exception as e:
                            logger.warning(f"Failed to save debug flux mask: {e}")
                    # --- End Debug Mask Save ---

                    # 3. Prepare flux-specific arguments using the new assets
                    args = {
                        "prompt": prompt,
                        # Use the uploaded BIG image URL here
                        "image_url": big_image_url,
                        # Use the uploaded BIG mask URL here
                        "mask_url": big_mask_url,
                        "lora_spec": kwargs.get("lora_spec"),
                        "width": outpaint_config.target_width,  # Target size
                        "height": outpaint_config.target_height,  # Target size
                        "guidance_scale": outpaint_config.guidance_scale
                        or config.guidance_scale,
                        "num_inference_steps": outpaint_config.num_inference_steps
                        or config.num_inference_steps,
                        "negative_prompt": outpaint_config.negative_prompt,
                        "enable_safety_checker": outpaint_config.enable_safety_checker,
                        "num_images": outpaint_config.num_images,
                        # Strength might be relevant for inpainting?
                        # Check fal docs if needed.
                        "strength": 0.85,  # Default from flux inpaint docs
                    }

                    logger.debug(
                        f"Calling client.process_outpaint (as inpaint) for FLUX with args: {args}"
                    )
                    # Call process_outpaint, but provide the big_image and big_mask URLs
                    # The client method might need adjustment if it doesn't handle this case,
                    # but let's assume it uses image_url and mask_url appropriately.
                    return await self.client.process_outpaint(
                        prompt=prompt,
                        image_url=big_image_url,  # Use the big image URL
                        lora_spec=kwargs.get("lora_spec"),
                        output_dir=self.output_dir,
                        filename_suffix=kwargs.get("filename_suffix")
                        or "_flux_outpaint",
                        filename_prefix=kwargs.get("filename_prefix"),
                        outpaint_tool="flux",  # Still identify as flux
                        target_width=outpaint_config.target_width,
                        target_height=outpaint_config.target_height,
                        mask_url=big_mask_url,  # Use the big mask URL
                        guidance_scale=outpaint_config.guidance_scale,
                        num_inference_steps=outpaint_config.num_inference_steps,
                        negative_prompt=outpaint_config.negative_prompt,
                        enable_safety_checker=outpaint_config.enable_safety_checker,
                        num_images=outpaint_config.num_images,
                        # Pass other relevant args if the client method expects them
                        strength=args.get("strength"),
                    )
                finally:
                    # Clean up the temporary asset files
                    logger.debug("Cleaning up temporary flux asset files...")
                    files_to_delete = [big_image_path, big_mask_path]
                    # The input image download path is managed within create_flux_inpainting_assets
                    for file_path in files_to_delete:
                        if file_path:
                            try:
                                Path(file_path).unlink(missing_ok=True)
                                logger.debug(f"Deleted temp file: {file_path}")
                            except Exception as e:
                                logger.warning(
                                    f"Failed to clean up temporary asset file {file_path}: {e}"
                                )

        # --- Handle Text-to-Image ---
        elif model == ModelTypes.TEXT:
            # Check for image_config (should not be provided for TTI)
            if kwargs.get("image_config"):
                logger.warning(
                    "image_config provided for TEXT model, but will be ignored."
                )

            logger.debug(f"Calling client.process_tti for {model.name}")
            return await self.client.process_tti(
                prompt=prompt,
                lora_spec=kwargs.get("lora_spec"),
                output_dir=self.output_dir,
                filename_suffix=kwargs.get("filename_suffix"),
                filename_prefix=kwargs.get("filename_prefix"),
                guidance_scale=config.guidance_scale,
                num_inference_steps=config.num_inference_steps,
                image_size=config.image_size,
                negative_prompt=kwargs.get("negative_prompt"),
            )

        # --- Handle Image-to-Image (regular, Canny, Depth) ---
        elif model in [ModelTypes.IMAGE, ModelTypes.CANNY, ModelTypes.DEPTH]:
            image_config: ImageToImageConfig | None = kwargs.get("image_config")
            if not image_config or not image_config.input_image:
                msg = (
                    "image_config with input_image is required for image-based models."
                )
                raise ValueError(msg)

            # Prepare image input URL and get dimensions
            (
                image_url,
                original_width,
                original_height,
                input_path,
            ) = await self._prepare_image_input(
                image_config.input_image, model_type=model
            )

            # Default output_dir to input image subfolder if no explicit output_dir provided
            output_dir = self.output_dir
            if not output_dir and input_path:
                output_dir = input_path.parent / input_path.stem
                output_dir.mkdir(parents=True, exist_ok=True)
                logger.debug(
                    f"Using input image basename for output directory: {output_dir}"
                )

            # Select appropriate client method based on model type
            if model == ModelTypes.IMAGE:
                logger.debug(f"Calling client.process_i2i for {model.name}")
                return await self.client.process_i2i(
                    prompt=prompt,
                    image_url=image_url,
                    lora_spec=kwargs.get("lora_spec"),
                    output_dir=output_dir,
                    filename_suffix=kwargs.get("filename_suffix"),
                    filename_prefix=kwargs.get("filename_prefix")
                    or (input_path.stem if input_path else None),
                    strength=image_config.strength,
                    guidance_scale=config.guidance_scale,
                    num_inference_steps=config.num_inference_steps,
                    image_size=config.image_size,
                    negative_prompt=image_config.negative_prompt or None,
                )
            elif model == ModelTypes.CANNY:
                logger.debug(f"Calling client.process_canny for {model.name}")
                return await self.client.process_canny(
                    prompt=prompt,
                    image_url=image_url,
                    lora_spec=kwargs.get("lora_spec"),
                    output_dir=output_dir,
                    filename_suffix=kwargs.get("filename_suffix"),
                    filename_prefix=kwargs.get("filename_prefix")
                    or (input_path.stem if input_path else None),
                    guidance_scale=config.guidance_scale,
                    num_inference_steps=config.num_inference_steps,
                    image_size=config.image_size,
                    negative_prompt=image_config.negative_prompt or None,
                )
            elif model == ModelTypes.DEPTH:
                logger.debug(f"Calling client.process_depth for {model.name}")
                return await self.client.process_depth(
                    prompt=prompt,
                    image_url=image_url,
                    lora_spec=kwargs.get("lora_spec"),
                    output_dir=output_dir,
                    filename_suffix=kwargs.get("filename_suffix"),
                    filename_prefix=kwargs.get("filename_prefix")
                    or (input_path.stem if input_path else None),
                    guidance_scale=config.guidance_scale,
                    num_inference_steps=config.num_inference_steps,
                    image_size=config.image_size,
                    negative_prompt=image_config.negative_prompt or None,
                )

        # Should never reach here due to the exhaustive conditionals
        msg = f"Unsupported model type: {model}"
        raise ValueError(msg)

    async def shutdown(self) -> None:
        """Clean up resources."""
        logger.debug("Shutting down FALEngine.")
        # No specific cleanup needed for client currently
</file>

<file path="src/twat_genai/engines/fal/client.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = ["fal-client", "rich"]
# ///
# this_file: src/twat_genai/engines/fal/client.py

"""
API client for interacting with the FAL.ai API.
Derived from the superres tool's client.
"""

from __future__ import annotations

import logging  # TODO: Switch to loguru?
from pathlib import Path
from typing import Any, Literal
from collections.abc import Callable
import httpx
from slugify import slugify
from loguru import logger
from datetime import datetime, timezone
import asyncio
import json
import time

import fal_client
from fal_client.client import FalClientError  # Add direct import for FalClientError
from twat_genai.core.config import ImageResult

# Import ModelTypes, the unified enum
# from fal.config import ModelTypes
from twat_genai.engines.fal.config import ModelTypes
from twat_genai.engines.fal.lora import build_lora_arguments

# Logging setup
# TODO: Adapt logging to use loguru
log = logging.getLogger("twat_genai.engines.fal.client")

# Constants
METHOD_KEY = "method"  # Key for method parameter


# --- Helper Functions (adapted from src/twat_genai/__main__.py) ---


async def _download_image_helper(url: str, output_path: Path) -> None:
    """
    Internal helper to download an image from a URL and save it to disk.
    Uses httpx.
    """
    try:
        async with httpx.AsyncClient() as client:
            logger.debug(f"Downloading result image from {url} to {output_path}")
            response = await client.get(url, follow_redirects=True)
            response.raise_for_status()
            output_path.parent.mkdir(parents=True, exist_ok=True)  # Ensure dir exists
            output_path.write_bytes(response.content)
            logger.info(f"Saved result image to: {output_path}")
    except httpx.RequestError as e:
        logger.error(
            f"HTTP error downloading result image from {url}: {e}", exc_info=True
        )
        msg = f"Failed to download result image from {url}: {e}"
        raise RuntimeError(msg) from e
    except Exception as e:
        logger.error(f"Error saving result image {output_path}: {e}", exc_info=True)
        # Don't re-raise, allow get_result to continue if download fails
        # We still want the metadata if possible.


async def _submit_fal_job(model_endpoint: str, arguments: dict[str, Any]) -> str:
    """
    Internal helper to submit an asynchronous job to FAL.

    Args:
        model_endpoint: The full FAL model endpoint string (e.g., 'fal-ai/flux-lora')
        arguments: The arguments dictionary for the FAL job.

    Returns:
        The FAL request ID.
    """
    logger.debug(f"Submitting job to {model_endpoint} with args: {arguments}")
    handler = await fal_client.submit_async(model_endpoint, arguments=arguments)
    logger.info(f"Submitted job to {model_endpoint} with ID: {handler.request_id}")
    return handler.request_id


# --- FalApiClient Class ---


class FalApiClient:
    """
    Client for interacting with the Fal.ai API.
    Encapsulates job submission and result handling for different operations.
    """

    def __init__(self):
        """Initialize the API client."""
        # TODO: Integrate with FALEngine key handling? Ensure FAL_KEY is checked.
        pass  # No state needed currently, methods use fal_client directly

    async def upload_image(self, image_path: str | Path) -> str:
        """
        Upload an image to the Fal.ai storage.

        Args:
            image_path: Path to the image file

        Returns:
            str: URL of the uploaded image
        """
        try:
            path_obj = Path(image_path)
            logger.debug(f"Uploading image from: {path_obj}")
            response = await fal_client.upload_file_async(path_obj)
            logger.info(f"Image uploaded to: {response}")
            return response
        except Exception as e:
            logger.error(f"Failed to upload image {image_path}: {e}", exc_info=True)
            msg = f"Failed to upload image: {e!s}"
            raise RuntimeError(msg) from e

    async def process_upscale(
        self,
        image_url: str,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        upscaler: Literal[
            "drct",
            "ideogram",
            "recraft_creative",
            "recraft_clarity",
            "ccsr",
            "esrgan",
            "aura_sr",
            "clarity",
        ] = "clarity",
        **kwargs: Any,  # Upscale-specific parameters
    ) -> ImageResult:
        """
        Process an upscale job.

        Args:
            image_url: URL of the image to upscale
            output_dir: Optional directory to save the result
            filename_suffix: Optional suffix for the result filename
            filename_prefix: Optional prefix for the result filename
            upscaler: Which upscaler to use (default: "clarity")
            **kwargs: Additional parameters specific to the chosen upscaler
                     See UpscaleConfig for details on model-specific parameters
        """
        # Map upscaler choice to ModelTypes enum
        upscaler_map = {
            "drct": ModelTypes.UPSCALER_DRCT,
            "ideogram": ModelTypes.UPSCALER_IDEOGRAM,
            "recraft_creative": ModelTypes.UPSCALER_RECRAFT_CREATIVE,
            "recraft_clarity": ModelTypes.UPSCALER_RECRAFT_CLARITY,
            "ccsr": ModelTypes.UPSCALER_CCSR,
            "esrgan": ModelTypes.UPSCALER_ESRGAN,
            "aura_sr": ModelTypes.UPSCALER_AURA_SR,
            "clarity": ModelTypes.UPSCALER_CLARITY,
        }

        model_type = upscaler_map.get(upscaler)
        if not model_type:
            msg = f"Invalid upscaler choice: {upscaler}. Valid choices: {list(upscaler_map.keys())}"
            raise ValueError(msg)

        logger.info(f"Processing upscale job with {upscaler} ({model_type.value})")

        # Upscale requires an image but no prompt or LoRAs
        endpoint = model_type.value

        # Build the arguments for this specific upscaler
        fal_args = {
            "image_url": image_url,
            # Add any model-specific parameters from kwargs
            **{k: v for k, v in kwargs.items() if v is not None},
        }

        logger.debug(f"Final FAL arguments for upscale: {fal_args}")

        # Submit job and get result
        try:
            request_id = await _submit_fal_job(endpoint, fal_args)

            # Prepare job_params for metadata logging
            job_params = {
                "model": endpoint,
                "upscaler": upscaler,
                "input_image_url": image_url,  # Include input image URL for filename generation
                **kwargs,  # Include all passed config params
            }

            return await self._get_fal_result(
                request_id=request_id,
                model_endpoint=endpoint,
                output_dir=output_dir,
                filename_suffix=filename_suffix,
                filename_prefix=filename_prefix,
                original_prompt=None,  # No prompt for upscale
                job_params=job_params,
            )
        except Exception as e:
            logger.error(f"Upscale process failed: {e}", exc_info=True)
            msg = f"Upscale process failed: {e}"
            raise RuntimeError(msg) from e

    async def process_outpaint(
        self,
        prompt: str,
        image_url: str,
        lora_spec: Any | None = None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        outpaint_tool: Literal["bria", "flux"] = "bria",
        target_width: int | None = None,
        target_height: int | None = None,
        canvas_size: list[int] | None = None,
        **kwargs: Any,  # Outpaint-specific parameters
    ) -> ImageResult:
        """
        Process an outpaint job to expand an image.

        Args:
            prompt: Text prompt describing the expanded areas
            image_url: URL of the image to outpaint
            lora_spec: Optional LoRA specification (only used with flux outpainting)
            output_dir: Optional directory to save the result
            filename_suffix: Optional suffix for the result filename
            filename_prefix: Optional prefix for the result filename
            outpaint_tool: Which outpainter to use ("bria" or "flux")
            target_width: Target width for the expanded image
            target_height: Target height for the expanded image
            canvas_size: List of two integers representing the canvas size
            **kwargs: Additional parameters specific to the chosen outpainter
        """
        # Map outpaint tool choice to ModelTypes enum
        outpaint_map = {
            "bria": ModelTypes.OUTPAINT_BRIA,
            "flux": ModelTypes.OUTPAINT_FLUX,
        }

        model_type = outpaint_map.get(outpaint_tool)
        if not model_type:
            msg = f"Invalid outpaint tool choice: {outpaint_tool}. Valid choices: {list(outpaint_map.keys())}"
            raise ValueError(msg)

        logger.info(
            f"Processing outpaint job with {outpaint_tool} ({model_type.value})"
        )

        endpoint = model_type.value

        # Prepare tool-specific args based on the chosen tool
        if outpaint_tool == "bria":
            # Bria requires target_width and target_height
            if not target_width or not target_height:
                msg = "target_width and target_height are required for bria outpainting"
                raise ValueError(msg)

            # If canvas_size is not explicitly provided, create it from target dimensions
            if not canvas_size and target_width and target_height:
                canvas_size = [target_width, target_height]

            # Set up args for Bria outpainting
            fal_args = {
                "image_url": image_url,
                "prompt": prompt,
                "target_width": target_width,
                "target_height": target_height,
                "canvas_size": canvas_size,  # Always include canvas_size
                # Add general kwargs for additional parameters
                **{k: v for k, v in kwargs.items() if v is not None},
            }

        elif outpaint_tool == "flux":
            # Flux requires canvas_size instead of target dimensions
            if not canvas_size:
                # If no canvas_size but we have target dimensions, create it
                if target_width and target_height:
                    canvas_size = [target_width, target_height]
                else:
                    msg = "canvas_size is required for flux outpainting"
                    raise ValueError(msg)

            # Set up args for Flux outpainting
            fal_args = {
                "image_url": image_url,
                "prompt": prompt,
                "canvas_size": canvas_size,
                # Add general kwargs for additional parameters
                **{k: v for k, v in kwargs.items() if v is not None},
            }

            # Add LoRA parameters if specified (only supported by Flux)
            if lora_spec:
                from twat_genai.engines.fal.lora import build_lora_arguments

                lora_args = build_lora_arguments(lora_spec)
                fal_args.update(lora_args)

        else:
            # This should never happen due to the validation above
            msg = f"Unexpected outpaint tool: {outpaint_tool}"
            raise ValueError(msg)

        logger.debug(f"Final FAL arguments for outpaint: {fal_args}")

        # Submit job and get result
        try:
            request_id = await _submit_fal_job(endpoint, fal_args)

            # Prepare job_params for metadata logging
            job_params = {
                "model": endpoint,
                "outpaint_tool": outpaint_tool,
                "prompt": prompt,
                "input_image_url": image_url,  # Include input URL for filename generation
                "target_width": target_width,
                "target_height": target_height,
                "canvas_size": canvas_size,
                **kwargs,  # Include all passed config params
                # Include lora info if used
                "lora": lora_spec if lora_spec else None,
            }

            return await self._get_fal_result(
                request_id=request_id,
                model_endpoint=endpoint,
                output_dir=output_dir,
                filename_suffix=filename_suffix,
                filename_prefix=filename_prefix,
                original_prompt=prompt,
                job_params=job_params,
            )
        except Exception as e:
            logger.error(f"Outpaint process failed: {e!r}", exc_info=True)
            msg = f"Outpaint process failed: {e!r}"
            raise RuntimeError(msg) from e

    async def process_depth(
        self,
        prompt: str,
        image_url: str,
        lora_spec: Any | None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        **kwargs: Any,  # Base config + I2I specific (neg_prompt)
    ) -> ImageResult:
        """
        Process a depth-based image-to-image job.

        This is essentially a wrapper around _process_generic that handles the model selection
        and proper parameter passing.
        """
        return await self._process_generic(
            model_type=ModelTypes.DEPTH,
            prompt=prompt,
            lora_spec=lora_spec,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
            image_url=image_url,
            input_image_url=image_url,  # Include input URL for filename generation
            **kwargs,
        )

    async def _process_generic(
        self,
        model_type: ModelTypes,
        prompt: str,
        lora_spec: Any | None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        image_url: str | None = None,  # Optional for I2I, Canny, Depth
        input_image_url: str | None = None,  # For filename generation
        **kwargs: Any,  # Base config (image_size, steps, etc.) + I2I specific (strength, negative_prompt)
    ) -> ImageResult:
        """
        Generic method to process various image generation jobs.

        This provides a single implementation for TTI, I2I, and control model variants.
        The model_type parameter determines which endpoint to call.

        Args:
            model_type: ModelTypes enum value for this request
            prompt: Text prompt guiding generation
            lora_spec: LoRA specification string
            output_dir: Optional directory to save results
            filename_suffix: Optional suffix for result filenames
            filename_prefix: Optional prefix for result filenames
            image_url: Optional input image URL (required for I2I and control models)
            input_image_url: Optional input image URL for filename generation
            **kwargs: Additional parameters specific to the generation method
        """
        endpoint = model_type.value
        logger.info(f"Processing job with model {model_type.name} ({endpoint})")

        # Validate input for I2I and controlnet models
        if model_type in {ModelTypes.IMAGE, ModelTypes.CANNY, ModelTypes.DEPTH}:
            if not image_url:
                msg = f"input_image is required for {model_type.name} mode"
                raise ValueError(msg)

        # Prepare base arguments
        fal_args = {"prompt": prompt}

        # Add image URL for image-to-image and controlnet variants
        if image_url:
            fal_args["image_url"] = image_url

        # Add strength parameter for I2I if provided (default handled by API)
        if "strength" in kwargs:
            strength = kwargs.pop("strength")
            if strength is not None:
                fal_args["strength"] = strength

        # Add negative_prompt if provided
        if "negative_prompt" in kwargs:
            negative_prompt = kwargs.pop("negative_prompt")
            if negative_prompt:  # Only add if not empty
                fal_args["negative_prompt"] = negative_prompt

        # Handle image size (preset or dimensions)
        if "image_size" in kwargs:
            image_size = kwargs.pop("image_size")
            if image_size is not None:
                # Import required types here to avoid circular imports
                from twat_genai.core.image import ImageSizes, ImageSizeWH

                if isinstance(image_size, ImageSizeWH):
                    fal_args["width"] = image_size.width
                    fal_args["height"] = image_size.height
                elif isinstance(image_size, ImageSizes):
                    # Get the dimensions using the value property or preset table
                    if hasattr(image_size, "dimensions"):
                        width, height = image_size.dimensions
                    else:
                        # Fallback to known presets if dimensions not available
                        size_map = {
                            ImageSizes.SQ: (1024, 1024),  # Square
                            ImageSizes.PORT: (768, 1024),  # Portrait
                            ImageSizes.LAND: (1024, 768),  # Landscape
                            ImageSizes.WIDE: (1216, 832),  # Wide
                            ImageSizes.ULTRA: (1344, 768),  # Ultra wide
                        }
                        width, height = size_map.get(image_size, (1024, 1024))

                    fal_args["width"] = width
                    fal_args["height"] = height

        # Add remaining standard parameters if provided
        standard_params = ["guidance_scale", "num_inference_steps"]
        for param in standard_params:
            if param in kwargs:
                value = kwargs.pop(param)
                if value is not None:
                    fal_args[param] = value

        # Add any remaining kwargs as direct parameters
        # This allows for future extension without code changes
        for k, v in kwargs.items():
            if v is not None:
                fal_args[k] = v

        # Add LoRA parameters if specified
        if lora_spec:
            from twat_genai.engines.fal.lora import build_lora_arguments

            lora_args = build_lora_arguments(lora_spec)
            fal_args.update(lora_args)

        logger.debug(f"Final FAL arguments: {fal_args}")

        # Submit job and get result
        try:
            request_id = await _submit_fal_job(endpoint, fal_args)

            # Prepare job_params for metadata logging
            job_params = {
                "model": endpoint,
                "prompt": prompt,
                "input_image_url": input_image_url
                or image_url,  # Include input URL for filename generation
                **kwargs,  # Include all passed config params
                # Include original args that may have been transformed
                "lora": lora_spec if lora_spec else None,
            }

            return await self._get_fal_result(
                request_id=request_id,
                model_endpoint=endpoint,
                output_dir=output_dir,
                filename_suffix=filename_suffix,
                filename_prefix=filename_prefix,
                original_prompt=prompt,
                job_params=job_params,
            )
        except Exception as e:
            logger.error(f"Job processing failed: {e}", exc_info=True)
            msg = f"Processing {model_type.name} job failed: {e}"
            raise RuntimeError(msg) from e

    async def process_tti(
        self,
        prompt: str,
        lora_spec: Any | None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        **kwargs: Any,  # Base config params (image_size, steps, etc.)
    ) -> ImageResult:
        """Process a Text-to-Image job."""
        return await self._process_generic(
            model_type=ModelTypes.TEXT,
            prompt=prompt,
            lora_spec=lora_spec,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
            image_url=None,  # No image for TTI
            **kwargs,
        )

    async def process_i2i(
        self,
        prompt: str,
        image_url: str,
        lora_spec: Any | None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        **kwargs: Any,  # Base config + I2I specific (strength, neg_prompt)
    ) -> ImageResult:
        """
        Process an image-to-image job.

        This is essentially a wrapper around _process_generic that handles the model selection
        and proper parameter passing.
        """
        return await self._process_generic(
            model_type=ModelTypes.IMAGE,
            prompt=prompt,
            lora_spec=lora_spec,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
            image_url=image_url,
            input_image_url=image_url,  # Include input URL for filename generation
            **kwargs,
        )

    async def process_canny(
        self,
        prompt: str,
        image_url: str,
        lora_spec: Any | None,
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        **kwargs: Any,  # Base config + I2I specific (neg_prompt)
    ) -> ImageResult:
        """
        Process a canny edge detection image-to-image job.

        This is essentially a wrapper around _process_generic that handles the model selection
        and proper parameter passing.
        """
        return await self._process_generic(
            model_type=ModelTypes.CANNY,
            prompt=prompt,
            lora_spec=lora_spec,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
            image_url=image_url,
            input_image_url=image_url,  # Include input URL for filename generation
            **kwargs,
        )

    async def process_genfill(
        self,
        prompt: str,
        image_url: str,  # The outpainted image URL
        mask_url: str,  # The border mask URL
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        num_images: int = 1,
        **kwargs: Any,  # Should include things like negative_prompt if needed
    ) -> ImageResult:
        """
        Process an image using the Bria GenFill model.

        Args:
            prompt: The original prompt used for outpainting.
            image_url: URL of the image to process (the result from outpainting).
            mask_url: URL of the border mask to use for genfill.
            output_dir: Directory to save the final image and metadata.
            filename_suffix: Suffix to add to the generated filename.
            filename_prefix: Prefix for the generated filename.
            num_images: Number of images to generate (usually 1 for genfill).
            **kwargs: Additional arguments for the genfill API.

        Returns:
            ImageResult containing info about the processed image.
        """
        model_endpoint = "fal-ai/bria/genfill"
        job_params = {
            "prompt": prompt,
            "image_url": image_url,
            "mask_url": mask_url,
            "num_images": num_images,
            # Explicitly include known/expected optional args from kwargs
            # Add more as needed based on the genfill API specifics
            "negative_prompt": kwargs.get("negative_prompt", ""),
        }
        # Filter out None values if the API doesn't like them
        job_params = {k: v for k, v in job_params.items() if v is not None}

        try:
            logger.info(
                f"Submitting GenFill job to {model_endpoint} with params: {job_params}"
            )
            request_id = await _submit_fal_job(model_endpoint, job_params)
            logger.info(f"Submitted GenFill job {request_id}")

            return await self._get_fal_result(
                request_id=request_id,
                model_endpoint=model_endpoint,
                output_dir=output_dir,
                filename_suffix=filename_suffix,
                filename_prefix=filename_prefix,
                original_prompt=prompt,
                job_params=job_params,
            )
        except Exception as e:
            logger.error(
                f"Error processing GenFill job for {model_endpoint}: {e}",
                exc_info=True,
            )
            msg = f"Failed GenFill job: {e!s}"
            raise RuntimeError(msg) from e

    async def _get_fal_result(
        self,
        request_id: str,
        model_endpoint: str,  # Need the endpoint to poll status/result
        output_dir: Path | None = None,
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        original_prompt: str | None = None,
        job_params: dict[str, Any] | None = None,
    ) -> ImageResult:
        """
        Get the result of a FAL job and process it into an ImageResult.

        Args:
            request_id: The FAL API request ID
            model_endpoint: The model endpoint being used
            output_dir: Optional directory to save the result
            filename_suffix: Optional suffix for the result filename
            filename_prefix: Optional prefix for the result filename
            original_prompt: Optional prompt text used for the generation
            job_params: Optional parameters dictionary used in job submission

        Returns:
            An ImageResult object containing information about the generated image

        Raises:
            RuntimeError: If the request fails or returns an error
        """
        logger.debug(f"Waiting for FAL result for request: {request_id}")

        # Imports kept local to avoid unnecessary imports when not using this
        import fal_client

        try:
            result = await fal_client.result_async(model_endpoint, request_id)
            logger.debug(f"Received result for request {request_id}: {result}")

            # Extract image result data
            result_extractor = self._get_result_extractor(model_endpoint)
            image_dicts = result_extractor(result)

            if not image_dicts:
                msg = "No valid image results returned from FAL API."
                raise ValueError(msg)

            # Download and save the result images if output_dir is provided
            if output_dir:
                timestamp = time.strftime("%Y%m%d%H%M%S")
                base_filename = (
                    f"{filename_prefix or ''}{timestamp}{filename_suffix or ''}"
                )
                base_filename = slugify(base_filename)

                # Ensure the output directory exists
                output_dir = Path(output_dir)
                output_dir.mkdir(parents=True, exist_ok=True)

                for i, img_dict in enumerate(image_dicts):
                    if not img_dict.get("url"):
                        logger.warning(f"No image URL in result {i}, skipping save.")
                        continue

                    # Derive file extension from mimetype, with fallback to jpg
                    mimetype = img_dict.get("mimetype", "image/jpeg")
                    extension = mimetype.split("/")[-1].split("+")[
                        0
                    ]  # e.g. 'image/jpeg+xml' -> 'jpeg'
                    if extension not in {"jpeg", "jpg", "png", "webp", "gif"}:
                        logger.warning(
                            f"Unrecognized mimetype {mimetype}, defaulting to jpg."
                        )
                        extension = "jpg"

                    # Create numbered filenames if multiple results
                    index_suffix = f"_{i}" if len(image_dicts) > 1 else ""
                    output_filename = f"{base_filename}{index_suffix}.{extension}"
                    output_path = output_dir / output_filename

                    # Save metadata alongside the image
                    metadata_filename = f"{base_filename}{index_suffix}_metadata.json"
                    metadata_path = output_dir / metadata_filename

                    # Try to download and save the image
                    try:
                        # Make a new request to download the image to avoid chunked transfer issues
                        await _download_image_helper(img_dict["url"], output_path)
                        logger.info(f"Saved image to: {output_path}")

                        # Save metadata (if we have job parameters)
                        metadata = {
                            "request_id": request_id,
                            "model": model_endpoint,
                            "created_at": datetime.now().isoformat(),
                            "prompt": original_prompt,
                            "parameters": job_params or {},
                            "output_file": str(output_path),
                            "fal_result": img_dict,
                        }
                        with open(metadata_path, "w") as f:
                            json.dump(metadata, f, indent=2, default=str)
                        logger.debug(f"Saved metadata to: {metadata_path}")

                        # Update the image_dict with the local file path for the caller
                        img_dict["path"] = str(output_path)
                        img_dict["metadata_path"] = str(metadata_path)

                    except Exception as e:
                        logger.error(f"Failed to save image: {e}", exc_info=True)

            # Return the first image info, with the complete list in all_images
            return ImageResult(
                request_id=request_id,
                timestamp=datetime.now().strftime("%Y%m%d%H%M%S"),
                result=result
                if isinstance(result, dict)
                else {"raw_result": str(result)},
                image_info=image_dicts[0],
                original_prompt=original_prompt,
                job_params=job_params,
            )

        except FalClientError as e:
            # Handle FAL API errors specifically
            try:
                # Try to access the error details, which might be a list of dicts or a string
                error_details = e.args[0] if e.args else str(e)
                logger.error(f"FAL API error: {error_details!r}")
            except Exception as nested:
                # If there's an issue accessing/formatting the error, use a safer repr
                logger.error(f"FAL API error (details unavailable): {repr(e)}")

            # Re-raise with a more readable message
            raise RuntimeError(f"FAL API error: {repr(e)}") from e

        except Exception as e:
            # For other exceptions, just log and re-raise
            logger.error(
                f"Error during FAL result processing: {repr(e)}", exc_info=True
            )
            raise RuntimeError(f"Error processing FAL result: {repr(e)}") from e

    def _get_result_extractor(
        self, model_endpoint: str
    ) -> Callable[[dict[str, Any]], list[dict[str, Any]]]:
        """Return the appropriate result extraction function based on model endpoint."""
        # TODO: Implement specific extractors for different model types
        if model_endpoint == ModelTypes.OUTPAINT_BRIA.value:
            # return self._extract_outpaint_info # Placeholder
            logger.warning(
                f"Using generic extractor for outpaint model {model_endpoint}"
            )
            return FalApiClient._extract_generic_image_info
        elif (
            model_endpoint.startswith("fal-ai/drct")
            or model_endpoint.startswith("fal-ai/ideogram/upscale")
            or model_endpoint.startswith("fal-ai/recraft")
            or model_endpoint.startswith("fal-ai/ccsr")
            or model_endpoint.startswith("fal-ai/esrgan")
            or model_endpoint.startswith("fal-ai/aura-sr")
            or model_endpoint.startswith("fal-ai/clarity-upscaler")
        ):
            # return self._extract_upscale_info # Placeholder
            logger.warning(
                f"Using generic extractor for upscale model {model_endpoint}"
            )
            return FalApiClient._extract_generic_image_info
        else:  # Default to generic TTI/I2I extractor
            # return self._extract_tti_i2i_info # Placeholder
            return FalApiClient._extract_generic_image_info

    @staticmethod
    def _extract_generic_image_info(
        result: dict[str, Any],
    ) -> list[dict[str, Any]]:
        """
        Extract image information from common API response patterns (images, image, url).
        Now an instance method.

        Args:
            result: The API response dictionary

        Returns:
            List of dictionaries containing image information.
            Returns empty list if no image info found.
        """
        extracted_list = []
        try:
            # Define a helper function to create the info dict
            def _create_info_dict(img_data: dict[str, Any] | str) -> dict[str, Any]:
                if isinstance(img_data, str):  # Handle cases where only URL is given
                    return {
                        "url": img_data,
                        "content_type": "image/png",  # Assume PNG if not specified
                        "file_name": "output.png",
                        "file_size": 0,
                        "width": None,
                        "height": None,
                        "seed": result.get("seed"),  # Try to get top-level seed
                        "file_data": None,
                    }
                # Extract potential seed from image data or top level
                seed = img_data.get("seed", result.get("seed"))
                return {
                    "url": img_data.get("url", ""),
                    "content_type": img_data.get("content_type", "image/png"),
                    "file_name": img_data.get("file_name", "output.png"),
                    "file_size": img_data.get("file_size", 0),
                    "width": img_data.get("width"),
                    "height": img_data.get("height"),
                    "seed": seed,
                    "file_data": img_data.get("file_data"),  # Base64 data if available
                }

            # Handle responses with multiple images
            if "images" in result and isinstance(result["images"], list):
                extracted_list = [_create_info_dict(img) for img in result["images"]]
            elif "image" in result:
                if isinstance(result["image"], list):
                    extracted_list = [_create_info_dict(img) for img in result["image"]]
                elif isinstance(result["image"], dict) or isinstance(
                    result["image"], str
                ):
                    # Single image dictionary or URL string
                    extracted_list = [_create_info_dict(result["image"])]
            elif "url" in result and isinstance(result["url"], str):
                # Single image URL string at top level
                extracted_list = [_create_info_dict(result["url"])]
            else:
                logger.warning(
                    f"No standard image key (images, image, url) found in API response: {result}"
                )
                return []  # Return empty list

            # Filter out entries without a URL
            valid_extracted_list = [info for info in extracted_list if info.get("url")]
            if not valid_extracted_list:
                logger.warning(
                    f"Extractor found image keys but no valid URLs in result: {result}"
                )

            return valid_extracted_list

        except Exception as e:
            # Log error but let _get_fal_result handle raising the final RuntimeError
            logger.error(f"Internal error extracting image info: {e}", exc_info=True)
            return []  # Return empty list to signal failure

    # --- Placeholder for specific extractors --- #
    # def _extract_outpaint_info(self, result: dict[str, Any]) -> list[dict[str, Any]]:
    #     # Implementation specific to outpaint result structure
    #     base_info = self._extract_generic_image_info(result)
    #     # Extract seed or other specific fields
    #     seed = result.get("seed")
    #     for info in base_info:
    #         info["seed"] = seed # Add/overwrite seed
    #     return base_info

    # def _extract_upscale_info(self, result: dict[str, Any]) -> list[dict[str, Any]]:
    #     # Implementation specific to various upscaler result structures
    #     return self._extract_generic_image_info(result)

    # def _extract_tti_i2i_info(self, result: dict[str, Any]) -> list[dict[str, Any]]:
    #     # May be same as generic, or extract specific TTI/I2I metadata
    #     return self._extract_generic_image_info(result)
</file>

<file path="src/twat_genai/engines/fal/config.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic"]
# ///
"""FAL-specific configuration and models."""

from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING, Literal

from pydantic import BaseModel

from twat_genai.core.config import ImageInput

if TYPE_CHECKING:
    pass


class ModelTypes(str, Enum):
    """Available FAL model types."""

    TEXT = "fal-ai/flux-lora"
    IMAGE = "fal-ai/flux-lora/image-to-image"
    CANNY = "fal-ai/flux-lora-canny"
    DEPTH = "fal-ai/flux-lora-depth"

    # Upscalers from superres
    UPSCALER_DRCT = "fal-ai/drct-super-resolution"
    UPSCALER_IDEOGRAM = "fal-ai/ideogram/upscale"
    UPSCALER_RECRAFT_CREATIVE = "fal-ai/recraft-creative-upscale"
    UPSCALER_RECRAFT_CLARITY = "fal-ai/recraft-clarity-upscale"
    UPSCALER_CCSR = "fal-ai/ccsr"
    UPSCALER_ESRGAN = "fal-ai/esrgan"
    UPSCALER_AURA_SR = "fal-ai/aura-sr"
    UPSCALER_CLARITY = "fal-ai/clarity-upscaler"

    # Outpainters
    OUTPAINT_BRIA = "fal-ai/bria/expand"
    OUTPAINT_FLUX = "fal-ai/flux-lora/inpainting"


class ImageToImageConfig(BaseModel):
    """Configuration for image-to-image operations."""

    model_type: ModelTypes
    input_image: ImageInput
    strength: float = 0.75  # Only used for standard image-to-image
    negative_prompt: str = ""


# --- Upscaling Config ---
class UpscaleConfig(BaseModel):
    """Configuration specific to upscaling operations."""

    input_image: ImageInput  # Required for upscaling
    prompt: str | None = None
    negative_prompt: str | None = None
    seed: int | None = None
    scale: float | None = None  # General scale, default depends on model

    # --- Tool-specific parameters ---
    # Ideogram
    ideogram_resemblance: int | None = None
    ideogram_detail: int | None = None
    ideogram_expand_prompt: bool | None = None

    # Recraft
    recraft_sync_mode: bool | None = None

    # Fooocus
    fooocus_styles: list[str] | None = None
    fooocus_performance: (
        Literal["Speed", "Quality", "Extreme Speed", "Lightning"] | None
    ) = None
    fooocus_guidance_scale: float | None = None
    fooocus_sharpness: float | None = None
    fooocus_uov_method: (
        Literal[
            "Vary (Subtle)",
            "Vary (Strong)",
            "Upscale (1.5x)",
            "Upscale (2x)",
            "Upscale (Fast 2x)",
        ]
        | None
    ) = None

    # ESRGAN
    esrgan_model: (
        Literal[
            "RealESRGAN_x4plus",
            "RealESRGAN_x2plus",
            "RealESRGAN_x4plus_anime_6B",
            "RealESRGAN_x4_v3",
            "RealESRGAN_x4_wdn_v3",
            "RealESRGAN_x4_anime_v3",
        ]
        | None
    ) = None
    esrgan_tile: int | None = None
    esrgan_face: bool | None = None

    # Clarity / AuraSR (Shared params)
    clarity_creativity: float | None = None  # (0.0 - 1.0)
    clarity_resemblance: float | None = None  # (0.0 - 1.0)
    clarity_guidance_scale: float | None = None
    clarity_num_inference_steps: int | None = None

    # CCSR
    ccsr_scale: int | None = None
    ccsr_tile_diffusion: Literal["none", "mix", "gaussian"] | None = None
    ccsr_color_fix_type: Literal["none", "wavelet", "adain"] | None = None
    ccsr_steps: int | None = None


# --- Outpainting Config ---
class OutpaintConfig(BaseModel):
    """Configuration specific to outpainting operations."""

    input_image: ImageInput  # Required for outpainting
    prompt: str
    target_width: int
    target_height: int
    num_images: int = 1
    outpaint_tool: Literal["bria", "flux"] = (
        "bria"  # Default to bria for backward compatibility
    )
    # Extra parameters for flux outpainting
    guidance_scale: float | None = None  # Flux-specific
    num_inference_steps: int | None = None  # Flux-specific
    negative_prompt: str | None = None  # Flux-specific
    enable_safety_checker: bool | None = None  # Flux-specific
    border_thickness_factor: float = (
        0.05  # Border thickness for GenFill post-processing
    )

    # Note: original_image_size and original_image_location are typically calculated
    # just before the API call based on the input_image dimensions and target size.
    # They are not stored here directly.


# --- Lora Handling (Moved to core/lora.py) ---
# TODO: Confirm all LoRA logic/models are removed from here and __main__.py
</file>

<file path="src/twat_genai/engines/fal/lora.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic"]
# ///
"""LoRA handling and processing utilities."""

from __future__ import annotations

from pathlib import Path
from typing import Any

# Import from core
# from fal.config import CombinedLoraSpecEntry, LoraLib, LoraSpecEntry # OLD import
from twat_genai.core.lora import (
    CombinedLoraSpecEntry,
    LoraLib,
    LoraSpecEntry,  # Import LoraRecord needed by LoraLib usage in get_lora_lib
)

from loguru import logger

# Try importing PathManager, handle optional dependency
try:
    from twat_os.paths import PathManager
except ImportError:
    PathManager = None
    logger.warning(
        "'twat' package not fully available. PathManager features disabled."
        " LoRA library resolution will use defaults."
    )


def get_lora_lib() -> LoraLib:
    """Get the LoRA library from the appropriate location.

    Priority:
    1. User-provided location (via environment variable)
    2. Central path management (if available)
    3. Bundled default library
    """
    # Check for user-provided location
    import os

    user_path = os.getenv("TWAT_GENAI_LORA_LIB")
    if user_path:
        lib_path = Path(user_path)
        if lib_path.exists():
            return LoraLib.model_validate_json(lib_path.read_text())

    # Check central path management if available
    if PathManager:
        try:
            paths = PathManager.for_package("twat_genai")
            lib_path = paths.genai.lora_dir / "loras.json"
            if lib_path.exists():
                return LoraLib.model_validate_json(lib_path.read_text())
        except (AttributeError, Exception) as e:
            logger.warning(
                f"Error using PathManager for LoRA library: {e}. Using bundled library."
            )

    # Fall back to bundled library
    bundled_path = Path(__file__).parent.parent.parent / "__main___loras.json"
    return LoraLib.model_validate_json(bundled_path.read_text())


# Initialize LoRA library
LORA_LIB = get_lora_lib()


def parse_lora_phrase(phrase: str) -> LoraSpecEntry | CombinedLoraSpecEntry:
    """
    Parse a LoRA phrase which may include an optional scale.

    A phrase is either:
      - A key from LORA_LIB (the prompt portion) or
      - A URL/path optionally followed by a colon and a numeric scale

    Args:
        phrase: LoRA specification phrase

    Returns:
        Parsed LoRA specification

    Raises:
        ValueError: If the phrase has invalid format
    """
    phrase = phrase.strip()
    if phrase in LORA_LIB.root:
        entries = []
        for record in LORA_LIB.root[phrase].root:
            entries.append(
                LoraSpecEntry(path=record.url, scale=record.scale, prompt=phrase + ",")
            )
        return CombinedLoraSpecEntry(entries=entries, factory_key=phrase)

    if ":" in phrase:
        identifier, scale_str = phrase.split(":", 1)
        identifier = identifier.strip()
        try:
            scale = float(scale_str.strip())
        except ValueError:
            msg = f"Invalid scale value in LoRA phrase: {phrase}"
            raise ValueError(msg) from None
    else:
        identifier = phrase
        scale = 1.0

    return LoraSpecEntry(path=identifier, scale=scale, prompt="")


def normalize_lora_spec(
    spec: str | list | tuple | None,
) -> list[LoraSpecEntry | CombinedLoraSpecEntry]:
    """
    Normalize various LoRA specification formats into a unified list.

    Args:
        spec: LoRA specification in various formats

    Returns:
        List of normalized LoRA specifications

    Raises:
        ValueError: If the specification format is invalid
    """
    if spec is None:
        return []

    normalized: list[LoraSpecEntry | CombinedLoraSpecEntry] = []

    match spec:
        case list() | tuple() as items:
            for item in items:
                match item:
                    case dict(path=path, scale=scale, prompt=prompt):
                        normalized.append(
                            LoraSpecEntry(path=path, scale=float(scale), prompt=prompt)
                        )
                    case dict() as d:
                        if "path" not in d:
                            msg = "LoRA spec dictionary must have a 'path'."
                            raise ValueError(msg)
                        normalized.append(
                            LoraSpecEntry(
                                path=d["path"],
                                scale=float(d.get("scale", 1.0)),
                                prompt=d.get("prompt", ""),
                            )
                        )
                    case str() as phrase:
                        normalized.append(parse_lora_phrase(phrase))
                    case list() as sublist:
                        combined = [
                            parse_lora_phrase(sub)
                            for sub in sublist
                            if isinstance(sub, str)
                        ]
                        normalized.append(CombinedLoraSpecEntry(entries=combined))
                    case _:
                        msg = f"Unsupported LoRA spec item type: {type(item)}"
                        raise ValueError(msg)
        case str() as s:
            if s in LORA_LIB.root:
                return [parse_lora_phrase(s)]
            phrases = [phrase.strip() for phrase in s.split(";") if phrase.strip()]
            return [parse_lora_phrase(phrase) for phrase in phrases]
        case _:
            msg = f"Unsupported LoRA spec type: {type(spec)}"
            raise ValueError(msg)

    return normalized


async def build_lora_arguments(
    lora_spec: str | list | tuple | None, prompt: str
) -> tuple[list[dict[str, Any]], str]:
    """
    Build the list of inference LoRA dictionaries and a final prompt.

    Args:
        lora_spec: LoRA specification
        prompt: Base prompt to augment

    Returns:
        Tuple of (LoRA argument list, final prompt)
    """
    entries = normalize_lora_spec(lora_spec)
    lora_list: list[dict[str, Any]] = []
    prompt_prefixes: list[str] = []

    def process_entry(entry: LoraSpecEntry | CombinedLoraSpecEntry) -> None:
        if isinstance(entry, LoraSpecEntry):
            lora_list.append({"path": entry.path, "scale": entry.scale})
            if entry.prompt:
                prompt_prefixes.append(entry.prompt.rstrip(","))
        else:
            for sub_entry in entry.entries:
                process_entry(sub_entry)

    for entry in entries:
        process_entry(entry)

    logger.debug(f"Using LoRA configuration: {lora_list}")

    final_prompt = (
        f"{', '.join(prompt_prefixes)}, {prompt}".strip() if prompt_prefixes else prompt
    )
    return lora_list, final_prompt
</file>

<file path="src/twat_genai/engines/fal/models.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic", "fal-client>=0.5.9", "Pillow>=11.1.0"]
# ///
"""FAL-specific model classes."""

import tempfile
from pathlib import Path
from typing import Any

# Try importing fal_client, but make its usage conditional or handle absence
try:
    import fal_client
except ImportError:
    fal_client = None  # type: ignore
    # logger.warning("fal_client not found. Image upload functionality will be limited.") # Requires logger

from twat_genai.core.config import ImageInput  # Use absolute import


class FALImageInput(ImageInput):
    """FAL-specific implementation of ImageInput."""

    @classmethod
    def from_base(cls, base: ImageInput) -> "FALImageInput":
        """Create a FALImageInput instance from a base ImageInput."""
        return cls(url=base.url, path=base.path, pil_image=base.pil_image)

    async def to_url(self, client: Any = None) -> str:
        """Convert the input to a URL format using fal_client.

        Requires fal_client to be installed for path/PIL image upload.

        Args:
            client: Optional API client, not used in this implementation
                   as we directly use fal_client for uploads.

        Returns:
            URL string to the image.

        Raises:
            ImportError: If fal_client is not installed.
            ValueError: If no valid input is provided.
        """
        if self.url:
            return self.url

        if not fal_client:
            msg = (
                "fal_client package is required to upload local files or PIL images."
                " Please install it (`uv pip install fal-client`) or provide a URL."
            )
            raise ImportError(msg)

        if self.path:
            return await fal_client.upload_file_async(self.path)
        elif self.pil_image:
            # For PIL images, save to a temporary file first
            with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as tmp:
                tmp_path = Path(tmp.name)
                try:
                    self.pil_image.save(tmp_path, format="JPEG", quality=95)
                    # logger.debug(f"Saved PIL image to temp file {tmp_path} for upload") # Requires logger
                    upload_url = await fal_client.upload_file_async(tmp_path)
                finally:
                    # Ensure temporary file is deleted even if upload fails
                    try:
                        tmp_path.unlink()
                        # logger.debug(f"Deleted temporary PIL image file {tmp_path}") # Requires logger
                    except OSError:
                        # logger.warning(f"Could not delete temp file {tmp_path}: {e}") # Requires logger
                        pass  # Log warning, but don't fail the operation
                return upload_url
        else:
            # This should not happen if is_valid is checked before calling
            msg = "No valid image input (URL, Path, PIL Image) provided to convert to URL."
            raise ValueError(msg)
</file>

<file path="src/twat_genai/engines/fal/outpaint.py">
# this_file: src/twat_genai/engines/fal/outpaint.py
"""Handles image outpainting using the FAL BRIA model."""

from __future__ import annotations

from typing import TYPE_CHECKING
from pathlib import Path
from loguru import logger

from twat_genai.core.config import ImageResult
from twat_genai.core.image_utils import create_outpaint_mask

if TYPE_CHECKING:
    from twat_genai.engines.fal.client import FalApiClient
    from twat_genai.engines.fal.config import OutpaintConfig


async def run_outpaint(
    client: FalApiClient,
    config: OutpaintConfig,
    original_width: int,
    original_height: int,
    output_dir: Path | None = None,
    filename_suffix: str | None = None,
    filename_prefix: str | None = None,
) -> ImageResult:
    """
    Handle image outpainting based on the configuration, choosing the appropriate outpaint tool.

    Args:
        client: The FalApiClient instance to use for API calls.
        config: The OutpaintConfig object containing the configuration.
        original_width: The width of the original input image.
        original_height: The height of the original input image.
        output_dir: Directory to save the result image and metadata.
        filename_suffix: Optional suffix for generated filenames.
        filename_prefix: Optional prefix for generated filenames.

    Returns:
        An ImageResult object containing information about the generated image.

    Raises:
        ValueError: If required parameters are missing or invalid.
        RuntimeError: If the image upload or API call fails.
    """
    logger.info(
        f"Starting outpaint process with {config.outpaint_tool} tool, "
        f"target size {config.target_width}x{config.target_height}"
    )

    # Choose the appropriate outpainting implementation based on the tool
    if config.outpaint_tool == "flux":
        return await run_flux_outpaint(
            client=client,
            config=config,
            original_width=original_width,
            original_height=original_height,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
        )
    elif config.outpaint_tool == "bria":
        # Use the original implementation for bria
        # The validation of dimensions is handled in both implementations
        if (
            original_width > config.target_width
            or original_height > config.target_height
        ):
            msg = (
                f"Original image ({original_width}x{original_height}) cannot be larger "
                f"than target size ({config.target_width}x{config.target_height}) for outpainting."
            )
            logger.error(msg)
            raise ValueError(msg)

        try:
            # Calculate where the original image goes in the target
            offset_x = (config.target_width - original_width) // 2
            offset_y = (config.target_height - original_height) // 2
            original_image_location = [offset_x, offset_y]
            original_image_size = [original_width, original_height]

            # Get the image URL
            image_url = await config.input_image.to_url(client)
            logger.info(f"Using image URL for bria outpaint: {image_url}")

            # Assemble arguments for the outpaint API
            api_args = {
                "image_url": image_url,
                "prompt": config.prompt,
                "original_image_location": original_image_location,
                "original_image_size": original_image_size,
                "canvas_size": [config.target_width, config.target_height],
                "num_outputs": config.num_images,
            }

            # Call the client process_outpaint method
            logger.debug(f"Calling BRIA outpaint API with arguments: {api_args}")
            return await client.process_outpaint(
                prompt=config.prompt,
                image_url=image_url,
                lora_spec=None,  # No LoRA for BRIA
                output_dir=output_dir,
                filename_suffix=filename_suffix,
                filename_prefix=filename_prefix,
                outpaint_tool="bria",
                target_width=config.target_width,
                target_height=config.target_height,
                original_image_location=original_image_location,
                original_image_size=original_image_size,
                num_images=config.num_images,
            )
        except Exception as e:
            logger.error(f"Outpaint process failed: {e}", exc_info=True)
            msg = f"Outpaint process failed: {e}"
            raise RuntimeError(msg) from e
    else:
        # Should never happen due to validation in OutpaintConfig
        msg = f"Invalid outpaint tool: {config.outpaint_tool}. Valid options are 'bria' or 'flux'."
        logger.error(msg)
        raise ValueError(msg)


async def run_flux_outpaint(
    client: FalApiClient,
    config: OutpaintConfig,
    original_width: int,
    original_height: int,
    output_dir: Path | None = None,
    filename_suffix: str | None = None,
    filename_prefix: str | None = None,
) -> ImageResult:
    """
    Uses the flux-lora/inpainting endpoint to perform outpainting by creating
    a mask image where the original image area is white and the rest is black.

    Args:
        client: The FalApiClient instance.
        config: The OutpaintConfig object containing specific parameters.
        original_width: The width of the original input image.
        original_height: The height of the original input image.
        output_dir: Directory to save the result image and metadata.
        filename_suffix: Optional suffix for generated filenames.
        filename_prefix: Optional prefix for generated filenames.

    Returns:
        An ImageResult object containing information about the generated image.

    Raises:
        ValueError: If required parameters are missing or invalid.
        RuntimeError: If the image upload or API call fails.
    """
    logger.info(
        f"Starting flux outpaint process with target size {config.target_width}x{config.target_height}"
    )

    # 1. Validate dimensions
    if original_width > config.target_width or original_height > config.target_height:
        msg = (
            f"Original image ({original_width}x{original_height}) cannot be larger "
            f"than target size ({config.target_width}x{config.target_height}) for outpainting."
        )
        logger.error(msg)
        raise ValueError(msg)

    # 2. Create a mask image (black with white rectangle where original image goes)
    mask_path, original_image_location = create_outpaint_mask(
        image_width=original_width,
        image_height=original_height,
        target_width=config.target_width,
        target_height=config.target_height,
    )

    # 3. Get the image URLs
    try:
        image_url = await config.input_image.to_url(client)
        logger.info(f"Using image URL for flux outpaint: {image_url}")

        # Upload the mask image using client.upload_image (previously upload_file)
        mask_url = await client.upload_image(mask_path)
        logger.info(f"Using mask URL for flux outpaint: {mask_url}")
    except Exception as e:
        logger.error(f"Failed to get image or mask URL: {e}", exc_info=True)
        msg = f"Failed to prepare images for flux outpainting: {e}"
        raise RuntimeError(msg) from e

    # 4. Call the client's process_outpaint method with flux as the outpaint_tool
    logger.info("Calling client.process_outpaint for flux outpainting")
    try:
        # Use the new client API that handles outpainting properly
        result = await client.process_outpaint(
            prompt=config.prompt,
            image_url=image_url,
            lora_spec=None,  # No LoRA for this call - we'll let the client handle it
            output_dir=output_dir,
            filename_suffix=filename_suffix or "_flux_outpaint",
            filename_prefix=filename_prefix,
            outpaint_tool="flux",  # Use the flux outpaint tool
            target_width=config.target_width,
            target_height=config.target_height,
            # Additional parameters specific to flux outpainting
            mask_url=mask_url,
            guidance_scale=config.guidance_scale,
            num_inference_steps=config.num_inference_steps,
            negative_prompt=config.negative_prompt,
            enable_safety_checker=config.enable_safety_checker,
            num_images=config.num_images,
        )
        logger.success(f"Flux outpaint job completed. Request ID: {result.request_id}")
        return result
    except Exception as e:
        logger.error(f"FalApiClient.process_outpaint failed: {e}", exc_info=True)
        msg = f"Flux outpaint process failed: {e}"
        raise RuntimeError(msg) from e
</file>

<file path="src/twat_genai/engines/fal/upscale.py">
# this_file: src/twat_genai/engines/fal/upscale.py
"""Handles image upscaling using various FAL models."""

from __future__ import annotations

from dataclasses import dataclass  # Keep dataclass if needed later
from typing import Any, TYPE_CHECKING
from pathlib import Path
from loguru import logger

# from fal.config import ModelTypes  # Import the unified ModelTypes
from twat_genai.engines.fal.config import ModelTypes  # Corrected import
from twat_genai.core.config import ImageResult

if TYPE_CHECKING:
    from twat_genai.engines.fal.client import FalApiClient
    from twat_genai.engines.fal.config import UpscaleConfig

# TODO: Re-evaluate if a separate enum is needed or if ModelTypes suffices.
# For now, keep the mapping structure for parameters.


# Mapping of upscale ModelTypes to their default parameters
# Based on superres/tools.py TOOL_DEFAULT_PARAMS
UPSCALE_TOOL_DEFAULT_PARAMS: dict[ModelTypes, dict[str, Any]] = {
    ModelTypes.UPSCALER_DRCT: {"upscaling_factor": 4},
    ModelTypes.UPSCALER_IDEOGRAM: {"resemblance": 50, "detail": 50},
    ModelTypes.UPSCALER_RECRAFT_CREATIVE: {"sync_mode": True},
    ModelTypes.UPSCALER_RECRAFT_CLARITY: {"sync_mode": True},
    ModelTypes.UPSCALER_CCSR: {"scale": 2, "steps": 50, "color_fix_type": "adain"},
    ModelTypes.UPSCALER_ESRGAN: {
        "model": "RealESRGAN_x4plus",
        "scale": 2,
        "face": False,
        "tile": 0,
    },
    ModelTypes.UPSCALER_AURA_SR: {
        "model_type": "SD_1_5",
        "scale": 2,
        "creativity": 0.5,
        "detail": 1.0,
        "shape_preservation": 0.25,
        "prompt_suffix": " high quality, highly detailed, high resolution, sharp",
        "negative_prompt": "blurry, low resolution, bad, ugly, low quality",
    },
    ModelTypes.UPSCALER_CLARITY: {
        "scale": 2,
        "creativity": 0.35,
        "resemblance": 0.6,
        "prompt": "masterpiece, best quality, highres",
        "negative_prompt": "(worst quality, low quality, normal quality:2)",
    },
}

# Maximum input image dimensions for each tool
# Based on superres/tools.py TOOL_MAX_INPUT_SIZES
UPSCALE_TOOL_MAX_INPUT_SIZES: dict[ModelTypes, int] = {
    ModelTypes.UPSCALER_DRCT: 2048,
    ModelTypes.UPSCALER_IDEOGRAM: 1024,
    ModelTypes.UPSCALER_RECRAFT_CREATIVE: 2048,
    ModelTypes.UPSCALER_RECRAFT_CLARITY: 2048,
    ModelTypes.UPSCALER_CCSR: 2048,
    ModelTypes.UPSCALER_ESRGAN: 2048,
    ModelTypes.UPSCALER_AURA_SR: 1024,
    ModelTypes.UPSCALER_CLARITY: 1024,
}


# Dataclasses for specific tool parameters (e.g., Fooocus)
@dataclass
class ImagePrompt:
    """Represents an image prompt for Fooocus upscaling"""

    type: str = "ImagePrompt"
    image_url: str = ""
    stop_at: float = 0.5
    weight: float = 1.0


@dataclass
class LoraWeight:
    """Represents a LoRA weight configuration"""

    path: str
    scale: float = 0.1


async def run_upscale(
    client: FalApiClient,
    config: UpscaleConfig,
    model_type: ModelTypes,
    output_dir: Path | None = None,
    filename_suffix: str | None = None,
    filename_prefix: str | None = None,
) -> ImageResult:
    """
    Assembles parameters from UpscaleConfig and defaults, then calls the appropriate
    FalApiClient method to perform image upscaling.

    Args:
        client: The FalApiClient instance.
        config: The UpscaleConfig object containing specific parameters.
        model_type: The specific upscale model to use.
        output_dir: Directory to save the result image and metadata.
        filename_suffix: Optional suffix for generated filenames.
        filename_prefix: Optional prefix for generated filenames.

    Returns:
        An ImageResult object containing information about the generated image.

    Raises:
        ValueError: If the model_type is not a valid upscale model.
        RuntimeError: If the image upload or API call fails.
    """
    if not model_type.name.startswith("UPSCALER_"):
        msg = f"Invalid model type provided to run_upscale: {model_type}"
        logger.error(msg)
        raise ValueError(msg)

    # 1. Get the default parameters for the model type
    default_params = UPSCALE_TOOL_DEFAULT_PARAMS.get(model_type, {})
    logger.debug(f"Default params for {model_type.name}: {default_params}")

    # 2. Prepare arguments from UpscaleConfig, excluding None values and input_image
    config_args = {
        k: v
        for k, v in config.model_dump(exclude={"input_image"}).items()
        if v is not None
    }
    logger.debug(f"Config args (non-None): {config_args}")

    # 3. Remap config keys to FAL API expected keys if necessary
    #    (Currently mapping based on UpscaleConfig field names)
    #    This requires careful mapping between UpscaleConfig fields and FAL API params.
    #    Example mapping (adjust as needed based on actual API requirements):
    api_args_from_config = {
        "prompt": config_args.get("prompt"),
        "negative_prompt": config_args.get("negative_prompt"),
        "seed": config_args.get("seed"),
        # "scale": config_args.get("scale"),  # General scale - Handled below for CCSR
        # Ideogram
        "resemblance": config_args.get("ideogram_resemblance"),
        "detail": config_args.get("ideogram_detail"),
        "expand_prompt": config_args.get("ideogram_expand_prompt"),
        # Recraft
        "sync_mode": config_args.get("recraft_sync_mode"),
        # Fooocus (Assuming FAL model takes these directly - CHECK API)
        "styles": config_args.get("fooocus_styles"),
        "performance": config_args.get("fooocus_performance"),
        "guidance_scale": config_args.get("fooocus_guidance_scale"),
        "sharpness": config_args.get("fooocus_sharpness"),
        "uov_method": config_args.get("fooocus_uov_method"),
        # ESRGAN
        "model": config_args.get("esrgan_model"),
        "tile": config_args.get("esrgan_tile"),
        "face": config_args.get("esrgan_face"),
        # Clarity / Aura SR
        "creativity": config_args.get("clarity_creativity"),
        # "resemblance": config_args.get("clarity_resemblance"), # REMOVED - Duplicate key with Ideogram
        # "guidance_scale": config_args.get("clarity_guidance_scale"), # Duplicate key?
        # "num_inference_steps": config_args.get("clarity_num_inference_steps"), # Duplicate key?
        # CCSR
        "scale": config_args.get("ccsr_scale")
        if config_args.get("ccsr_scale") is not None
        else config_args.get("scale"),  # Use general scale if ccsr_scale not set
        "tile_diffusion": config_args.get("ccsr_tile_diffusion"),
        "color_fix_type": config_args.get("ccsr_color_fix_type"),
        "steps": config_args.get("ccsr_steps"),
    }
    # Remove None values after mapping
    api_args_from_config = {
        k: v for k, v in api_args_from_config.items() if v is not None
    }
    logger.debug(f"API args from config (mapped & non-None): {api_args_from_config}")

    # 4. Merge defaults and config arguments, prioritizing config
    #    Start with defaults, update with mapped config args
    final_api_args = default_params.copy()
    final_api_args.update(api_args_from_config)
    logger.debug(f"Final API args merged: {final_api_args}")

    # 5. Get the image URL (upload if necessary)
    #    The config.input_image is already a FALImageInput which handles upload
    try:
        image_url = await config.input_image.to_url(client)  # Pass client for upload
        logger.info(f"Using image URL for upscale: {image_url}")
    except Exception as e:
        logger.error(f"Failed to get image URL: {e}", exc_info=True)
        msg = f"Failed to get image URL: {e}"
        raise RuntimeError(msg) from e

    # 6. Call the client's process_upscale method
    logger.info(f"Calling client.process_upscale for {model_type.name}")
    try:
        result = await client.process_upscale(
            model_type=model_type,
            image_url=image_url,
            output_dir=output_dir,
            filename_suffix=filename_suffix,
            filename_prefix=filename_prefix,
            **final_api_args,  # Pass the merged arguments
        )
        logger.success(
            f"Upscale job completed for {model_type.name}. Request ID: {result.request_id}"
        )
        return result
    except Exception as e:
        logger.error(f"FalApiClient.process_upscale failed: {e}", exc_info=True)
        msg = f"Upscale process failed: {e}"
        raise RuntimeError(msg) from e
</file>

<file path="src/twat_genai/engines/__init__.py">
"""Engines for interacting with various AI platforms."""
</file>

<file path="src/twat_genai/engines/base.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic"]
# ///
"""Base interface for image generation engines."""

from __future__ import annotations

from abc import ABC, abstractmethod
from typing import Any

from pydantic import BaseModel

# Import core types used by EngineConfig and the ABC
from twat_genai.core.image import ImageSizes
from twat_genai.core.config import ImageResult, ImageSizeWH


class EngineConfig(BaseModel):
    """Base configuration for image generation engines.

    This class defines the common configuration parameters applicable
    across different image generation engines.
    """

    guidance_scale: float = 3.5
    num_inference_steps: int = 28
    # Use the Union type alias directly
    image_size: ImageSizes | ImageSizeWH = ImageSizes.SQ
    enable_safety_checker: bool = False


class ImageGenerationEngine(ABC):
    """Abstract base class for image generation engines."""

    @abstractmethod
    async def initialize(self) -> None:
        """Initialize the engine and any required resources."""

    @abstractmethod
    async def generate(
        self, prompt: str, config: EngineConfig, **kwargs: Any
    ) -> ImageResult:
        """
        Generate an image from the given prompt and configuration.

        Args:
            prompt: Text prompt for image generation
            config: An instance of EngineConfig defined in this module.
            **kwargs: Additional engine-specific parameters (e.g., image_config,
                      upscale_config, lora_spec for specific engines).

        Returns:
            Generated image result (ImageResult object from core.config)
        """

    @abstractmethod
    async def shutdown(self) -> None:
        """Clean up resources and shut down the engine."""

    async def __aenter__(self) -> ImageGenerationEngine:
        """Context manager entry."""
        await self.initialize()
        return self

    async def __aexit__(self, exc_type: Any, exc_val: Any, exc_tb: Any) -> None:
        """Context manager exit."""
        await self.shutdown()
</file>

<file path="src/twat_genai/__init__.py">
"""Main package for twat-genai."""

from importlib.metadata import PackageNotFoundError, version

from twat_genai.cli import TwatGenAiCLI
from twat_genai.core.config import ImageInput, ImageResult, ImageSizeWH
from twat_genai.core.image import ImageFormats, ImageSizes
from twat_genai.engines.base import EngineConfig, ImageGenerationEngine
from twat_genai.engines.fal import FALEngine
from twat_genai.engines.fal.config import (
    ImageToImageConfig,
    ModelTypes,
    OutpaintConfig,
    UpscaleConfig,
)

try:
    __version__ = version("twat-genai")
except PackageNotFoundError:
    # package is not installed
    __version__ = "0.0.0"

__all__ = [
    "EngineConfig",
    "FALEngine",
    "ImageFormats",
    "ImageGenerationEngine",
    "ImageInput",
    "ImageResult",
    "ImageSizeWH",
    "ImageSizes",
    "ImageToImageConfig",
    "ModelTypes",
    "OutpaintConfig",
    "TwatGenAiCLI",
    "UpscaleConfig",
    "__version__",
]
</file>

<file path="src/twat_genai/__main___loras.json">
{
  "gstdrw style": [
    {
      "url": "glif/Gesture-Draw",
      "scale": 1.0
    }
  ],
  "Sketch Smudge": [
    {
      "url": "strangerzonehf/Flux-Sketch-Smudge-LoRA",
      "scale": 1.0
    }
  ],
  "2color illustration": [
    {
      "url": "strangerzonehf/2Color-Illustration",
      "scale": 1.2
    }
  ],
  "shou_xin": [
    {
      "url": "hassanelmghari/shou_xin",
      "scale": 1.0
    }
  ],
  "shou_xin, Simple Pencil sketch": [
    {
      "url": "hassanelmghari/shou_xin",
      "scale": 0.8
    },
    {
      "url": "prithivMLmods/Super-Pencil-Flux-LoRA",
      "scale": 1.2
    }
  ],
  "shou_xin, gstdrw style sketch": [
    {
      "url": "hassanelmghari/shou_xin",
      "scale": 2.0
    },
    {
      "url": "glif/Gesture-Draw",
      "scale": 0.4
    }
  ],
  "in the style of TOK a trtcrd tarot style": [
    {
      "url": "multimodalart/flux-tarot-v1",
      "scale": 1.0
    }
  ],
  "shou_xin, Sketch Smudge": [
    {
      "url": "hassanelmghari/shou_xin",
      "scale": 1.0
    },
    {
      "url": "strangerzonehf/Flux-Sketch-Smudge-LoRA",
      "scale": 0.5
    }
  ]
}
</file>

<file path="src/twat_genai/__main__.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["fire", "loguru"]
# ///
# this_file: src/twat_genai/__main__.py

"""
Main entry point for the twat-genai CLI when run as a module (`python -m twat_genai`).
Delegates directly to the main CLI class defined in `cli.py`.
"""

import sys
import fire
from loguru import logger

# Import the actual CLI class from the dedicated module
from twat_genai.cli import TwatGenAiCLI

# Configure logging minimally for the entry point
# The main configuration happens within TwatGenAiCLI.__init__
logger.remove()
logger.add(sys.stderr, level="WARNING")  # Default level


if __name__ == "__main__":
    logger.debug("Running twat-genai via __main__.py entry point")
    fire.Fire(TwatGenAiCLI)
</file>

<file path="src/twat_genai/cli.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["fire", "loguru", "twat"]
# ///
"""Command-line interface for twat-genai."""

from __future__ import annotations

import sys
from pathlib import Path
from typing import get_args, Any, cast, Literal

import fire
from loguru import logger

# Try importing PathManager, handle optional dependency
try:
    from twat_os.paths import PathManager
except ImportError:
    PathManager = None
    logger.warning(
        "'twat' package not fully available. PathManager features disabled."
        " Output directory resolution will use defaults."
    )

from twat_genai.core.config import ImageInput, ImageResult, ImageSizeWH
from twat_genai.core.image import ImageSizes
from twat_genai.core.prompt import normalize_prompts
from twat_genai.engines.base import EngineConfig
from twat_genai.engines.fal import FALEngine
from twat_genai.engines.fal.config import (
    ImageToImageConfig,
    ModelTypes,
    UpscaleConfig,
    OutpaintConfig,
)

# Helper to map upscale tool names to ModelTypes
UPSCALE_TOOL_MAP = {
    tool.name.replace("UPSCALER_", "").lower(): tool
    for tool in ModelTypes
    if tool.name.startswith("UPSCALER_")
}


def parse_image_size(size_str: str) -> ImageSizes | ImageSizeWH:
    """Parse image size string into appropriate type."""
    try:
        return ImageSizes[size_str.upper()]
    except KeyError:
        if "," in size_str:
            try:
                w, h = (int(x.strip()) for x in size_str.split(",", 1))
                return ImageSizeWH(width=w, height=h)
            except (ValueError, TypeError) as err:
                msg = "For custom image sizes use 'width,height' with integers."
                raise ValueError(msg) from err
        valid_names = ", ".join(s.name for s in ImageSizes)
        msg = f"image_size must be one of: {valid_names} or in 'width,height' format."
        raise ValueError(msg) from None


def get_output_dir(
    user_dir: str | Path | None = None,
    subdirectory: str | None = None,
    input_image_path: Path | None = None,
) -> Path:
    """Get the output directory for generated images.

    Priority:
    1. User-provided directory
    2. Input image parent directory + basename subfolder (if input_image_path is provided)
    3. Central path management (if twat is installed)
    4. Default 'generated_images' in current directory

    Args:
        user_dir: Base output directory path
        subdirectory: Optional subdirectory inside the output directory
        input_image_path: Input image path to use for determining default output dir

    Returns:
        Path: Resolved output directory path
    """
    if user_dir:
        base_dir = Path(user_dir).resolve()
    elif input_image_path:
        # Create output in a subfolder named after the input image's basename
        # in the same directory as the input image
        parent_dir = input_image_path.parent
        basename = input_image_path.stem
        base_dir = (parent_dir / basename).resolve()
    elif PathManager:
        try:
            paths = PathManager.for_package("twat_genai")
            if paths.genai.output_dir:
                base_dir = paths.genai.output_dir.resolve()
            else:
                # Default to 'generated_images' in current directory
                base_dir = Path("generated_images").resolve()
        except (AttributeError, Exception) as e:
            logger.warning(
                f"Error using PathManager for output directory: {e}. Using default directory."
            )
            # Default to 'generated_images' in current directory
            base_dir = Path("generated_images").resolve()
    else:
        # Default to 'generated_images' in current directory
        base_dir = Path("generated_images").resolve()

    # Handle subdirectory if provided
    if subdirectory:
        return base_dir / subdirectory
    return base_dir


class TwatGenAiCLI:
    """Command-line interface for twat-genai.

    This class provides a structured interface for various image generation tasks
    through dedicated methods. Each public method represents a subcommand.

    Shared parameters are defined in __init__ and are available across all commands.
    """

    def __init__(
        self,
        output_dir: str | Path = "generated_images",
        filename_suffix: str | None = None,
        filename_prefix: str | None = None,
        verbose: bool = False,
        image_size: str = "SQ",
        guidance_scale: float = 3.5,
        num_inference_steps: int = 28,
        negative_prompt: str = "",
        lora: str | None = None,
    ):
        """Initialize shared parameters for all commands.

        Args:
            output_dir: Directory to save generated images
            filename_suffix: Optional suffix for generated filenames
            filename_prefix: Optional prefix for generated filenames
            verbose: Enable verbose logging
            image_size: Output image size preset or custom dimensions
            guidance_scale: Guidance scale for generation
            num_inference_steps: Number of inference steps
            negative_prompt: Negative prompt for generation
            lora: LoRA specification string
        """
        # Configure logging
        logger.remove()
        logger.add(sys.stderr, level="DEBUG" if verbose else "WARNING")
        logger.debug(f"CLI initialized with shared params: {locals()}")

        # Store shared parameters
        self.verbose = verbose
        self.output_dir = output_dir
        self.filename_suffix = filename_suffix
        self.filename_prefix = filename_prefix
        self.image_size = image_size
        self.guidance_scale = guidance_scale
        self.num_inference_steps = num_inference_steps
        self.negative_prompt = negative_prompt
        self.lora = lora

        # Prepare base config
        self.base_config = EngineConfig(
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
            image_size=parse_image_size(image_size),
        )

    def _prepare_image_input(self, input_image: str | None) -> ImageInput | None:
        """Helper to parse and validate image input."""
        if not input_image:
            return None

        try:
            is_url = isinstance(input_image, str) and input_image.startswith(
                ("http:", "https:")
            )
            input_img_obj = ImageInput(
                url=input_image if is_url else None,
                path=Path(input_image) if not is_url else None,
            )
            if input_img_obj.path and not input_img_obj.path.exists():
                msg = f"Input image path does not exist: {input_img_obj.path}"
                raise FileNotFoundError(msg)
            logger.debug(f"Parsed input image: {input_img_obj}")

            # If no filename_prefix is specified and we have a local file path,
            # use the input image's basename as the prefix
            if self.filename_prefix is None and input_img_obj.path:
                self.filename_prefix = input_img_obj.path.stem
                logger.debug(
                    f"Using input image basename as filename prefix: {self.filename_prefix}"
                )

            return input_img_obj
        except Exception as e:
            logger.error(
                f"Failed to parse input_image argument '{input_image}': {e}",
                exc_info=True,
            )
            msg = f"Invalid input_image source: '{input_image}'. {e}"
            raise ValueError(msg) from e

    async def _run_generation(
        self,
        prompts: str | list[str],
        model: ModelTypes,
        image_config: ImageToImageConfig | None = None,
        upscale_config: UpscaleConfig | None = None,
        outpaint_config: OutpaintConfig | None = None,
        output_subdirectory: str | None = None,
        input_image_path: Path | None = None,
    ) -> list[ImageResult]:
        """Core generation logic shared across commands."""
        # Use input_image_path to determine output directory if not explicitly provided by user
        output_dir_path = get_output_dir(
            self.output_dir, output_subdirectory, input_image_path
        )
        output_dir_path.mkdir(parents=True, exist_ok=True)

        final_prompts = normalize_prompts(prompts)
        logger.debug(f"Expanded prompts: {final_prompts}")

        # Ensure at least one prompt exists for upscaling and other modes
        # that can work with empty prompts
        if not final_prompts:
            if model.name.startswith("UPSCALER_"):
                logger.debug(f"Using empty string as default prompt for {model.name}")
                final_prompts = [""]  # Use a single empty string prompt

        # If we still have no prompts, warn but don't fail (some models may work without prompts)
        if not final_prompts:
            logger.warning(f"No prompts available for model {model.name}")
            final_prompts = [""]  # Fallback to empty string

        async with FALEngine(output_dir_path) as engine:
            results = []
            for prompt in final_prompts:
                kwargs = {
                    "model": model,
                    "image_config": image_config,
                    "upscale_config": upscale_config,
                    "outpaint_config": outpaint_config,
                    "lora_spec": self.lora,
                    "filename_suffix": self.filename_suffix,
                    "filename_prefix": self.filename_prefix,
                    "verbose": self.verbose,
                }
                result = await engine.generate(prompt, self.base_config, **kwargs)
                results.append(result)

        return results

    def text(self, prompts: str | list[str] = "", output: str | None = None) -> None:
        """Generate images from text prompts.

        Args:
            prompts: One or more text prompts for generation
            output: Optional subdirectory for output files
        """
        if not prompts:
            msg = "Prompts are required for text generation."
            raise ValueError(msg)

        import asyncio

        results = asyncio.run(
            self._run_generation(prompts, ModelTypes.TEXT, output_subdirectory=output)
        )
        self._print_results(results)

    def image(
        self,
        input_image: str,
        prompts: str | list[str] = "",
        strength: float = 0.75,
        output: str | None = None,
    ) -> None:
        """Generate images using an existing image as a reference.

        Args:
            input_image: Path or URL to the source image
            prompts: One or more text prompts for generation
            strength: How much influence the prompt should have over the image (0-1)
            output: Optional subdirectory for output files
        """
        import asyncio

        input_img = self._prepare_image_input(input_image)
        if not input_img:
            msg = "Invalid input_image source."
            raise ValueError(msg)

        # Create I2I config
        i2i_config = ImageToImageConfig(
            input_image=input_img,
            strength=strength,
            negative_prompt=self.negative_prompt,
            model_type=ModelTypes.IMAGE,
        )

        results = asyncio.run(
            self._run_generation(
                prompts,
                ModelTypes.IMAGE,
                image_config=i2i_config,
                output_subdirectory=output,
                input_image_path=input_img.path,
            )
        )
        self._print_results(results)

    def canny(
        self,
        input_image: str,
        prompts: str | list[str] = "",
        output: str | None = None,
    ) -> None:
        """Generate images using the Canny edge detection version of the input image.

        Args:
            input_image: Path or URL to the source image
            prompts: One or more text prompts for generation
            output: Optional subdirectory for output files
        """
        import asyncio

        input_img = self._prepare_image_input(input_image)
        if not input_img:
            msg = "Invalid input_image source."
            raise ValueError(msg)

        # Create I2I config with strength=1.0 for controlnet
        i2i_config = ImageToImageConfig(
            input_image=input_img,
            strength=1.0,  # Typical for controlnet
            negative_prompt=self.negative_prompt,
            model_type=ModelTypes.CANNY,
        )

        results = asyncio.run(
            self._run_generation(
                prompts,
                ModelTypes.CANNY,
                image_config=i2i_config,
                output_subdirectory=output,
                input_image_path=input_img.path,
            )
        )
        self._print_results(results)

    def depth(
        self,
        input_image: str,
        prompts: str | list[str] = "",
        output: str | None = None,
    ) -> None:
        """Generate images using the depth map of the input image.

        Args:
            input_image: Path or URL to the source image
            prompts: One or more text prompts for generation
            output: Optional subdirectory for output files
        """
        import asyncio

        input_img = self._prepare_image_input(input_image)
        if not input_img:
            msg = "Invalid input_image source."
            raise ValueError(msg)

        # Create I2I config with strength=1.0 for controlnet
        i2i_config = ImageToImageConfig(
            input_image=input_img,
            strength=1.0,  # Typical for controlnet
            negative_prompt=self.negative_prompt,
            model_type=ModelTypes.DEPTH,
        )

        results = asyncio.run(
            self._run_generation(
                prompts,
                ModelTypes.DEPTH,
                image_config=i2i_config,
                output_subdirectory=output,
                input_image_path=input_img.path,
            )
        )
        self._print_results(results)

    def upscale(
        self,
        input_image: str,
        tool: Literal[
            "aura_sr",
            "ccsr",
            "clarity",
            "drct",
            "esrgan",
            "ideogram",
            "recraft_clarity",
            "recraft_creative",
        ],
        prompts: str | list[str] = "",
        scale: float | None = None,
        resemblance: float | None = None,
        ideogram_detail: int | None = None,
        esrgan_model: str | None = None,
        esrgan_tile: int | None = None,
        clarity_creativity: float | None = None,
        clarity_guidance_scale: float | None = None,
        clarity_num_inference_steps: int | None = None,
        ccsr_scale: int | None = None,
        ccsr_tile_diffusion: str | None = None,
        ccsr_color_fix_type: str | None = None,
        ccsr_steps: int | None = None,
        output: str | None = None,
    ) -> None:
        """Upscale an image using one of several available models.

        Args:
            input_image: Path or URL to the source image
            tool: Which upscaling model/algorithm to use
            prompts: Optional prompt for models that support it (clarity, etc.)
            scale: Scale factor for upscaling (model dependent)
            resemblance: How closely to match original (0.0-1.0, model dependent)
            ideogram_detail: Detail level for ideogram (1-5)
            esrgan_model: ESRGAN model name
            esrgan_tile: Tile size for ESRGAN
            clarity_creativity: Creativity for clarity upscaling (0.0-1.0)
            clarity_guidance_scale: Guidance scale for clarity
            clarity_num_inference_steps: Number of steps for clarity
            ccsr_scale: Scale factor for CCSR (2, 3, or 4)
            ccsr_tile_diffusion: Tile diffusion mode for CCSR
            ccsr_color_fix_type: Color fix type for CCSR
            ccsr_steps: Number of steps for CCSR
            output: Optional subdirectory for output files
        """
        import asyncio

        # Get upscaler from tool argument
        upscaler = UPSCALE_TOOL_MAP.get(tool.lower())
        if not upscaler:
            valid_tools = ", ".join(UPSCALE_TOOL_MAP.keys())
            msg = f"Invalid upscale tool: {tool}. Valid options: {valid_tools}"
            raise ValueError(msg)

        # Parse and validate input image
        input_img = self._prepare_image_input(input_image)
        if not input_img:
            msg = "Invalid input_image source."
            raise ValueError(msg)

        # Create UpscaleConfig based on the selected tool
        upscale_kwargs = {}

        # DRCT params
        if tool == "drct" and scale is not None:
            upscale_kwargs["scale"] = scale

        # Ideogram params
        elif tool == "ideogram":
            if ideogram_detail is not None:
                upscale_kwargs["detail"] = ideogram_detail

        # Recraft params (common between clarity/creative)
        elif tool in ("recraft_clarity", "recraft_creative"):
            if scale is not None:
                upscale_kwargs["scale"] = scale

        # ESRGAN params
        elif tool == "esrgan":
            if esrgan_model is not None:
                upscale_kwargs["model"] = esrgan_model
            if esrgan_tile is not None:
                upscale_kwargs["tile"] = esrgan_tile

        # Aura SR params
        elif tool == "aura_sr":
            if scale is not None:
                upscale_kwargs["scale"] = scale
            if resemblance is not None:
                upscale_kwargs["resemblance"] = resemblance

        # Clarity params
        elif tool == "clarity":
            if clarity_creativity is not None:
                upscale_kwargs["creativity"] = clarity_creativity
            if clarity_guidance_scale is not None:
                upscale_kwargs["guidance_scale"] = clarity_guidance_scale
            if clarity_num_inference_steps is not None:
                upscale_kwargs["num_inference_steps"] = clarity_num_inference_steps

        # CCSR params
        elif tool == "ccsr":
            if ccsr_scale is not None:
                upscale_kwargs["scale"] = ccsr_scale
            if ccsr_tile_diffusion is not None:
                upscale_kwargs["tile_diffusion"] = ccsr_tile_diffusion
            if ccsr_color_fix_type is not None:
                upscale_kwargs["color_fix_type"] = ccsr_color_fix_type
            if ccsr_steps is not None:
                upscale_kwargs["steps"] = ccsr_steps

        upscale_config = UpscaleConfig(input_image=input_img, **upscale_kwargs)

        results = asyncio.run(
            self._run_generation(
                prompts,
                upscaler,
                upscale_config=upscale_config,
                output_subdirectory=output,
                input_image_path=input_img.path,
            )
        )
        self._print_results(results)

    def outpaint(
        self,
        input_image: str,
        prompts: str | list[str],
        target_width: int,
        target_height: int,
        tool: Literal["bria", "flux"] = "bria",
        guidance_scale: float | None = None,
        num_inference_steps: int | None = None,
        enable_safety_checker: bool | None = None,
        num_images: int = 1,
        border: float = 5,
        output: str | None = None,
    ) -> None:
        """Expand an image's canvas using outpainting.

        Args:
            input_image: Path or URL to the source image
            prompts: Text prompts for generating the expanded areas
            target_width: Desired width of the expanded image
            target_height: Desired height of the expanded image
            tool: Outpainting tool to use ("bria" or "flux")
            guidance_scale: Override default guidance scale
            num_inference_steps: Override default number of steps
            enable_safety_checker: Enable safety checker
            num_images: Number of result images to generate
            border: Border width percentage for Bria
            output: Optional subdirectory for output files
        """
        import asyncio

        # Parse and validate input image
        input_img = self._prepare_image_input(input_image)
        if not input_img:
            msg = "Invalid input_image source."
            raise ValueError(msg)

        # Validate target dimensions
        if target_width <= 0 or target_height <= 0:
            msg = "Target dimensions must be positive integers."
            raise ValueError(msg)

        # Determine model type from tool parameter
        outpaint_tool_literal = cast(Literal["bria", "flux"], tool.lower())
        model_type = (
            ModelTypes.OUTPAINT_BRIA if tool == "bria" else ModelTypes.OUTPAINT_FLUX
        )

        # Prepare kwargs for OutpaintConfig
        outpaint_kwargs = {
            "input_image": input_img,
            "prompt": prompts if isinstance(prompts, str) else "; ".join(prompts),
            "target_width": target_width,
            "target_height": target_height,
            "outpaint_tool": outpaint_tool_literal,
            "num_images": num_images,
            "negative_prompt": self.negative_prompt or None,
        }

        # Add tool-specific parameters
        if tool == "bria":
            outpaint_kwargs["border_thickness_factor"] = border / 100.0
        elif tool == "flux":
            if enable_safety_checker is not None:
                outpaint_kwargs["enable_safety_checker"] = enable_safety_checker
        else:
            valid_tools = '"bria" or "flux"'
            msg = f"Invalid outpaint tool: {tool}. Valid options: {valid_tools}"
            raise ValueError(msg)

        # Override guidance_scale and num_inference_steps if provided
        if guidance_scale is not None:
            outpaint_kwargs["guidance_scale"] = guidance_scale
        if num_inference_steps is not None:
            outpaint_kwargs["num_inference_steps"] = num_inference_steps

        # Create OutpaintConfig
        outpaint_config = OutpaintConfig(**outpaint_kwargs)

        results = asyncio.run(
            self._run_generation(
                prompts,
                model_type,
                outpaint_config=outpaint_config,
                output_subdirectory=output,
                input_image_path=input_img.path,
            )
        )
        self._print_results(results)

    def _print_results(self, results: list[ImageResult]) -> None:
        """Print generation results in a consistent format."""
        if results:
            logger.info("Results:")
            for result in results:
                path = result.image_info.get("path")
                meta_path = result.image_info.get("metadata_path")
                req_id = result.request_id
                log_msg = f"  - Request ID: {req_id}\n    Image: {path or '(Not saved)'}\n    Metadata: {meta_path or '(Not saved)'}"
                logger.info(log_msg)
        else:
            logger.warning("No results were generated.")


if __name__ == "__main__":
    fire.Fire(TwatGenAiCLI)
</file>

<file path="tests/conftest.py">
# this_file: tests/conftest.py
"""Pytest fixtures for twat-genai tests."""

import pytest
from unittest.mock import AsyncMock, MagicMock


@pytest.fixture
def mock_fal_api_client(mocker):
    """Provides a mocked instance of FalApiClient."""
    # Mock the class itself
    mock_client_class = mocker.patch(
        "twat_genai.engines.fal.client.FalApiClient", autospec=True
    )

    # Create an instance of the mock class
    mock_instance = mock_client_class.return_value

    # Mock async methods used by FALEngine
    mock_instance.upload_image = AsyncMock(
        return_value="https://fake.fal.ai/uploaded_image.jpg"
    )
    mock_instance.process_upscale = AsyncMock(
        return_value=MagicMock()
    )  # Return dummy ImageResult mock
    mock_instance.process_outpaint = AsyncMock(
        return_value=MagicMock()
    )  # Return dummy ImageResult mock
    # Add mocks for process_tti, process_i2i etc. if FALEngine calls those directly later

    # Mock static method (if needed directly in tests)
    # We mock the class, so staticmethod should be mocked too, but can override
    # mocker.patch("twat_genai.engines.fal.client.FalApiClient.extract_image_info", return_value={...})

    return mock_instance


# Add other common fixtures here if needed
</file>

<file path="tests/test_cli.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pytest", "pytest-asyncio", "twat-genai"]
# ///
# this_file: tests/test_cli.py
"""Tests for the command-line interface."""

import pytest
from unittest.mock import MagicMock, AsyncMock, Mock
from pathlib import Path

# Module to test
from twat_genai import cli as cli_module

# from twat_genai import cli # New import
from twat_genai.engines.fal.config import ModelTypes
from twat_genai.engines.base import EngineConfig
from twat_genai.core.config import ImageSizeWH
from twat_genai.core.image import ImageSizes

# --- Fixtures ---


@pytest.fixture
def mock_async_main(mocker: Mock) -> AsyncMock:
    """Mocks the core async_main function."""
    return mocker.patch("twat_genai.cli.async_main", new_callable=AsyncMock)


@pytest.fixture
def mock_fal_engine(mocker: Mock) -> MagicMock:
    """Mocks the FALEngine class and its methods."""
    mock_engine_class = mocker.patch("twat_genai.engines.fal.FALEngine", autospec=True)
    mock_instance = mock_engine_class.return_value
    mock_instance.generate = AsyncMock()
    # Mock the async context manager methods
    mock_instance.__aenter__ = AsyncMock(return_value=mock_instance)
    mock_instance.__aexit__ = AsyncMock()
    return mock_instance  # Return the mocked instance


# --- Test Cases ---


@pytest.mark.asyncio
async def test_cli_text_basic(mock_fal_engine: MagicMock) -> None:
    """Test basic text-to-image call."""
    prompts = "a test prompt"
    output_dir = "test_output"

    # Call the cli function directly (it orchestrates calls to engine)
    cli_module(prompts=prompts, output_dir=output_dir, model="text")

    # Assert FALEngine was initialized and called
    mock_fal_engine.__aenter__.assert_awaited_once()

    # Check the arguments passed to engine.generate
    # We expect cli() to create configs and call generate
    # The first positional arg is prompt, the second is config
    call_args, call_kwargs = mock_fal_engine.generate.await_args
    assert call_args[0] == prompts
    assert isinstance(call_args[1], EngineConfig)
    # Check kwargs passed to generate
    assert call_kwargs.get("model") == ModelTypes.TEXT
    assert call_kwargs.get("filename_suffix") is None
    assert call_kwargs.get("filename_prefix") is None
    assert call_kwargs.get("image_config") is None
    assert call_kwargs.get("upscale_config") is None
    assert call_kwargs.get("outpaint_config") is None
    assert call_kwargs.get("lora_spec") is None

    # Check config values (defaults)
    engine_config: EngineConfig = call_args[1]
    assert engine_config.guidance_scale == 3.5
    assert engine_config.num_inference_steps == 28
    assert engine_config.image_size == ImageSizes.SQ

    mock_fal_engine.__aexit__.assert_awaited_once()


@pytest.mark.asyncio
async def test_cli_text_with_options(mock_fal_engine: MagicMock) -> None:
    """Test text-to-image call with various options."""
    prompts = ["prompt 1", "prompt 2"]
    output_dir = Path("custom_dir")
    image_size = "1024,768"
    gs = 5.0
    steps = 40
    lora = "style1:0.8"
    suffix = "_abc"
    prefix = "xyz_"
    neg = "ugly"

    cli_module(
        prompts=prompts,
        output_dir=output_dir,
        model="text",
        image_size=image_size,
        guidance_scale=gs,
        num_inference_steps=steps,
        lora=lora,
        filename_suffix=suffix,
        filename_prefix=prefix,
        negative_prompt=neg,  # Note: neg prompt goes into image_config for img models, ignored here?
        verbose=True,  # Test verbose flag
    )

    # Assert generate called twice (once per prompt)
    assert mock_fal_engine.generate.await_count == 2

    # Check the arguments of the first call
    call_args, call_kwargs = mock_fal_engine.generate.await_args_list[0]
    assert call_args[0] == prompts[0]
    assert isinstance(call_args[1], EngineConfig)
    assert call_kwargs.get("model") == ModelTypes.TEXT
    assert call_kwargs.get("filename_suffix") == suffix
    assert call_kwargs.get("filename_prefix") == prefix
    assert call_kwargs.get("lora_spec") == lora

    # Check config values for the first call
    engine_config: EngineConfig = call_args[1]
    assert engine_config.guidance_scale == gs
    assert engine_config.num_inference_steps == steps
    assert isinstance(engine_config.image_size, ImageSizeWH)
    assert engine_config.image_size.width == 1024
    assert engine_config.image_size.height == 768

    # Check second call prompt
    call_args_2, _ = mock_fal_engine.generate.await_args_list[1]
    assert call_args_2[0] == prompts[1]

    mock_fal_engine.__aenter__.assert_awaited_once()
    mock_fal_engine.__aexit__.assert_awaited_once()


def test_cli_missing_prompt_for_text() -> None:
    """Test error when prompts are missing for text model."""
    with pytest.raises(ValueError, match="Prompts are required for text model."):
        cli_module(model="text", prompts="")
    with pytest.raises(ValueError, match="Prompts are required for text model."):
        cli_module(model="text", prompts=[])


def test_cli_invalid_image_size() -> None:
    """Test error handling for invalid image_size format."""
    # Expect SystemExit because the main cli function catches ValueError and exits
    with pytest.raises(SystemExit):
        cli_module(prompts="test", image_size="invalid_size")
    with pytest.raises(SystemExit):
        cli_module(prompts="test", image_size="100x200")
    with pytest.raises(SystemExit):
        cli_module(prompts="test", image_size="100,abc")


# TODO: Add tests for image, canny, depth models
# TODO: Add tests for upscale model with various upscale args
# TODO: Add tests for outpaint model
# TODO: Add tests for input_image parsing (URL, existing path, non-existing path)
# TODO: Add tests for invalid model name
</file>

<file path="tests/test_fal_client.py">
# this_file: tests/test_fal_client.py
"""Tests for the FAL API Client and its helpers."""

import pytest
from unittest.mock import AsyncMock, Mock
from pathlib import Path
from typing import Any, cast
from collections.abc import Callable

from twat_genai.engines.fal.client import FalApiClient
from twat_genai.engines.fal.types import ModelTypes, ImageResult

# --- Tests for _extract_generic_image_info (Instance Method) ---


@pytest.mark.parametrize(
    "api_result, expected_info_list",
    [
        # Single image dict
        (
            {"image": {"url": "url1", "width": 10, "height": 10}},
            [
                {
                    "url": "url1",
                    "width": 10,
                    "height": 10,
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "seed": None,
                    "file_data": None,
                }
            ],
        ),
        # Single image dict with seed
        (
            {
                "image": {"url": "url1", "width": 10, "height": 10, "seed": 123},
                "seed": 456,
            },
            [
                {
                    "url": "url1",
                    "width": 10,
                    "height": 10,
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "seed": 123,
                    "file_data": None,
                }
            ],
        ),
        # Single image URL string with top-level seed
        (
            {"url": "url2", "seed": 789},
            [
                {
                    "url": "url2",
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "width": None,
                    "height": None,
                    "seed": 789,
                    "file_data": None,
                }
            ],
        ),
        # List of image dicts
        (
            {
                "images": [
                    {"url": "url3", "width": 20},
                    {"url": "url4", "height": 30, "seed": 111},
                ]
            },
            [
                {
                    "url": "url3",
                    "width": 20,
                    "height": None,
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "seed": None,
                    "file_data": None,
                },
                {
                    "url": "url4",
                    "width": None,
                    "height": 30,
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "seed": 111,
                    "file_data": None,
                },
            ],
        ),
        # List of image URL strings (seed extraction not possible here)
        (
            {"image": ["url5", "url6"]},
            [
                {
                    "url": "url5",
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "width": None,
                    "height": None,
                    "seed": None,
                    "file_data": None,
                },
                {
                    "url": "url6",
                    "content_type": "image/png",
                    "file_name": "output.png",
                    "file_size": 0,
                    "width": None,
                    "height": None,
                    "seed": None,
                    "file_data": None,
                },
            ],
        ),
    ],
)
def test_extract_generic_image_info_success(
    api_result: dict[str, Any], expected_info_list: list[dict[str, Any]]
) -> None:
    """Test successful extraction of image info from various result formats."""
    client = FalApiClient()
    extracted = client._extract_generic_image_info(api_result)
    assert extracted == expected_info_list


def test_extract_generic_image_info_no_image() -> None:
    """Test extraction when no image/url key is present."""
    client = FalApiClient()
    extracted = client._extract_generic_image_info({"some_other_key": "value"})
    assert extracted == []  # Should return empty list on failure


def test_extract_generic_image_info_empty_result() -> None:
    """Test extraction with an empty result dict."""
    client = FalApiClient()
    extracted = client._extract_generic_image_info({})
    assert extracted == []


def test_extract_generic_image_info_no_url_in_data() -> None:
    """Test extraction when image data lacks a URL."""
    client = FalApiClient()
    extracted = client._extract_generic_image_info({"image": {"width": 10}})
    assert extracted == []  # Expect empty list if no URL found


# --- Tests for upload_image ---


@pytest.mark.asyncio
async def test_upload_image_success(mocker: Mock) -> None:
    """Test successful image upload."""
    mock_upload = mocker.patch("fal_client.upload_file_async", new_callable=AsyncMock)
    mock_upload.return_value = "https://fake.fal.ai/uploaded.jpg"

    client = FalApiClient()
    fake_path = Path("fake/image.jpg")
    # We don't need the file to exist for this mocked test

    result_url = await client.upload_image(fake_path)

    mock_upload.assert_called_once_with(fake_path)
    assert result_url == "https://fake.fal.ai/uploaded.jpg"


@pytest.mark.asyncio
async def test_upload_image_failure(mocker: Mock) -> None:
    """Test image upload failure."""
    mock_upload = mocker.patch("fal_client.upload_file_async", new_callable=AsyncMock)
    mock_upload.side_effect = Exception("FAL upload failed")

    client = FalApiClient()
    fake_path = Path("fake/image.jpg")

    with pytest.raises(RuntimeError, match="Failed to upload image: FAL upload failed"):
        await client.upload_image(fake_path)

    mock_upload.assert_called_once_with(fake_path)


# --- Tests for _download_image_helper ---
# (Moved to test_image_utils.py as it's a general helper now)

# --- Tests for _submit_fal_job ---


@pytest.mark.asyncio
async def test_submit_fal_job_success(mocker: Mock) -> None:
    """Test successful job submission."""
    mock_submit = mocker.patch("fal_client.submit_async", new_callable=AsyncMock)
    mock_handler = AsyncMock()
    mock_handler.request_id = "req-123"
    mock_submit.return_value = mock_handler

    # Import within function scope if needed or ensure it's globally available
    try:
        from twat_genai.engines.fal.client import _submit_fal_job
    except ImportError:
        # Handle case where it might be moved into the class
        # This part depends on the final location of _submit_fal_job
        msg = "_submit_fal_job helper function not found"
        raise AssertionError(msg)

    endpoint = "fal-ai/test-model"
    args = {"prompt": "test"}
    request_id = await _submit_fal_job(endpoint, args)

    mock_submit.assert_called_once_with(endpoint, arguments=args)
    assert request_id == "req-123"


# --- Tests for _get_fal_result ---
# TODO: Add tests for _get_fal_result (complex due to polling, saving, parsing)
# Requires mocking fal_client.status_async, result_async, _download_image_helper

# --- Tests for process_upscale ---


@pytest.mark.asyncio
async def test_process_upscale_success(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test successful process_upscale call with various params."""
    model_type = ModelTypes.UPSCALER_ESRGAN
    image_url = "https://fake.fal.ai/input_upscale.jpg"
    kwargs = {
        "prompt": "enhance photo",
        "negative_prompt": "blurry",
        "scale": 4,
        "seed": 12345,
        "esrgan_model": "RealESRGAN_x4plus",
        "esrgan_tile": 0,
        "esrgan_face": False,  # Example boolean
        # Add other relevant kwargs for the model if needed
    }
    output_dir = Path("/tmp/test_upscale_out")
    request_id = "req-upscale-789"

    # Mock submit and result
    mock_submit_job.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts_upscale",
        result={},
        image_info={"url": "fake_upscaled_url"},
        original_prompt=kwargs["prompt"],
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_upscale(
        model_type=model_type,
        image_url=image_url,
        output_dir=output_dir,
        filename_suffix="upscale_test",
        filename_prefix="test_up",
        **kwargs,
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, **kwargs}
    # Remove None values if any were added implicitly (though unlikely here)
    expected_fal_args = {k: v for k, v in expected_fal_args.items() if v is not None}
    mock_submit_job.assert_called_once_with(model_type.value, expected_fal_args)

    expected_job_params = {
        "model": model_type.value,
        "input_image_url": image_url,
        **kwargs,
    }
    mock_fal_client._get_fal_result.assert_called_once_with(
        request_id=request_id,
        model_endpoint=model_type.value,
        output_dir=output_dir,
        filename_suffix="upscale_test",
        filename_prefix="test_up",
        original_prompt=kwargs["prompt"],
        job_params=expected_job_params,
    )
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_upscale_invalid_model(mock_fal_client: FalApiClient) -> None:
    """Test process_upscale failure with non-upscaler model."""
    with pytest.raises(ValueError, match="Invalid model type for upscale"):
        await mock_fal_client.process_upscale(
            model_type=ModelTypes.TEXT,  # Invalid type
            image_url="fake_url",
        )


@pytest.mark.asyncio
async def test_process_upscale_submit_failure(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock
) -> None:
    """Test process_upscale failure during job submission."""
    mock_submit_job.side_effect = Exception("Upscale submit failed")

    with pytest.raises(
        RuntimeError, match="Upscale process failed: Upscale submit failed"
    ):
        await mock_fal_client.process_upscale(
            model_type=ModelTypes.UPSCALER_DRCT,
            image_url="fake_url",
            prompt="test",
        )


@pytest.mark.asyncio
async def test_process_upscale_get_result_failure(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_upscale failure during result fetching."""
    request_id = "req-upscale-fail"
    mock_submit_job.return_value = request_id
    mock_get_result(mock_fal_client, None)  # Setup mock on instance
    mock_fal_client._get_fal_result.side_effect = Exception("Upscale get result failed")

    with pytest.raises(
        RuntimeError, match="Upscale process failed: Upscale get result failed"
    ):
        await mock_fal_client.process_upscale(
            model_type=ModelTypes.UPSCALER_AURA_SR,
            image_url="fake_url",
        )


@pytest.mark.asyncio
async def test_process_upscale_ideogram_params(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Mock
) -> None:
    """Test upscaling with Ideogram-specific parameters."""
    image_url = "https://fake.fal.ai/input.jpg"
    output_dir = Path("fake/output/dir")
    request_id = "req-123"
    model_type = ModelTypes.UPSCALE_IDEOGRAM

    # Mock submit and result
    mock = cast(AsyncMock, mock_submit_job)
    mock.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts1",
        result={},
        image_info={"url": "fake_url"},
    )
    mock_get_result.return_value = expected_result

    # Call method
    result = await mock_fal_client.process_upscale(
        image_url=image_url,
        output_dir=output_dir,
        model_type=model_type,
        resemblance=0.8,
        detail=0.6,
        expand_prompt="test prompt",
    )

    # Assertions
    expected_fal_args = {
        "image_url": image_url,
        "resemblance": 0.8,
        "detail": 0.6,
        "expand_prompt": "test prompt",
    }
    mock.assert_called_once_with(model_type.value, expected_fal_args)
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_upscale_ccsr_params(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_upscale with CCSR-specific parameters."""
    model_type = ModelTypes.UPSCALER_CCSR
    image_url = "https://fake.fal.ai/input_upscale.jpg"
    kwargs = {
        "scale": 2,  # CCSR-specific
        "steps": 50,  # CCSR-specific
        "color_fix_type": "adain",  # CCSR-specific
        "tile_diffusion": True,  # CCSR-specific
    }
    request_id = "req-upscale-ccsr"

    # Mock submit and result
    mock_submit_job.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts_upscale",
        result={},
        image_info={"url": "fake_upscaled_url"},
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_upscale(
        model_type=model_type,
        image_url=image_url,
        **kwargs,
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, **kwargs}
    mock_submit_job.assert_called_once_with(model_type.value, expected_fal_args)
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_upscale_clarity_params(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_upscale with Clarity/Aura SR-specific parameters."""
    model_type = ModelTypes.UPSCALER_CLARITY
    image_url = "https://fake.fal.ai/input_upscale.jpg"
    kwargs = {
        "scale": 2,
        "creativity": 0.35,  # Clarity-specific
        "resemblance": 0.6,  # Clarity-specific
        "prompt": "masterpiece, best quality, highres",
        "negative_prompt": "(worst quality, low quality, normal quality:2)",
    }
    request_id = "req-upscale-clarity"

    # Mock submit and result
    mock_submit_job.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts_upscale",
        result={},
        image_info={"url": "fake_upscaled_url"},
        original_prompt=kwargs["prompt"],
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_upscale(
        model_type=model_type,
        image_url=image_url,
        **kwargs,
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, **kwargs}
    mock_submit_job.assert_called_once_with(model_type.value, expected_fal_args)
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_upscale_recraft_params(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_upscale with Recraft-specific parameters."""
    model_type = ModelTypes.UPSCALER_RECRAFT_CREATIVE
    image_url = "https://fake.fal.ai/input_upscale.jpg"
    kwargs = {
        "sync_mode": True,  # Recraft-specific
        "prompt": "enhance creative details",
    }
    request_id = "req-upscale-recraft"

    # Mock submit and result
    mock_submit_job.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts_upscale",
        result={},
        image_info={"url": "fake_upscaled_url"},
        original_prompt=kwargs["prompt"],
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_upscale(
        model_type=model_type,
        image_url=image_url,
        **kwargs,
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, **kwargs}
    mock_submit_job.assert_called_once_with(model_type.value, expected_fal_args)
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_upscale_drct_params(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_upscale with DRCT-specific parameters."""
    model_type = ModelTypes.UPSCALER_DRCT
    image_url = "https://fake.fal.ai/input_upscale.jpg"
    kwargs = {
        "upscaling_factor": 4,  # DRCT-specific
        "prompt": "enhance details",
    }
    request_id = "req-upscale-drct"

    # Mock submit and result
    mock_submit_job.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts_upscale",
        result={},
        image_info={"url": "fake_upscaled_url"},
        original_prompt=kwargs["prompt"],
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_upscale(
        model_type=model_type,
        image_url=image_url,
        **kwargs,
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, **kwargs}
    mock_submit_job.assert_called_once_with(model_type.value, expected_fal_args)
    assert result == expected_result


# --- Tests for process_i2i ---


@pytest.mark.asyncio
async def test_process_i2i_success(
    mock_fal_client: FalApiClient,
    mock_build_lora_arguments: AsyncMock,
    mock_submit_job: AsyncMock,
    mock_get_result: Callable,
) -> None:
    """Test successful process_i2i call."""
    prompt = "test i2i prompt"
    image_url = "https://fake.fal.ai/input.jpg"
    lora_spec = None
    kwargs = {
        "image_size": "landscape_hd",
        "guidance_scale": 5.0,
        "num_inference_steps": 25,
        "strength": 0.8,
        "negative_prompt": "bad quality",
    }
    output_dir = Path("/tmp/test_i2i_out")

    # Mock LoRA return
    mock_build_lora_arguments.return_value = ([], prompt)
    # Mock result return
    expected_result = ImageResult(
        request_id="req-test-456",
        timestamp="ts2",
        result={},
        image_info={"url": "fake_url_i2i"},
        original_prompt=prompt,
    )
    mock_submit_job.return_value = "req-test-456"
    mock_get_result(mock_fal_client, expected_result)

    # Call method
    result = await mock_fal_client.process_i2i(
        prompt=prompt,
        image_url=image_url,
        lora_spec=lora_spec,
        output_dir=output_dir,
        filename_suffix="i2i_test",
        filename_prefix="test_i2i",
        **kwargs,
    )

    # Assertions
    mock_build_lora_arguments.assert_called_once_with(lora_spec, prompt)
    expected_fal_args = {
        "loras": [],
        "prompt": prompt,
        "image_url": image_url,
        "strength": 0.8,
        "negative_prompt": "bad quality",
        "num_images": 1,
        "output_format": "jpeg",
        "enable_safety_checker": False,
        "image_size": "landscape_hd",
        "guidance_scale": 5.0,
        "num_inference_steps": 25,
    }
    mock_submit_job.assert_called_once_with(ModelTypes.IMAGE.value, expected_fal_args)

    expected_job_params = {
        "model": ModelTypes.IMAGE.value,
        "prompt": prompt,
        "lora_spec": lora_spec,
        "input_image_url": image_url,
        **kwargs,
    }
    mock_fal_client._get_fal_result.assert_called_once_with(
        request_id="req-test-456",
        model_endpoint=ModelTypes.IMAGE.value,
        output_dir=output_dir,
        filename_suffix="i2i_test",
        filename_prefix="test_i2i",
        original_prompt=prompt,
        job_params=expected_job_params,
    )
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_i2i_missing_image_url(mock_fal_client: FalApiClient) -> None:
    """Test process_i2i failure when image_url is missing (should not happen via _process_generic check)."""
    # This tests the internal check within _process_generic
    with pytest.raises(ValueError, match="Image URL is required for model type IMAGE"):
        # We call _process_generic directly here to test its internal validation
        await mock_fal_client._process_generic(
            model_type=ModelTypes.IMAGE,
            prompt="test",
            lora_spec=None,
            image_url=None,  # Explicitly pass None here
        )


# --- Tests for process_canny / process_depth ---
# (Similar structure to process_i2i, potentially combine or add specific checks)
# TODO: Add tests for process_canny, process_depth

# --- Tests for process_outpaint ---


@pytest.mark.asyncio
async def test_process_outpaint_success(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Mock
) -> None:
    """Test successful outpainting."""
    image_url = "https://fake.fal.ai/input.jpg"
    output_dir = Path("fake/output/dir")
    request_id = "req-123"

    # Mock submit and result
    mock = cast(AsyncMock, mock_submit_job)
    mock.return_value = request_id
    expected_result = ImageResult(
        request_id=request_id,
        timestamp="ts1",
        result={},
        image_info={"url": "fake_url"},
    )
    mock_get_result.return_value = expected_result

    # Call method
    result = await mock_fal_client.process_outpaint(
        image_url=image_url, output_dir=output_dir, prompt="test prompt"
    )

    # Assertions
    expected_fal_args = {"image_url": image_url, "prompt": "test prompt"}
    mock.assert_called_once_with("outpaint", expected_fal_args)
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_outpaint_missing_image_url(
    mock_fal_client: FalApiClient,
) -> None:
    """Test process_outpaint failure when image_url is missing in kwargs."""
    kwargs_no_url = {"prompt": "test"}
    with pytest.raises(ValueError, match="Missing required argument 'image_url'"):
        await mock_fal_client.process_outpaint(**kwargs_no_url)


@pytest.mark.asyncio
async def test_process_outpaint_submit_failure(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock
) -> None:
    """Test process_outpaint failure during job submission."""
    mock_submit_job.side_effect = Exception("Outpaint submit failed")
    kwargs = {"image_url": "fake_url", "prompt": "test"}

    with pytest.raises(
        RuntimeError, match="Outpainting process failed: Outpaint submit failed"
    ):
        await mock_fal_client.process_outpaint(**kwargs)


@pytest.mark.asyncio
async def test_process_outpaint_get_result_failure(
    mock_fal_client: FalApiClient, mock_submit_job: AsyncMock, mock_get_result: Callable
) -> None:
    """Test process_outpaint failure during result fetching."""
    request_id = "req-outpaint-fail"
    kwargs = {"image_url": "fake_url", "prompt": "test"}
    mock_submit_job.return_value = request_id
    mock_get_result(mock_fal_client, None)
    mock_fal_client._get_fal_result.side_effect = Exception(
        "Outpaint get result failed"
    )

    with pytest.raises(
        RuntimeError, match="Outpainting process failed: Outpaint get result failed"
    ):
        await mock_fal_client.process_outpaint(**kwargs)


# --- Mocks for process_* tests ---


@pytest.fixture
def mock_fal_client(mocker: Mock) -> FalApiClient:
    """Provides a FalApiClient instance with mocked internal methods."""
    client = FalApiClient()
    # Mock the helper methods that interact with the actual fal_client library or file system
    mocker.patch.object(client, "_get_fal_result", new_callable=AsyncMock)
    # Mock _submit_fal_job (even though it's top-level, process_* calls it)
    # TODO: Move _submit_fal_job into the class?
    mocker.patch(
        "twat_genai.engines.fal.client._submit_fal_job", new_callable=AsyncMock
    )
    # Mock LoRA building
    mocker.patch(
        "twat_genai.engines.fal.lora.build_lora_arguments", new_callable=AsyncMock
    )
    # Mock download helper used by _get_fal_result
    mocker.patch(
        "twat_genai.engines.fal.client._download_image_helper", new_callable=AsyncMock
    )
    return client


@pytest.fixture
def mock_build_lora_arguments(mocker: Mock) -> AsyncMock:
    """Fixture to mock build_lora_arguments."""
    mock = mocker.patch(
        "twat_genai.engines.fal.lora.build_lora_arguments", new_callable=AsyncMock
    )
    # Default return: no loras, original prompt
    mock.return_value = ([], "test prompt")
    return mock


@pytest.fixture
def mock_submit_job() -> AsyncMock:
    """Mock for _submit_fal_job."""
    return AsyncMock()


@pytest.fixture
def mock_get_result() -> Mock:
    """Mock for _get_fal_result."""
    mock = Mock()

    def _mock_get_result(client: FalApiClient, result: ImageResult) -> None:
        client._get_fal_result = Mock(return_value=result)

    mock.side_effect = _mock_get_result
    return mock


# --- Tests for process_tti ---


@pytest.mark.asyncio
async def test_process_tti_success(
    mock_fal_client: FalApiClient,  # Use the client with mocks
    mock_build_lora_arguments: AsyncMock,
    mock_submit_job: AsyncMock,
    mock_get_result: Callable,
) -> None:
    """Test successful process_tti call."""
    prompt = "test prompt"
    lora_spec = None
    kwargs = {
        "image_size": "square_hd",
        "guidance_scale": 7.0,
        "num_inference_steps": 30,
    }
    output_dir = Path("/tmp/test_out")

    # Setup mock return value for _get_fal_result
    expected_result = ImageResult(
        request_id="req-test-123",
        timestamp="ts",
        result={},
        image_info={"url": "fake_url"},  # Minimal valid info
        original_prompt=prompt,
    )
    mock_get_result(mock_fal_client, expected_result)

    # Call the method under test
    result = await mock_fal_client.process_tti(
        prompt=prompt,
        lora_spec=lora_spec,
        output_dir=output_dir,
        filename_suffix="tti_test",
        filename_prefix="test",
        **kwargs,
    )

    # Assertions
    mock_build_lora_arguments.assert_called_once_with(lora_spec, prompt)
    expected_fal_args = {
        "loras": [],
        "prompt": prompt,
        "num_images": 1,
        "output_format": "jpeg",
        "enable_safety_checker": False,
        "image_size": "square_hd",
        "guidance_scale": 7.0,
        "num_inference_steps": 30,
    }
    mock_submit_job.assert_called_once_with(ModelTypes.TEXT.value, expected_fal_args)

    expected_job_params = {
        "model": ModelTypes.TEXT.value,
        "prompt": prompt,
        "lora_spec": lora_spec,
        **kwargs,
    }
    mock_fal_client._get_fal_result.assert_called_once_with(
        request_id="req-test-123",
        model_endpoint=ModelTypes.TEXT.value,
        output_dir=output_dir,
        filename_suffix="tti_test",
        filename_prefix="test",
        original_prompt=prompt,
        job_params=expected_job_params,
    )
    assert result == expected_result


@pytest.mark.asyncio
async def test_process_tti_lora_failure(
    mock_fal_client: FalApiClient, mock_build_lora_arguments: AsyncMock
) -> None:
    """Test process_tti failure during LoRA building."""
    mock_build_lora_arguments.side_effect = Exception("LoRA build error")

    with pytest.raises(
        RuntimeError, match="Failed to build LoRA arguments: LoRA build error"
    ):
        await mock_fal_client.process_tti(prompt="test", lora_spec="invalid")


@pytest.mark.asyncio
async def test_process_tti_submit_failure(
    mock_fal_client: FalApiClient,
    mock_build_lora_arguments: AsyncMock,
    mock_submit_job: AsyncMock,
) -> None:
    """Test process_tti failure during job submission."""
    mock_submit_job.side_effect = Exception("Submit failed")

    with pytest.raises(RuntimeError, match="Generic process failed: Submit failed"):
        await mock_fal_client.process_tti(prompt="test", lora_spec=None)


@pytest.mark.asyncio
async def test_process_tti_get_result_failure(
    mock_fal_client: FalApiClient,
    mock_build_lora_arguments: AsyncMock,
    mock_submit_job: AsyncMock,
    mock_get_result: Callable,
) -> None:
    """Test process_tti failure during result fetching."""
    # Setup _get_fal_result mock to raise an error
    mock_get_result(mock_fal_client, None)  # Need instance
    mock_fal_client._get_fal_result.side_effect = Exception("Get result failed")

    with pytest.raises(RuntimeError, match="Generic process failed: Get result failed"):
        await mock_fal_client.process_tti(prompt="test", lora_spec=None)
</file>

<file path="tests/test_image_utils.py">
# this_file: tests/test_image_utils.py
"""Tests for image utility functions."""

import pytest
from unittest.mock import AsyncMock, MagicMock
from pathlib import Path

from PIL import Image

from twat_genai.core.image_utils import (
    download_image_to_temp,
    download_image,
    resize_image_if_needed,
)
from twat_genai.engines.fal.config import ModelTypes  # For resize test

# --- Tests for download_image_to_temp ---


@pytest.mark.asyncio
async def test_download_image_to_temp_success(mocker):
    "Test successful download to a temporary file."
    # Mock httpx response
    mock_response = AsyncMock()
    mock_response.content = b"fake image data"
    mock_response.headers = {"content-type": "image/jpeg"}
    mock_response.raise_for_status = MagicMock()

    # Mock httpx client
    mock_client = AsyncMock()
    mock_client.get.return_value = mock_response
    mock_async_client = mocker.patch("httpx.AsyncClient", return_value=mock_client)

    # Mock tempfile
    mock_named_temp_file = mocker.patch("tempfile.NamedTemporaryFile")
    mock_file_handle = MagicMock()
    mock_file_handle.name = "/tmp/fake_temp_file.jpg"
    mock_named_temp_file.return_value.__enter__.return_value = mock_file_handle

    temp_path = await download_image_to_temp("http://fake.url/img.jpg")

    mock_async_client.assert_called_once()
    mock_client.get.assert_called_once_with(
        "http://fake.url/img.jpg", follow_redirects=True
    )
    mock_named_temp_file.assert_called_once_with(suffix=".jpg", delete=False)
    mock_file_handle.write.assert_called_once_with(b"fake image data")
    assert temp_path == "/tmp/fake_temp_file.jpg"


@pytest.mark.asyncio
async def test_download_image_to_temp_http_error(mocker):
    "Test HTTP error during download to temp."
    # Mock httpx response to raise error
    mock_response = AsyncMock()
    mock_response.raise_for_status.side_effect = mocker.patch(
        "httpx.RequestError", side_effect=ValueError("Fake HTTP Error")
    )()

    mock_client = AsyncMock()
    mock_client.get.return_value = mock_response
    mocker.patch("httpx.AsyncClient", return_value=mock_client)

    with pytest.raises(
        RuntimeError, match="Failed to download image"
    ):  # Match the wrapped error
        await download_image_to_temp("http://fake.url/img.jpg")


# --- Tests for download_image (to specific path) ---


@pytest.mark.asyncio
async def test_download_image_success(mocker, tmp_path):
    "Test successful download to a specific path."
    output_path = tmp_path / "output.png"

    # Mock httpx response
    mock_response = AsyncMock()
    mock_response.content = b"fake png data"
    mock_response.headers = {"content-type": "image/png"}
    mock_response.raise_for_status = MagicMock()

    mock_client = AsyncMock()
    mock_client.get.return_value = mock_response
    mocker.patch("httpx.AsyncClient", return_value=mock_client)

    await download_image("http://fake.url/img.png", output_path)

    mock_client.get.assert_called_once_with(
        "http://fake.url/img.png", follow_redirects=True
    )
    assert output_path.exists()
    assert output_path.read_bytes() == b"fake png data"


# --- Tests for resize_image_if_needed ---


# Fixture to create a dummy image file
@pytest.fixture
def dummy_image_file(tmp_path):
    file_path = tmp_path / "test_image.jpg"
    img = Image.new("RGB", (3000, 2000), color="red")  # Larger than max size
    img.save(file_path)
    return file_path


# Mock the UPSCALE_TOOL_MAX_INPUT_SIZES for testing
@pytest.fixture(autouse=True)
def mock_upscale_sizes(mocker):
    mocker.patch(
        "twat_genai.core.image_utils.UPSCALE_TOOL_MAX_INPUT_SIZES",
        {ModelTypes.UPSCALER_DRCT: 2048, ModelTypes.UPSCALER_ESRGAN: 1024},
    )


def test_resize_needed(dummy_image_file):
    "Test when resizing is needed."
    # Test with DRCT (max 2048), image is 3000x2000 -> should resize based on width
    resized_path_str = resize_image_if_needed(
        str(dummy_image_file), ModelTypes.UPSCALER_DRCT
    )
    assert resized_path_str is not None
    resized_path = Path(resized_path_str)
    assert resized_path.exists()
    with Image.open(resized_path) as img:
        assert img.width == 2048
        assert img.height == int(2000 * (2048 / 3000))
    resized_path.unlink()  # Clean up temp file


def test_resize_not_needed(dummy_image_file):
    "Test when resizing is not needed (image within limits)."
    # Create smaller image
    small_img_path = dummy_image_file.parent / "small.jpg"
    img = Image.new("RGB", (1000, 800), color="blue")
    img.save(small_img_path)

    resized_path_str = resize_image_if_needed(
        str(small_img_path), ModelTypes.UPSCALER_DRCT
    )
    assert resized_path_str is None


def test_resize_not_upscaler(dummy_image_file):
    "Test that non-upscaler models are skipped."
    # Text model is not in our mocked UPSCALE_TOOL_MAX_INPUT_SIZES
    resized_path_str = resize_image_if_needed(str(dummy_image_file), ModelTypes.TEXT)
    assert resized_path_str is None


def test_resize_file_not_found():
    "Test resizing when the input file does not exist."
    with pytest.raises(FileNotFoundError):
        resize_image_if_needed("nonexistent_file.jpg", ModelTypes.UPSCALER_DRCT)
</file>

<file path="tests/test_twat_genai.py">
"""Test suite for twat_genai."""


def test_version():
    """Verify package exposes version."""
    import twat_genai

    assert twat_genai.__version__
</file>

<file path=".cursorindexingignore">
# Don't index SpecStory auto-save files, but allow explicit context inclusion via @ references
.specstory/**
</file>

<file path=".gitignore">
.specstory/
*_autogen/
.DS_Store
__version__.py
__pycache__/
_Chutzpah*
_deps
_NCrunch_*
_pkginfo.txt
_Pvt_Extensions
_ReSharper*/
_TeamCity*
_UpgradeReport_Files/
!?*.[Cc]ache/
!.axoCover/settings.json
!.vscode/extensions.json
!.vscode/launch.json
!.vscode/settings.json
!.vscode/tasks.json
!**/[Pp]ackages/build/
!Directory.Build.rsp
.*crunch*.local.xml
.axoCover/*
.builds
.cr/personal
.fake/
.history/
.ionide/
.localhistory/
.mfractor/
.ntvs_analysis.dat
.paket/paket.exe
.sass-cache/
.vs/
.vscode
.vscode/*
.vshistory/
[Aa][Rr][Mm]/
[Aa][Rr][Mm]64/
[Bb]in/
[Bb]uild[Ll]og.*
[Dd]ebug/
[Dd]ebugPS/
[Dd]ebugPublic/
[Ee]xpress/
[Ll]og/
[Ll]ogs/
[Oo]bj/
[Rr]elease/
[Rr]eleasePS/
[Rr]eleases/
[Tt]est[Rr]esult*/
[Ww][Ii][Nn]32/
*_h.h
*_i.c
*_p.c
*_wpftmp.csproj
*- [Bb]ackup ([0-9]).rdl
*- [Bb]ackup ([0-9][0-9]).rdl
*- [Bb]ackup.rdl
*.[Cc]ache
*.[Pp]ublish.xml
*.[Rr]e[Ss]harper
*.a
*.app
*.appx
*.appxbundle
*.appxupload
*.aps
*.azurePubxml
*.bim_*.settings
*.bim.layout
*.binlog
*.btm.cs
*.btp.cs
*.build.csdef
*.cab
*.cachefile
*.code-workspace
*.coverage
*.coveragexml
*.d
*.dbmdl
*.dbproj.schemaview
*.dll
*.dotCover
*.DotSettings.user
*.dsp
*.dsw
*.dylib
*.e2e
*.exe
*.gch
*.GhostDoc.xml
*.gpState
*.ilk
*.iobj
*.ipdb
*.jfm
*.jmconfig
*.la
*.lai
*.ldf
*.lib
*.lo
*.log
*.mdf
*.meta
*.mm.*
*.mod
*.msi
*.msix
*.msm
*.msp
*.ncb
*.ndf
*.nuget.props
*.nuget.targets
*.nupkg
*.nvuser
*.o
*.obj
*.odx.cs
*.opendb
*.opensdf
*.opt
*.out
*.pch
*.pdb
*.pfx
*.pgc
*.pgd
*.pidb
*.plg
*.psess
*.publishproj
*.publishsettings
*.pubxml
*.pyc
*.rdl.data
*.rptproj.bak
*.rptproj.rsuser
*.rsp
*.rsuser
*.sap
*.sbr
*.scc
*.sdf
*.sln.docstates
*.sln.iml
*.slo
*.smod
*.snupkg
*.so
*.suo
*.svclog
*.tlb
*.tlh
*.tli
*.tlog
*.tmp
*.tmp_proj
*.tss
*.user
*.userosscache
*.userprefs
*.vbp
*.vbw
*.VC.db
*.VC.VC.opendb
*.VisualState.xml
*.vsp
*.vspscc
*.vspx
*.vssscc
*.xsd.cs
**/[Pp]ackages/*
**/*.DesktopClient/GeneratedArtifacts
**/*.DesktopClient/ModelManifest.xml
**/*.HTMLClient/GeneratedArtifacts
**/*.Server/GeneratedArtifacts
**/*.Server/ModelManifest.xml
*~
~$*
$tf/
AppPackages/
artifacts/
ASALocalRun/
AutoTest.Net/
Backup*/
BenchmarkDotNet.Artifacts/
bld/
BundleArtifacts/
ClientBin/
cmake_install.cmake
CMakeCache.txt
CMakeFiles
CMakeLists.txt.user
CMakeScripts
CMakeUserPresets.json
compile_commands.json
coverage*.info
coverage*.json
coverage*.xml
csx/
CTestTestfile.cmake
dlldata.c
DocProject/buildhelp/
DocProject/Help/*.hhc
DocProject/Help/*.hhk
DocProject/Help/*.hhp
DocProject/Help/*.HxC
DocProject/Help/*.HxT
DocProject/Help/html
DocProject/Help/Html2
ecf/
FakesAssemblies/
FodyWeavers.xsd
Generated_Code/
Generated\ Files/
healthchecksdb
install_manifest.txt
ipch/
Makefile
MigrationBackup/
mono_crash.*
nCrunchTemp_*
node_modules/
nunit-*.xml
OpenCover/
orleans.codegen.cs
Package.StoreAssociation.xml
paket-files/
project.fragment.lock.json
project.lock.json
publish/
PublishScripts/
rcf/
ScaffoldingReadMe.txt
ServiceFabricBackup/
StyleCopReport.xml
Testing
TestResult.xml
UpgradeLog*.htm
UpgradeLog*.XML
x64/
x86/
# Python
__pycache__/
*.py[cod]
*$py.class
*.so
.Python
build/
develop-eggs/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# Distribution / packaging
!dist/.gitkeep

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/
.ruff_cache/

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# IDE
.idea/
.vscode/
*.swp
*.swo
*~

# OS
.DS_Store
.DS_Store?
._*
.Spotlight-V100
.Trashes
ehthumbs.db
Thumbs.db

# Project specific
__version__.py
_private
</file>

<file path=".pre-commit-config.yaml">
repos:
  - repo: https://github.com/astral-sh/ruff-pre-commit
    rev: v0.3.4
    hooks:
      - id: ruff
        args: [--fix]
      - id: ruff-format
        args: [--respect-gitignore]
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.5.0
    hooks:
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-toml
      - id: check-added-large-files
      - id: debug-statements
      - id: check-case-conflict
      - id: mixed-line-ending
        args: [--fix=lf]
</file>

<file path="CHANGELOG.md">
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [Unreleased]

### Added
- Initial `CHANGELOG.md` file.
- Initial `PLAN.md` with streamlining strategy.
- Initial `TODO.md` for tracking progress.

### Changed
- (Nothing yet)

### Removed
- (Nothing yet)
</file>

<file path="CHANGES.md">
# Changes

## 1. Fixed Issues

### 1.1. Directory Creation with Tilde (~) Character
Fixed issue where running `python -m twat_genai outpaint` would create a literal "~" directory with "Pictures/twat_genai" and "tmp/twat" subdirectories. This occurred because path expansion wasn't correctly handling tilde characters.

**Changes made:**
- Updated paths.toml to use `${HOME}` instead of `~` for paths that were causing issues
- Improved the path expansion logic in `PathConfig.expand_path` method to ensure proper order of operations:
  1. Expand environment variables first
  2. Then expand user home directory (~ character)
  3. Finally convert to absolute path
- Enhanced `format_path` method to follow the same expansion order
- Updated `GenAIConfig` to use proper path expansion and added null checks

### 1.2. Pydantic Model Definition Error in ImageInput
Fixed issue where attempting to use the `outpaint` or `upscale` commands with an image would fail with:
`ImageInput is not fully defined; you should define Image, then call ImageInput.model_rebuild()`

**Changes made:**
- Modified PIL Image import to be non-conditional, removing the TYPE_CHECKING condition
- Properly imported PIL.Image where needed in the core/config.py file
- Updated the `to_url` method signature in `ImageInput` to include an optional `client` parameter
- Updated the `FALImageInput.to_url` implementation to accept the client parameter
- Refactored the `_prepare_image_input` method in FAL engine to better handle image processing

## 2. Test Scripts
- Added a test script (`test_outpaint.py`) to verify fixes without running the full outpaint command
</file>

<file path="cleanup.py">
#!/usr/bin/env -S uv run -s
# /// script
# dependencies = [
#   "ruff>=0.9.6",
#   "pytest>=8.3.4",
#   "mypy>=1.15.0",
# ]
# ///
# this_file: cleanup.py

"""
Cleanup tool for managing repository tasks and maintaining code quality.

This script provides a comprehensive set of commands for repository maintenance:

When to use each command:

- `cleanup.py status`: Use this FIRST when starting work to check the current state
  of the repository. It shows file structure, git status, and runs all code quality
  checks. Run this before making any changes to ensure you're starting from a clean state.

- `cleanup.py venv`: Run this when setting up the project for the first time or if
  your virtual environment is corrupted/missing. Creates a new virtual environment
  using uv.

- `cleanup.py install`: Use after `venv` or when dependencies have changed. Installs
  the package and all development dependencies in editable mode.

- `cleanup.py update`: Run this when you've made changes and want to commit them.
  It will:
  1. Show current status (like `status` command)
  2. Stage and commit any changes with a generic message
  Use this for routine maintenance commits.

- `cleanup.py push`: Run this after `update` when you want to push your committed
  changes to the remote repository.

Workflow Example:
1. Start work: `cleanup.py status`
2. Make changes to code
3. Commit changes: `cleanup.py update`
4. Push to remote: `cleanup.py push`

The script maintains a CLEANUP.txt file that records all operations with timestamps.
It also includes content from README.md at the start and TODO.md at the end of logs
for context.

Required Files:
- LOG.md: Project changelog
- README.md: Project documentation
- TODO.md: Pending tasks and future plans
"""

import subprocess
import os
import sys
from datetime import datetime, timezone
from pathlib import Path
from typing import NoReturn
from shutil import which

# Configuration
IGNORE_PATTERNS = [
    ".git",
    ".venv",
    "__pycache__",
    "*.pyc",
    "dist",
    "build",
    "*.egg-info",
]
REQUIRED_FILES = ["LOG.md", ".cursor/rules/0project.mdc", "TODO.md"]
LOG_FILE = Path("CLEANUP.txt")

# Ensure we're working from the script's directory
os.chdir(Path(__file__).parent)


def new() -> None:
    """Remove existing log file."""
    if LOG_FILE.exists():
        LOG_FILE.unlink()


def prefix() -> None:
    """Write README.md content to log file."""
    readme = Path(".cursor/rules/0project.mdc")
    if readme.exists():
        log_message("\n=== PROJECT STATEMENT ===")
        content = readme.read_text()
        log_message(content)


def suffix() -> None:
    """Write TODO.md content to log file."""
    todo = Path("TODO.md")
    if todo.exists():
        log_message("\n=== TODO.md ===")
        content = todo.read_text()
        log_message(content)


def log_message(message: str) -> None:
    """Log a message to file and console with timestamp."""
    timestamp = datetime.now(timezone.utc).strftime("%Y-%m-%d %H:%M:%S")
    log_line = f"{timestamp} - {message}\n"
    with LOG_FILE.open("a") as f:
        f.write(log_line)


def run_command(cmd: list[str], *, check: bool = True) -> subprocess.CompletedProcess:
    """Run a shell command and return the result."""
    try:
        # Ensure shell=False for security
        result = subprocess.run(
            cmd,
            check=check,
            capture_output=True,
            text=True,
            shell=False,
        )
        if result.stdout:
            log_message(result.stdout)
        return result
    except subprocess.CalledProcessError as e:
        log_message(f"Command failed: {' '.join(cmd)}")
        log_message(f"Error: {e.stderr}")
        if check:
            raise
        return subprocess.CompletedProcess(cmd, 1, "", str(e))


def check_command_exists(cmd: str) -> bool:
    """Check if a command exists in the system."""
    try:
        return which(cmd) is not None
    except Exception:
        return False


class Cleanup:
    """Main cleanup tool class."""

    def __init__(self) -> None:
        self.workspace = Path.cwd()

    def _print_header(self, message: str) -> None:
        """Print a section header."""
        log_message(f"\n=== {message} ===")

    def _check_required_files(self) -> bool:
        """Check if all required files exist."""
        missing = False
        for file in REQUIRED_FILES:
            if not (self.workspace / file).exists():
                log_message(f"Error: {file} is missing")
                missing = True
        return not missing

    def _generate_tree(self) -> None:
        """Generate and display tree structure of the project."""
        if not check_command_exists("tree"):
            log_message("Warning: 'tree' command not found. Skipping tree generation.")
            return None

        try:
            # Create/overwrite the file with YAML frontmatter
            rules_dir = Path(".cursor/rules")
            rules_dir.mkdir(parents=True, exist_ok=True)
            # Get tree output
            tree_result = run_command(
                ["tree", "-a", "-I", ".git", "--gitignore", "-n", "-h", "-I", "*_cache"]
            )
            tree_text = tree_result.stdout
            # Write frontmatter and tree output to file
            with open(rules_dir / "filetree.mdc", "w") as f:
                f.write("---\ndescription: File tree of the project\nglobs: \n---\n")
                f.write(tree_text)

            # Log the contents
            log_message("\nProject structure:")
            log_message(tree_text)

        except Exception as e:
            log_message(f"Failed to generate tree: {e}")
        return None

    def _git_status(self) -> bool:
        """Check git status and return True if there are changes."""
        result = run_command(["git", "status", "--porcelain"], check=False)
        return bool(result.stdout.strip())

    def _venv(self) -> None:
        """Create and activate virtual environment using uv."""
        log_message("Setting up virtual environment")
        try:
            run_command(["uv", "venv"])
            # Activate the virtual environment
            venv_path = self.workspace / ".venv" / "bin" / "activate"
            if venv_path.exists():
                os.environ["VIRTUAL_ENV"] = str(self.workspace / ".venv")
                os.environ["PATH"] = (
                    f"{self.workspace / '.venv' / 'bin'}{os.pathsep}{os.environ['PATH']}"
                )
                log_message("Virtual environment created and activated")
            else:
                log_message("Virtual environment created but activation failed")
        except Exception as e:
            log_message(f"Failed to create virtual environment: {e}")

    def _install(self) -> None:
        """Install package in development mode with all extras."""
        log_message("Installing package with all extras")
        try:
            self._venv()
            run_command(["uv", "pip", "install", "-e", ".[test,dev]"])
            log_message("Package installed successfully")
        except Exception as e:
            log_message(f"Failed to install package: {e}")

    def _run_checks(self) -> None:
        """Run code quality checks using ruff and pytest."""
        log_message("Running code quality checks")

        try:
            # Run ruff checks
            log_message(">>> Running code fixes...")
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "check",
                    "--fix",
                    "--unsafe-fixes",
                    "src",
                    "tests",
                ],
                check=False,
            )
            run_command(
                [
                    "python",
                    "-m",
                    "ruff",
                    "format",
                    "--respect-gitignore",
                    "src",
                    "tests",
                ],
                check=False,
            )

            # Run type checks
            log_message(">>>Running type checks...")
            run_command(["python", "-m", "mypy", "src", "tests"], check=False)

            # Run tests
            log_message(">>> Running tests...")
            run_command(["python", "-m", "pytest", "tests"], check=False)

            log_message("All checks completed")
        except Exception as e:
            log_message(f"Failed during checks: {e}")

    def status(self) -> None:
        """Show current repository status: tree structure, git status, and run checks."""
        prefix()  # Add README.md content at start
        self._print_header("Current Status")

        # Check required files
        self._check_required_files()

        # Show tree structure
        self._generate_tree()

        # Show git status
        result = run_command(["git", "status"], check=False)
        log_message(result.stdout)

        # Run additional checks
        self._print_header("Environment Status")
        self._venv()
        self._install()
        self._run_checks()

        suffix()  # Add TODO.md content at end

    def venv(self) -> None:
        """Create and activate virtual environment."""
        self._print_header("Virtual Environment Setup")
        self._venv()

    def install(self) -> None:
        """Install package with all extras."""
        self._print_header("Package Installation")
        self._install()

    def update(self) -> None:
        """Show status and commit any changes if needed."""
        # First show current status
        self.status()

        # Then handle git changes if any
        if self._git_status():
            log_message("Changes detected in repository")
            try:
                # Add all changes
                run_command(["git", "add", "."])
                # Commit changes
                commit_msg = "Update repository files"
                run_command(["git", "commit", "-m", commit_msg])
                log_message("Changes committed successfully")
            except Exception as e:
                log_message(f"Failed to commit changes: {e}")
        else:
            log_message("No changes to commit")

    def push(self) -> None:
        """Push changes to remote repository."""
        self._print_header("Pushing Changes")
        try:
            run_command(["git", "push"])
            log_message("Changes pushed successfully")
        except Exception as e:
            log_message(f"Failed to push changes: {e}")


def repomix(
    *,
    compress: bool = True,
    remove_empty_lines: bool = True,
    ignore_patterns: str = ".specstory/**/*.md,.venv/**,_private/**,CLEANUP.txt,**/*.json,*.lock",
    output_file: str = "REPO_CONTENT.txt",
) -> None:
    """Combine repository files into a single text file.

    Args:
        compress: Whether to compress whitespace in output
        remove_empty_lines: Whether to remove empty lines
        ignore_patterns: Comma-separated glob patterns of files to ignore
        output_file: Output file path
    """
    try:
        # Build command
        cmd = ["repomix"]
        if compress:
            cmd.append("--compress")
        if remove_empty_lines:
            cmd.append("--remove-empty-lines")
        if ignore_patterns:
            cmd.append("-i")
            cmd.append(ignore_patterns)
        cmd.extend(["-o", output_file])

        # Run repomix
        run_command(cmd)
        log_message(f"Repository content mixed into {output_file}")

    except Exception as e:
        log_message(f"Failed to mix repository: {e}")


def print_usage() -> None:
    """Print usage information."""
    log_message("Usage:")
    log_message("  cleanup.py status   # Show current status and run all checks")
    log_message("  cleanup.py venv     # Create virtual environment")
    log_message("  cleanup.py install  # Install package with all extras")
    log_message("  cleanup.py update   # Update and commit changes")
    log_message("  cleanup.py push     # Push changes to remote")


def main() -> NoReturn:
    """Main entry point."""
    new()  # Clear log file

    MIN_ARGS = 2
    if len(sys.argv) < MIN_ARGS:
        print_usage()
        sys.exit(1)

    command = sys.argv[1]
    cleanup = Cleanup()

    try:
        if command == "status":
            cleanup.status()
        elif command == "venv":
            cleanup.venv()
        elif command == "install":
            cleanup.install()
        elif command == "update":
            cleanup.update()
        elif command == "push":
            cleanup.push()
        else:
            print_usage()
    except Exception as e:
        log_message(f"Error: {e}")
    repomix()
    sys.stdout.write(Path("CLEANUP.txt").read_text())
    sys.exit(0)  # Ensure we exit with a status code


if __name__ == "__main__":
    main()
</file>

<file path="LICENSE">
MIT License

Copyright (c) 2025 Adam Twardoch

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.
</file>

<file path="LOG.md">
---
this_file: LOG.md
---

# Changelog

All notable changes to the twat-genai project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.1.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [v1.7.5] - 2025-02-15

### Changed

- Improved error message formatting across the codebase
- Updated type hints to use modern Python syntax (e.g., `list[str]` instead of `List[str]`)
- Simplified union type hints using `|` operator (e.g., `str | None` instead of `Optional[str]`)

### Fixed

- Fixed circular imports in FAL engine modules
- Improved error handling and messaging in LoRA processing

## [v1.7.3] - 2025-02-15

### Added

- New FAL-specific image input handling
- Added `FALImageInput` class for better FAL API integration

### Changed

- Refactored image input processing for better type safety
- Updated dependency requirements for better compatibility

## [v1.6.2] - 2025-02-06

### Changed

- Updated dependency versions:
  - `twat>=1.0.0`
  - `twat-image>=1.0.0`

### Fixed

- Package dependency issues

## [v1.6.1] - 2025-02-06

### Changed

- Reorganized module exports in `__init__.py`
- Improved code organization and imports
- Enhanced type annotations throughout the codebase

### Fixed

- Various import and circular dependency issues
- Code style and formatting improvements

## [v1.6.0] - 2025-02-06

### Added

- Initial public release with core functionality
- Support for text-to-image generation
- Support for image-to-image transformation
- LoRA integration with FAL.ai
- Command-line interface
- Python API
- Configuration management
- Image processing utilities

### Features

- Multiple model support through FAL.ai
- Flexible prompt expansion system
- LoRA configuration management
- Image size presets and custom sizes
- Output directory management
- File naming conventions
- Environment variable configuration

## [v1.0.0] - 2025-02-06

### Added

- Initial project structure
- Basic package setup
- Core dependencies
- Development environment configuration

[v1.7.5]: https://github.com/twardoch/twat-genai/compare/v1.7.3...v1.7.5
[v1.7.3]: https://github.com/twardoch/twat-genai/compare/v1.6.2...v1.7.3
[v1.6.2]: https://github.com/twardoch/twat-genai/compare/v1.6.1...v1.6.2
[v1.6.1]: https://github.com/twardoch/twat-genai/compare/v1.6.0...v1.6.1
[v1.6.0]: https://github.com/twardoch/twat-genai/compare/v1.0.0...v1.6.0
[v1.0.0]: https://github.com/twardoch/twat-genai/releases/tag/v1.0.0
</file>

<file path="PLAN.md">
1.  **Project Setup & Initial Cleanup:**
    *   Verify `npc-engine` installation or proceed with manual analysis if installation continues to fail. (Already attempted, will proceed manually)
    *   Create `PLAN.md`, `TODO.md`, and `CHANGELOG.md`.
2.  **Address Code Duplication and Unused Code:**
    *   **`core/models.py`**: Investigate `src/twat_genai/core/models.py`. If it's entirely redundant with `core/config.py` and `core/image.py` (as suggested in `README.md`), remove it. Update any imports if necessary.
    *   **`engines/fal/config.py`**: Remove the duplicate `FalApiClient` definition from `src/twat_genai/engines/fal/config.py`. Ensure all usages point to the correct client in `engines/fal/client.py`.
    *   **`ImageSizeWH`**: Consolidate the `ImageSizeWH` definition. Prefer keeping it in `core/config.py` as it's a configuration-related model. Update `core/image.py` to import it from `core/config.py`.
    *   **Unused `run_upscale` and `run_outpaint`**: Examine `run_upscale` in `src/twat_genai/engines/fal/upscale.py` and `run_outpaint` in `src/twat_genai/engines/fal/outpaint.py`. If they are truly unused and their logic is incorporated elsewhere (likely in `FALEngine` or `FalApiClient`), remove them.
3.  **Streamline Configuration and Core Logic:**
    *   **`ImageInput.to_url`**: Confirm that `FALImageInput.to_url` in `src/twat_genai/engines/fal/models.py` is the sole and correct implementation. The base method in `core/config.py` is designed to be overridden, which is good. No change needed here unless further review shows issues.
    *   **Output Directory Logic (`cli.py`)**:
        *   Review the `get_output_dir` function in `src/twat_genai/cli.py`.
        *   For an MVP, consider simplifying the logic by removing the `twat.PathManager` dependency or making its usage more straightforward. Prioritize a simple default (e.g., `./generated_images`) and user-provided paths. The input image-based subfolder is a good feature to keep.
    *   **CLI Async Structure (`cli.py`)**: Evaluate if refactoring the `TwatGenAiCLI` methods to call a central async orchestrator (instead of `asyncio.run()` in each method) simplifies the code or improves clarity. This is a minor point and might be deferred.
4.  **Review and Refine CLI Arguments and Commands:**
    *   **Upscale Tool Parameters (`cli.py`)**: The `upscale` command in `cli.py` has many specific arguments for different tools (e.g., `ideogram_detail`, `esrgan_model`). While this provides fine-grained control, assess if a more generic approach (e.g., passing a dict of tool-specific params) could simplify the CLI signature for an MVP, or if the current explicitness is preferred. For now, assume current explicitness is fine but keep in mind for future versions.
    *   **Default Prompts for Upscalers (`cli.py`)**: The CLI's `_run_generation` method adds an empty string prompt `[""]` if no prompts are provided for upscalers. Confirm this is the desired behavior for all upscale models supported by FAL.
5.  **Documentation and Helper Scripts:**
    *   **`README.md`**: Update `README.md` to reflect any changes made, especially regarding removed files/code and simplified logic.
    *   **`cleanup.py`**: Decide on the role of `cleanup.py`. If CI/CD handles most of its functions (linting, testing, building), consider removing it or simplifying it to only cover tasks not automated elsewhere. For an MVP, it might be safe to remove if it's primarily a developer convenience for local workflows that are replicated in CI.
    *   **LoRA Configuration (`__main___loras.json`)**: Review the LoRA loading mechanism. The current approach using `__main___loras.json` and `TWAT_GENAI_LORA_LIB` environment variable is flexible. For an MVP, ensure this is clearly documented and robust. No immediate change planned unless issues are found.
6.  **Testing:**
    *   Ensure existing tests pass after changes.
    *   Add new tests or modify existing ones if significant logic changes occur (e.g., output directory handling).
7.  **Final Review and Submission:**
    *   Perform a final review of all changes.
    *   Update `PLAN.md` and `TODO.md` to reflect completed tasks.
    *   Update `CHANGELOG.md` with a summary of changes.
    *   Submit the changes.
</file>

<file path="pyproject.toml">
# this_file: pyproject.toml
# this_project: twat_genai
# Build System Configuration
# -------------------------
# Specifies the build system and its requirements for packaging the project
# Specifies the build backend and its requirements for building the package
[build-system]
requires = [
    "hatchling>=1.27.0", # Core build backend for Hatch
    "hatch-vcs>=0.4.0", # Version Control System plugin for Hatch

]
build-backend = "hatchling.build" # Use Hatchling as the build backend

# Wheel build configuration
# Specifies which packages to include in the wheel distribution
[tool.hatch.build.targets.wheel]
packages = ["src/twat_genai"]

# Project Metadata Configuration
# ------------------------------
# Comprehensive project description, requirements, and compatibility information
[project]
name = "twat-genai"
dynamic = ["version"] # Version is determined dynamically from VCS
description = ""
readme = "README.md"
requires-python = ">=3.10" # Minimum Python version required
license = "MIT"
keywords = []
classifiers = [
    "Development Status :: 4 - Beta",
    "Programming Language :: Python",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "Programming Language :: Python :: Implementation :: CPython",
    "Programming Language :: Python :: Implementation :: PyPy",
]
# Runtime Dependencies
# -------------------
# External packages required for the project to function
dependencies = [
    "fal-client>=0.5.9", # FAL.ai client for AI operations
    "fire>=0.5.0", # For building command-line interfaces (used by superres CLI)
    "httpx>=0.28.1", # HTTP client library
    "dotenv>=0.9.9", # Environment variable management
    "loguru>=0.7.3", # Logging library
    "numpy>=2.2.4", # Numerical computing library
    "Pillow>=11.2.1", # Image processing library
    "pydantic>=2.11.3", # Data validation using Python type annotations
    "python-slugify>=8.0.4", # Text slugification library
    "requests>=2.31.0", # HTTP library (used by superres API client)
    "rich>=13.7.0", # For rich text and beautiful formatting in the terminal (used by superres CLI)
    "tqdm>=4.67.1", # Fast, extensible progress bar (used by superres)
    "twat>=1.8.1", # Main twat package
    "twat-image>=1.8.1", # Image handling functionality
    "twat-fs>=2.7.5", # File system handling functionality
    "twat-os>=2.7.5", # OS/path handling functionality
    "webcolors>=24.11.1", # Color name and value conversion

]

# Project Authors
# ---------------
[[project.authors]]
name = "Adam Twardoch"
email = "adam+github@twardoch.com"

# Project URLs
# ------------
# Links to project resources for documentation, issues, and source code
[project.urls]
Documentation = "https://github.com/twardoch/twat-genai#readme"
Issues = "https://github.com/twardoch/twat-genai/issues"
Source = "https://github.com/twardoch/twat-genai"

# Twat Plugin Registration
# -----------------------
# Registers this package as a plugin for the twat ecosystem
[project.entry-points."twat.plugins"]
genai = "twat_genai"

# Version configuration using VCS (Git)
[tool.hatch.version]
source = "vcs"

[tool.hatch.version.raw-options]
version_scheme = "post-release"

# VCS hook configuration for version file generation
[tool.hatch.build.hooks.vcs]
version-file = "src/twat_genai/__version__.py"

# Default development environment configuration
[tool.hatch.envs.default]
dependencies = [
    "pytest", # Testing framework
    "pytest-cov", # Coverage reporting
    "pytest-mock", # Mocking support
    "pytest-asyncio", # Async test support
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

# Scripts available in the default environment
[tool.hatch.envs.default.scripts]
test = "pytest {args:tests}"
test-cov = "pytest --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_genai --cov=tests {args:tests}"
type-check = "mypy src/twat_genai tests"
lint = ["ruff check src/twat_genai tests", "ruff format src/twat_genai tests"]

# Python version matrix for testing
[[tool.hatch.envs.all.matrix]]
python = ["3.10", "3.11", "3.12"]

# Linting environment configuration
[tool.hatch.envs.lint]
detached = true # Run in isolated environment
dependencies = [
    "mypy>=1.15.0", # Static type checker
    "ruff>=0.9.6", # Fast Python linter

]

# Linting environment scripts
[tool.hatch.envs.lint.scripts]
typing = "mypy --install-types --non-interactive {args:src/twat_genai tests}"
style = ["ruff check {args:.}", "ruff format {args:.}"]
fmt = ["ruff format {args:.}", "ruff check --fix {args:.}"]
all = ["style", "typing"]

# Ruff (linter) configuration
[tool.ruff]
target-version = "py310"
line-length = 88

# Ruff lint rules configuration
[tool.ruff.lint]
extend-select = [
    "A", # flake8-builtins
    "ARG", # flake8-unused-arguments
    "B", # flake8-bugbear
    "C", # flake8-comprehensions
    "DTZ", # flake8-datetimez
    "E", # pycodestyle errors
    "EM", # flake8-errmsg
    "F", # pyflakes
    "FBT", # flake8-boolean-trap
    "I", # isort
    "ICN", # flake8-import-conventions
    "ISC", # flake8-implicit-str-concat
    "N", # pep8-naming
    "PLC", # pylint convention
    "PLE", # pylint error
    "PLR", # pylint refactor
    "PLW", # pylint warning
    "Q", # flake8-quotes
    "RUF", # Ruff-specific rules
    "S", # flake8-bandit
    "T", # flake8-debugger
    "TID", # flake8-tidy-imports
    "UP", # pyupgrade
    "W", # pycodestyle warnings
    "YTT", # flake8-2020

]
ignore = [
    "ARG001", # Unused function argument
    "E501", # Line too long
    "I001", # Import block formatting

]

# isort configuration within Ruff
[tool.ruff.lint.isort]
known-first-party = ["twat_genai"] # Treat as first-party imports for sorting

# flake8-tidy-imports configuration within Ruff
[tool.ruff.lint.flake8-tidy-imports]
ban-relative-imports = "all" # Ban all relative imports for consistency

# Per-file rule exceptions
[tool.ruff.lint.per-file-ignores]
# Tests can use magic values, assertions, and relative imports
"tests/**/*" = [
    "PLR2004", # Allow magic values in tests for readability
    "S101", # Allow assertions in tests
    "TID252"
    # Allow relative imports in tests for convenience
]

# MyPy (type checker) configuration
[tool.mypy]
python_version = "3.10"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true

# Coverage.py configuration for test coverage
[tool.coverage.run]
source_pkgs = ["twat_genai", "tests"]
branch = true
parallel = true
omit = ["src/twat_genai/__about__.py"]

# Coverage path mappings
[tool.coverage.paths]
twat_genai = ["src/twat_genai", "*/twat-genai/src/twat_genai"]
tests = ["tests", "*/twat-genai/tests"]

# Coverage report configuration
[tool.coverage.report]
exclude_lines = ["no cov", "if __name__ == .__main__.:", "if TYPE_CHECKING:"]

# Optional dependencies
[project.optional-dependencies]
test = [
    "pytest>=8.3.4",
    "pytest-cov>=6.0.0",
    "pytest-xdist>=3.6.1", # For parallel test execution
    "pytest-benchmark[histogram]>=5.1.0", # For performance testing
    "pytest-mock", # Mocking support
    "pytest-asyncio", # Async test support

]
dev = [
    "pre-commit>=4.1.0", # Git pre-commit hooks
    "ruff>=0.9.6", # Fast Python linter
    "mypy>=1.15.0", # Static type checker

]
all = [
    "fal-client>=0.5.9", # FAL.ai client
    "httpx>=0.28.1", # HTTP client
    "numpy>=2.2.3", # Numerical computing
    "Pillow>=11.1.0", # Image processing
    "pydantic>=2.10.6", # Data validation
    "python-slugify>=8.0.4", # Text slugification
    "twat>=1.8.1", # Main package
    "twat-image>=1.8.1", # Image handling
    "twat-os>=1.8.1", # OS/path handling
    "webcolors>=24.11.1", # Color utilities

]

# Test environment configuration
[tool.hatch.envs.test]
dependencies = [".[test]"]

# Test environment scripts
[tool.hatch.envs.test.scripts]
test = "python -m pytest -n auto {args:tests}"
test-cov = "python -m pytest -n auto --cov-report=term-missing --cov-config=pyproject.toml --cov=src/twat_genai --cov=tests {args:tests}"
bench = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only"
bench-save = "python -m pytest -v -p no:briefcase tests/test_benchmark.py --benchmark-only --benchmark-json=benchmark/results.json"

# Pytest configuration
[tool.pytest.ini_options]
markers = ["benchmark: marks tests as benchmarks (select with '-m benchmark')"]
addopts = "-v -p no:briefcase"
testpaths = ["tests"]
python_files = ["test_*.py"]
filterwarnings = ["ignore::DeprecationWarning", "ignore::UserWarning"]
asyncio_mode = "auto"

# Pytest-benchmark configuration
[tool.pytest-benchmark]
min_rounds = 100
min_time = 0.1
histogram = true
storage = "file"
save-data = true
compare = [
    "min", # Minimum time
    "max", # Maximum time
    "mean", # Mean time
    "stddev", # Standard deviation
    "median", # Median time
    "iqr", # Inter-quartile range
    "ops", # Operations per second
    "rounds", # Number of rounds

]
</file>

<file path="README.md">
# twat-genai

## Overview

`twat-genai` is a Python package designed to provide a unified interface for various generative AI image tasks, currently focusing on models available through the FAL (fal.ai) platform. It allows users to perform Text-to-Image, Image-to-Image, ControlNet-like operations (Canny, Depth), Image Upscaling, and Image Outpainting via both a command-line interface (CLI) and a Python API.

The package aims for modularity and extensibility, separating concerns into core components, engine implementations, and the user interface.

## Installation

1.  **Prerequisites**:
    *   Python 3.10 or higher.
    *   `uv` (recommended for faster installation): `pip install uv` or `curl -LsSf https://astral.sh/uv/install.sh | sh`.
    *   `git` (for versioning).
2.  **Clone the repository**:
    ```bash
    git clone https://github.com/twardoch/twat-genai.git
    cd twat-genai
    ```
3.  **Set up environment and install**:
    ```bash
    # Create virtual environment (optional but recommended)
    uv venv
    source .venv/bin/activate  # On Linux/macOS
    # .venv\Scripts\activate  # On Windows

    # Install the package in editable mode with all dependencies
    uv pip install -e ".[all]"
    ```
4.  **Set FAL API Key**:
    Obtain an API key from [fal.ai](https://fal.ai/) and set it as an environment variable:
    ```bash
    export FAL_KEY="your-fal-api-key"
    # Or add FAL_KEY="your-fal-api-key" to a .env file in the project root.
    ```

## Architecture

The package follows a layered architecture:

1.  **Core (`src/twat_genai/core`)**: Contains fundamental building blocks and utilities independent of specific AI engines.
    *   `config.py`: Defines core data structures like `ImageInput` (representing input images via URL, path, or PIL object), `ImageResult` (standardized output containing metadata, paths, and results), `ImageSizeWH`, type aliases (`Prompts`, `OutputDir`, etc.), and `ImageInput` validation logic.
    *   `lora.py`: Defines Pydantic models for representing LoRA configurations (`LoraRecord`, `LoraRecordList`, `LoraLib`, `LoraSpecEntry`, `CombinedLoraSpecEntry`). Handles loading LoRA definitions from JSON.
    *   `image.py`: Defines image format (`ImageFormats`) and size (`ImageSizes`) enums, and provides a utility function `save_image` for saving PIL images to disk. Also includes `validate_image_size` for parsing size strings.
    *   `image_utils.py`: Provides utilities for handling images, such as asynchronous downloading from URLs (`download_image_to_temp`, `download_image` using `httpx`) and resizing based on model constraints (`resize_image_if_needed`, utilizing size limits defined in `engines/fal/upscale.py`).
    *   `prompt.py`: Includes logic for parsing and expanding Midjourney-style prompt syntax, handling image prompts (URLs), multi-prompts (`::`), weighted parts, permutation prompts (`{alt1, alt2}`), and parameters (`--param value`). Provides `normalize_prompts` for processing input prompts.
    *   `models.py`: *Note: This module appears to contain duplicates of models defined elsewhere (e.g., `ImageSizes`, `ImageResult`). This might be a remnant of refactoring and the definitions in `core/config.py` and `core/image.py` are likely the canonical ones.*
    *   `__init__.py`: Makes the `core` directory a Python package.

2.  **Engines (`src/twat_genai/engines`)**: Implements the logic for interacting with specific AI platforms or model families.
    *   `base.py`: Defines the abstract base class `ImageGenerationEngine` which specifies the common interface (`initialize`, `generate`, `shutdown`, async context manager support) and the base `EngineConfig` Pydantic model (containing `guidance_scale`, `num_inference_steps`, `image_size`, `enable_safety_checker`).
    *   **FAL Engine (`src/twat_genai/engines/fal`)**: Implementation for the FAL platform.
        *   `__init__.py`: Contains the `FALEngine` class, which inherits from `ImageGenerationEngine`. It orchestrates the process by:
            *   Initializing the `FalApiClient` on `initialize()` or first `generate()` call, checking for the `FAL_KEY` env var.
            *   Handling input image preparation (downloading URLs via `core.image_utils`, resizing for upscalers via `core.image_utils`, uploading local/temp files via `client.upload_image`) in `_prepare_image_input`. Handles temp file cleanup.
            *   Receiving generation requests via the `generate` method.
            *   Determining the operation type (TTI, I2I, Upscale, Outpaint, Canny, Depth) based on the `model` parameter (`ModelTypes` enum).
            *   Calling the appropriate `process_*` method on the `FalApiClient` (e.g., `process_upscale`, `process_outpaint`, `process_tti`, `process_i2i`).
            *   Handling `shutdown`.
        *   `client.py`: Contains the `FalApiClient` class responsible for direct interaction with the FAL API using the `fal_client` library.
            *   Provides `upload_image` using `fal_client.upload_file_async`.
            *   Includes methods for specific operations: `process_tti`, `process_i2i`, `process_canny`, `process_depth`, `process_upscale`, `process_outpaint`.
            *   These methods delegate job submission to the top-level helper `_submit_fal_job` (which uses `fal_client.submit_async`).
            *   Result retrieval and processing happen in `_get_fal_result`, which polls using `fal_client.status_async` and fetches with `fal_client.result_async`.
            *   Parses results using a dispatch mechanism (`_get_result_extractor`) that selects an appropriate parsing function (currently `_extract_generic_image_info`) to standardize output from different FAL endpoints into `image_info` dicts.
            *   Handles downloading the final image using the top-level helper `_download_image_helper` (uses `httpx`).
            *   Constructs and returns the final `ImageResult` object, saving metadata to a JSON file if `output_dir` is provided.
        *   `config.py`: Defines FAL-specific Pydantic configuration models used as *schemas* for arguments:
            *   `ModelTypes` enum mapping model names to FAL API endpoint strings.
            *   `ImageToImageConfig`, `UpscaleConfig` (with many tool-specific fields), and `OutpaintConfig`. These models expect an image input conforming to the `FALImageInput` structure.
            *   *Note: This file also incorrectly contains a duplicate definition of `FalApiClient` and related helpers, which should be ignored.*
        *   `lora.py`: Contains FAL-specific LoRA handling logic:
            *   Loading the LoRA library from JSON (`get_lora_lib`, using `__main___loras.json` or `TWAT_GENAI_LORA_LIB` path).
            *   Parsing LoRA specification strings (library keys, `url:scale` syntax) via `parse_lora_phrase`.
            *   Normalizing various input spec formats (`str`, `list`, `dict`) into a list of `LoraSpecEntry` or `CombinedLoraSpecEntry` objects using `normalize_lora_spec`.
            *   Building the final LoRA argument list (list of dicts with `path` and `scale`) and augmenting the text prompt for the FAL API call using `build_lora_arguments`.
        *   `upscale.py`: Contains data and helper logic specific to upscaling:
            *   `UPSCALE_TOOL_DEFAULT_PARAMS`: Dictionary mapping upscale `ModelTypes` to their default API parameters.
            *   `UPSCALE_TOOL_MAX_INPUT_SIZES`: Dictionary mapping upscale `ModelTypes` to their maximum allowed input dimensions (used by `core.image_utils.resize_image_if_needed`).
            *   `run_upscale` (currently unused): Helper function intended to assemble API arguments from `UpscaleConfig` and defaults before calling the client.
        *   `outpaint.py`: Contains helper logic specific to outpainting:
            *   `run_outpaint` (currently unused): Helper function that calculates image placement (`original_image_location`, `original_image_size`), gets the image URL, assembles API arguments from `OutpaintConfig`, and calls `client.process_outpaint`.
        *   `models.py`: Defines the concrete `FALImageInput` class, inheriting from `core.config.ImageInput`. Its `to_url` async method implements the logic to convert Path or PIL image inputs into uploadable URLs using `fal_client.upload_file_async`.

3.  **CLI (`src/twat_genai/cli.py`)**: Provides the command-line interface using the `python-fire` library.
    *   Defines the main `cli` function that parses arguments for all supported modes and parameters.
    *   Maps CLI arguments to the appropriate `ModelTypes`, constructs the base `EngineConfig`, and creates specific configuration objects (`ImageToImageConfig`, `UpscaleConfig`, `OutpaintConfig`) as needed.
    *   Handles parsing of `input_image` (path vs URL) into `core.config.ImageInput`.
    *   Delegates execution to the `async_main` helper function, which instantiates and runs the `FALEngine`.
    *   Handles output directory resolution using `twat.paths.PathManager` (if available) or defaults to `./generated_images`.
    *   Includes helper functions for parsing arguments like `image_size`.

4.  **Entry Point (`src/twat_genai/__main__.py`)**: A minimal script that allows the package to be run as a module (`python -m twat_genai`). It simply imports and calls `fire.Fire(cli)` from `cli.py`.

5.  **Tests (`tests/`)**: Contains unit tests using `pytest` and `unittest.mock`.
    *   `test_fal_client.py`: Tests the `FalApiClient` methods (like `process_tti`, `_extract_generic_image_info`) and internal helpers, using mocked dependencies (`fal_client` calls, LoRA building, file system access).
    *   `test_image_utils.py`: Tests image downloading (`download_image_to_temp`, `download_image`) and resizing (`resize_image_if_needed`) utilities.
    *   `test_twat_genai.py`: Basic package tests (e.g., checking `__version__`).
    *   `conftest.py`: Provides shared `pytest` fixtures, such as `mock_fal_api_client` for injecting a mocked client instance into tests.

## Functionality

1.  **Initialization**: When the `FALEngine` is instantiated (either via CLI or Python API) and its `initialize()` method is called (or implicitly via context manager `__aenter__` or first `generate()` call), it ensures the `FAL_KEY` environment variable is set and initializes the `FalApiClient`.
2.  **Input Handling**:
    *   Accepts prompts as strings or lists of strings. Supports brace expansion for permutations (e.g., `a {red,blue} car`) and semicolon separation for multiple distinct prompts via `core.prompt.normalize_prompts`.
    *   Accepts image inputs as URLs, local file paths, or PIL Images via the `core.config.ImageInput` model. The `engines.fal.models.FALImageInput` subclass handles the conversion of paths/PIL images to URLs by uploading them using `fal_client.upload_file_async` when its `to_url()` method is invoked (typically within `FALEngine._prepare_image_input`).
    *   Handles LoRA specifications via strings (keywords defined in `__main___loras.json`, `url:scale` pairs, `;` separated lists) or structured lists/dicts. The `engines.fal.lora.build_lora_arguments` function parses the spec and prepares the arguments for the FAL API.
3.  **Configuration**: Uses Pydantic models (`EngineConfig`, `ImageToImageConfig`, `UpscaleConfig`, `OutpaintConfig`) for type validation and structuring configuration. These models are populated based on CLI arguments or Python API calls and passed to the `FALEngine.generate` method.
4.  **Image Preparation (`FALEngine._prepare_image_input`)**: Before calling the FAL API for modes requiring an input image (I2I, Canny, Depth, Upscale, Outpaint), this method ensures a usable image URL is available:
    *   If the input is a URL, it's downloaded to a temporary file using `core.image_utils.download_image_to_temp`.
    *   If the input is a PIL Image, it's saved to a temporary file.
    *   If the operation is an upscale, it checks the dimensions against `engines.fal.upscale.UPSCALE_TOOL_MAX_INPUT_SIZES`. If the image exceeds the limits for the specific `model_type`, it's resized using `core.image_utils.resize_image_if_needed`, saving the result to another temporary file.
    *   The final image file (original path, downloaded temp, or resized temp) is uploaded using `client.upload_image` to get a FAL-usable URL.
    *   Temporary files created during this process are cleaned up afterwards.
5.  **API Interaction (`FalApiClient`)**:
    *   Builds the final API argument dictionary based on the operation type, base config, specific config, and processed LoRA/prompt information.
    *   Submits the job to the appropriate FAL endpoint (defined in `ModelTypes` enum) using `fal_client.submit_async` via the `_submit_fal_job` helper.
    *   Polls for job status using `fal_client.status_async` in `_get_fal_result`.
    *   Fetches the final result dictionary using `fal_client.result_async` in `_get_fal_result`.
    *   Parses the result structure using `_extract_generic_image_info` (selected via `_get_result_extractor`) to extract key information like image URL(s), dimensions, content type, and seed (if provided by the API).
6.  **Result Handling (`FalApiClient._get_fal_result`)**:
    *   Downloads the generated image URL(s) to the specified `output_dir` using `_download_image_helper`.
    *   Constructs a standardized `ImageResult` object (`core.config.ImageResult`) containing:
        *   Request ID, timestamp.
        *   The raw API result dictionary.
        *   Parsed `image_info` dictionary (containing the final local path, URL, dimensions, content type, seed, metadata path, etc.).
        *   Original prompt and the full `job_params` dictionary used for the request (for reproducibility/logging).
    *   Saves the `ImageResult` object (excluding the PIL image itself) as a JSON metadata file alongside the downloaded image in the `output_dir`.
    *   Returns the `ImageResult` object (without the PIL image loaded by default).
7.  **CLI Operation (`cli.py`)**: The command-line interface translates flags and arguments into the necessary configuration objects (`EngineConfig`, `ImageInput`, `ImageToImageConfig`, `UpscaleConfig`, `OutpaintConfig`). It then calls the core asynchronous logic (`async_main`), which instantiates the `FALEngine` and invokes its `generate` method, finally printing basic information about the generated result(s). Error handling for invalid arguments or configuration issues is included.

## Usage

### CLI

The main entry point is the `twat-genai` command (or `python -m twat_genai`).

```bash
# Basic Text-to-Image (TTI)
twat-genai --prompts "a futuristic cityscape at sunset" --output_dir generated --filename_prefix city

# TTI with multiple prompts and brace expansion
twat-genai --prompts "a photo of a {red,blue} car; a drawing of a {cat,dog}" --image_size HD

# Image-to-Image (I2I)
twat-genai --model image --input_image path/to/input.jpg --prompts "make it look like an oil painting" --strength 0.65

# ControlNet Canny
twat-genai --model canny --input_image path/to/drawing.png --prompts "a detailed spaceship based on the sketch"

# Upscale using ESRGAN
twat-genai --model upscale --input_image path/to/lowres.png --upscale_tool esrgan --scale 4 --output_dir upscaled

# Upscale using Ideogram (requires prompt)
twat-genai --model upscale --input_image path/to/photo.jpg --upscale_tool ideogram --prompts "enhance the details of the landscape photo"

# Outpaint an image
twat-genai --model outpaint --input_image path/to/center_image.png --prompts "expand the scene with a forest on the left and a river on the right" --target_width 2048 --target_height 1024

# Using LoRA from library (defined in __main___loras.json)
twat-genai --prompts "a portrait" --lora "gstdrw style"

# Using specific LoRA URL with scale
twat-genai --prompts "a robot" --lora "https://huggingface.co/path/to/lora:0.8"

# Verbose logging
twat-genai --prompts "debug prompt" --verbose
```

**Common Arguments:**

*   `--prompts`: One or more prompts (string or list). Use `;` to separate multiple prompts in a single string. Use `{a,b}` for permutations.
*   `--output_dir`: Directory to save results (defaults to `generated_images`).
*   `--model`: Operation type (`text`, `image`, `canny`, `depth`, `upscale`, `outpaint`). Default: `text`.
*   `--input_image`: Path or URL to an input image (required for non-text models).
*   `--filename_suffix`, `--filename_prefix`: Customize output filenames.
*   `--image_size`: Output image size preset (`SQ`, `SQL`, `SD`, `HD`, `SDV`, `HDV`) or custom `width,height`. Default: `SQ`.
*   `--guidance_scale`, `--num_inference_steps`: Control generation process.
*   `--negative_prompt`: Specify negative prompts.
*   `--lora`: LoRA specification string (see examples).
*   `--strength`: Control influence of input image in I2I (0.0 to 1.0). Default: 0.75.
*   `--upscale_tool`: Name of the upscaler model (e.g., `esrgan`, `ideogram`, `ccsr`). Required for `--model upscale`.
*   `--scale`: Target upscale factor (e.g., 2, 4). Used by some upscalers.
*   `--target_width`, `--target_height`: Target dimensions for outpainting. Required for `--model outpaint`.
*   `--verbose`: Enable debug logging.

### Python API

```python
import asyncio
from pathlib import Path
from twat_genai import (
    FALEngine, ModelTypes, EngineConfig, ImageInput,
    ImageToImageConfig, FALImageInput, UpscaleConfig, OutpaintConfig
)

async def run_generation():
    output_dir = Path("api_generated")
    # Use context manager for initialization and shutdown
    async with FALEngine(output_dir=output_dir) as engine:

        # --- Text-to-Image ---
        print("--- Running Text-to-Image ---")
        tti_config = EngineConfig(image_size="HD", guidance_scale=4.0)
        result_tti = await engine.generate(
            prompt="A photorealistic image of a bioluminescent forest at night",
            config=tti_config,
            model=ModelTypes.TEXT,
            filename_prefix="bioluminescent_forest"
        )
        print(f"TTI Result Path: {result_tti.image_info.get('path')}")
        print(f"TTI Metadata Path: {result_tti.image_info.get('metadata_path')}")

        # --- Image-to-Image ---
        print("\n--- Running Image-to-Image ---")
        # Use a placeholder URL for the example
        input_img_i2i_url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/sketch-mountains-input.jpg"
        print(f"Using input image URL: {input_img_i2i_url}")
        # Or use a local path if needed:
        # input_img_i2i = ImageInput(path=Path("input.jpg"))
        # if not input_img_i2i.path or not input_img_i2i.path.exists():
        #      print("Skipping I2I: input.jpg not found.")
        # else:
        i2i_base_config = EngineConfig(num_inference_steps=30)
        # Create FALImageInput directly from URL
        i2i_model_config = ImageToImageConfig(
            model_type=ModelTypes.IMAGE, # Technically redundant if passed to generate
            input_image=FALImageInput(url=input_img_i2i_url),
            strength=0.7,
            negative_prompt="blurry, low quality"
        )
        result_i2i = await engine.generate(
            prompt="Convert this image to a comic book style",
            config=i2i_base_config,
            model=ModelTypes.IMAGE,
            image_config=i2i_model_config, # Pass the specific config
            lora_spec="shou_xin:0.5", # Example LoRA spec
            filename_prefix="comic_style"
        )
        print(f"I2I Result Path: {result_i2i.image_info.get('path')}")

        # --- Upscale ---
        print("\n--- Running Upscale ---")
        # Use a placeholder URL for the example
        input_img_upscale_url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-input.png"
        print(f"Using input image URL: {input_img_upscale_url}")
        # Or use a local path:
        # input_img_upscale = ImageInput(path=Path("low_res.png"))
        # if not input_img_upscale.path or not input_img_upscale.path.exists():
        #     print("Skipping Upscale: low_res.png not found.")
        # else:
        upscale_base_config = EngineConfig() # Upscalers might ignore some base params
        upscale_model_config = UpscaleConfig(
                input_image=FALImageInput(url=input_img_upscale_url),
                # Prompt might be needed for some upscalers like Ideogram
                prompt="Enhance details, sharp focus, high resolution",
                scale=4 # General scale, specific tools might override
        )
        result_upscale = await engine.generate(
                prompt="Enhance details, sharp focus, high resolution", # Often passed directly too
                config=upscale_base_config,
                model=ModelTypes.UPSCALER_ESRGAN, # Choose the specific upscaler
                upscale_config=upscale_model_config, # Pass the specific config
                filename_prefix="upscaled_esrgan"
        )
        print(f"Upscale Result Path: {result_upscale.image_info.get('path')}")

        # --- Outpaint ---
        print("\n--- Running Outpaint ---")
        # Use a placeholder URL for the example
        input_img_outpaint_url = "https://raw.githubusercontent.com/CompVis/stable-diffusion/main/assets/stable-samples/img2img/mountains-input.png"
        print(f"Using input image URL: {input_img_outpaint_url}")
        # Or use a local path:
        # input_img_outpaint = ImageInput(path=Path("center.png"))
        # if not input_img_outpaint.path or not input_img_outpaint.path.exists():
        #      print("Skipping Outpaint: center.png not found.")
        # else:
        outpaint_base_config = EngineConfig()
        outpaint_model_config = OutpaintConfig(
                input_image=FALImageInput(url=input_img_outpaint_url),
                prompt="Expand the mountain scene with a clear blue sky and forest below",
                target_width=1536,
                target_height=1024
        )
        result_outpaint = await engine.generate(
                prompt="Expand the mountain scene with a clear blue sky and forest below", # Pass prompt here too
                config=outpaint_base_config,
                model=ModelTypes.OUTPAINT_BRIA,
                outpaint_config=outpaint_model_config,
                filename_prefix="outpainted_scene"
        )
        print(f"Outpaint Result Path: {result_outpaint.image_info.get('path')}")


if __name__ == "__main__":
    # Remove the dummy file creation logic to make the example cleaner
    # Users should provide their own images or URLs if modifying the example.
    # print("Note: This example uses placeholder URLs. Provide image paths or URLs for real use.")
    # try:
    #     from PIL import Image
    #     dummy_size = (512, 512)
    #     if not Path("input.jpg").exists(): Image.new('RGB', dummy_size, color = 'red').save("input.jpg")
    #     if not Path("low_res.png").exists(): Image.new('RGB', (256, 256), color = 'blue').save("low_res.png")
    #     if not Path("center.png").exists(): Image.new('RGB', (512, 512), color = 'green').save("center.png")
    # except ImportError:
    #     print("Pillow not installed, cannot create dummy images if paths are used.")

    # Check for FAL_KEY
    import os
    if not os.getenv("FAL_KEY"):
        print("Error: FAL_KEY environment variable not set.")
        print("Please set your FAL API key: export FAL_KEY='your-key'")
    else:
        asyncio.run(run_generation())

## Maintenance & Development

*   **Code Quality**: The project uses `ruff` for linting and formatting, `mypy` for static type checking, and `pytest` for unit testing.
    *   Run checks locally: `uv run lint` and `uv run test`.
    *   Pre-commit hooks are configured in `.pre-commit-config.yaml` to enforce quality standards before committing. Install hooks with `pre-commit install`.
*   **Dependencies**: Managed using `uv` and specified in `pyproject.toml`. Install with `uv pip install -e ".[all]"`.
*   **Cleanup Script**: `cleanup.py` provides commands for maintaining the repository:
    *   `python cleanup.py status`: Checks file structure, git status, installs dependencies, and runs quality checks. Use this first.
    *   `python cleanup.py update`: Runs `status`, then stages and commits changes. Use for routine commits.
    *   `python cleanup.py push`: Pushes committed changes.
    *   `python cleanup.py venv`: Sets up the virtual environment.
    *   `python cleanup.py install`: Installs dependencies.
    It logs operations to `CLEANUP.txt`.
*   **Versioning**: Handled by `hatch-vcs`, deriving the version from `git` tags.
*   **Documentation**:
    *   `README.md`: This file.
    *   `LOG.md`: Changelog.
    *   `TODO.md`: Task tracking.
    *   Docstrings in code.
*   **CI/CD**: GitHub Actions workflows in `.github/workflows` handle:
    *   `push.yml`: Runs quality checks, tests, and builds distributions on pushes/PRs to `main`.
    *   `release.yml`: Builds and publishes the package to PyPI and creates a GitHub Release when a tag matching `v*` is pushed.
</file>

<file path="test_outpaint.py">
#!/usr/bin/env -S uv run
# /// script
# dependencies = ["pydantic", "Pillow", "loguru", "fire"]
# ///
"""
Test script for twat_genai outpaint command.
This script can be used to verify the fixed outpaint functionality.
"""

import sys
from loguru import logger

# Configure logging
logger.remove()
logger.add(sys.stderr, level="DEBUG")


def test_outpaint(input_image: str | None = None):
    """
    Test the outpaint functionality without running the actual command.

    Args:
        input_image: Path to an image file (optional)
    """
    from twat_genai.cli import TwatGenAiCLI

    logger.info(f"Testing outpaint with input_image: {input_image}")

    cli = TwatGenAiCLI(verbose=True)

    if input_image:
        try:
            # Test the _prepare_image_input function directly
            input_img = cli._prepare_image_input(input_image)
            logger.info(f"Successfully prepared input image: {input_img}")

            # Check the path exists
            if input_img and input_img.path:
                logger.info(f"Path exists: {input_img.path.exists()}")

            return "Success: Image input preparation is working correctly!"
        except Exception as e:
            logger.error(f"Error preparing image: {e}")
            return f"Error: {e}"
    else:
        logger.warning("No input_image provided. Checking directory creation only.")
        # Just check if the image directory is created properly
        from twat_os.paths import PathManager

        paths = PathManager.for_package("twat_genai")
        logger.info(f"twat_genai paths: {paths}")
        logger.info(f"GenAI output directory: {paths.genai.output_dir}")
        return f"Output directory would be: {paths.genai.output_dir}"


if __name__ == "__main__":
    import fire

    fire.Fire(test_outpaint)
</file>

<file path="TODO.md">
- [ ] **Project Setup & Initial Cleanup:**
    - [x] Verify `npc-engine` installation or proceed with manual analysis (Proceeding manually)
    - [x] Create `PLAN.md`
    - [x] Create `TODO.md` (or update if existing)
    - [x] Create `CHANGELOG.md`
- [ ] **Address Code Duplication and Unused Code:**
    - [ ] **`core/models.py`**: Investigate and remove if redundant.
    - [ ] **`engines/fal/config.py`**: Remove duplicate `FalApiClient`.
    - [ ] **`ImageSizeWH`**: Consolidate definition to `core/config.py`.
    - [ ] **Unused `run_upscale`/`run_outpaint`**: Remove if truly unused.
- [ ] **Streamline Configuration and Core Logic:**
    - [ ] **Output Directory Logic (`cli.py`)**: Simplify, potentially remove `PathManager` dependency for MVP.
    - [ ] **CLI Async Structure (`cli.py`)**: Evaluate refactoring (minor).
- [ ] **Review and Refine CLI Arguments and Commands:**
    - [ ] **Upscale Tool Parameters (`cli.py`)**: Assess if simplification is needed (assume current is fine for now).
    - [ ] **Default Prompts for Upscalers (`cli.py`)**: Confirm behavior.
- [ ] **Documentation and Helper Scripts:**
    - [ ] **`README.md`**: Update to reflect changes.
    - [ ] **`cleanup.py`**: Decide on its role/necessity for MVP.
    - [ ] **LoRA Configuration**: Review and ensure robustness.
- [ ] **Testing:**
    - [ ] Ensure existing tests pass.
    - [ ] Add/modify tests for significant changes.
- [ ] **Final Review and Submission:**
    - [ ] Final code review.
    - [ ] Update `PLAN.md` and `TODO.md`.
    - [ ] Update `CHANGELOG.md`.
    - [ ] Submit.
</file>

<file path="VERSION.txt">
v2.8.5
</file>

</files>
